{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"spider\\train_spider.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file at path D:/DS/Learnin/Essex MS/CE888/git/CE888/assignment2/seq2seq/spider/train_spider.json\n",
      "7000 Rows in Total\n",
      "Reading file at path D:/DS/Learnin/Essex MS/CE888/git/CE888/assignment2/seq2seq/spider/train_others.json\n",
      "8659 Rows in Total\n",
      "Filter Easy Queries\n",
      "Splittin the Train and Test data\n",
      "(727, 7)\n",
      "Data for Training (2908, 7)\n",
      "Data for Testing (727, 7)\n",
      "Sample Vocabulary ['', '[UNK]', 'the', '[start]', '[end]', 'of', '?', '.', 'what', 'are']\n",
      "Sample Vocabulary ['', '[UNK]', 'select', 'from', '[start]', '[end]', 'where', ')', '(', '=']\n"
     ]
    }
   ],
   "source": [
    "handlr = data.Train_Data('D:/DS/Learnin/Essex MS/CE888/git/CE888/assignment2/seq2seq/spider/',['train_spider.json','train_others.json'])\n",
    "input_text_processor = data.Features().vectorizor(handlr.questions , data.Max_Vocab_Size)\n",
    "output_text_processor = data.Features().vectorizor(handlr.sql , data.Max_Vocab_Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2253"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_processor.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = handlr.questions\n",
    "targ = handlr.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(inp)\n",
    "BATCH_SIZE = data.BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'Show the names of members whose country is \"United States\" or \"Canada\".'\n",
      " b'What are the dates of the assessment notes?'\n",
      " b'What is the average time span of contact channels in the database?'\n",
      " b'What is the average number of bank customers?'\n",
      " b'What campuses are located in Los Angeles county and opened after 1950?'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'SELECT Name FROM member WHERE Country  =  \"United States\" OR Country  =  \"Canada\"'\n",
      " b'SELECT date_of_notes FROM Assessment_Notes'\n",
      " b'SELECT avg(active_to_date - active_from_date) FROM customer_contact_channels'\n",
      " b'SELECT avg(no_of_customers) FROM bank'\n",
      " b'SELECT campus FROM campuses WHERE county  =  \"Los Angeles\" AND YEAR  >  1950'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_input_batch, example_target_batch in dataset.take(1):\n",
    "    print(example_input_batch[:5])\n",
    "    print()\n",
    "    print(example_target_batch[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 26), dtype=int64, numpy=\n",
       "array([[   3,   27,    2,   14,    5,  120,   35,  147,   13, 1480, 1076,\n",
       "          38, 1024,    7,    4,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0],\n",
       "       [   3,    8,    9,    2,   96,    5,    2,  990, 1162,    6,    4,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0],\n",
       "       [   3,    8,   13,    2,   32,  232, 1082,    5,  593,  520,   11,\n",
       "           2,  298,    6,    4,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0]], dtype=int64)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tokens = input_text_processor(example_input_batch)\n",
    "example_tokens[:3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[start] show the names of members whose country is \"united states\" or \"canada\" . [end]           '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
    "tokens = input_vocab[example_tokens[0].numpy()]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Mask')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(example_tokens)\n",
    "plt.title('Token IDs')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(example_tokens != 0)\n",
    "plt.title('Mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = data.embedding_dim\n",
    "units = data.units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "\n",
    "        # The embedding layer converts tokens to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
    "                                                embedding_dim)\n",
    "\n",
    "        # The GRU RNN layer processes those vectors sequentially.\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                    # Return the sequence and state\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, tokens, state=None):\n",
    "        \n",
    "\n",
    "\n",
    "        # 2. The embedding layer looks up the embedding for each token.\n",
    "        vectors = self.embedding(tokens)\n",
    "\n",
    "\n",
    "        # 3. The GRU processes the embedding sequence.\n",
    "        #    output shape: (batch, s, enc_units)\n",
    "        #    state shape: (batch, enc_units)\n",
    "        output, state = self.gru(vectors, initial_state=state)\n",
    "\n",
    "\n",
    "        # 4. Returns the new sequence and its state.\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2253"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_processor.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch, shape (batch): (64,)\n",
      "Input batch tokens, shape (batch, s): (64, 26)\n",
      "Encoder output, shape (batch, s, units): (64, 26, 1024)\n",
      "Encoder state, shape (batch, units): (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Convert the input text to tokens.\n",
    "example_tokens = input_text_processor(example_input_batch)\n",
    "\n",
    "# Encode the input sequence.\n",
    "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, units)\n",
    "example_enc_output, example_enc_state = encoder(example_tokens)\n",
    "\n",
    "print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
    "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
    "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
    "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # For Eqn. (4), the  Bahdanau attention\n",
    "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "        self.attention = tf.keras.layers.AdditiveAttention()\n",
    "\n",
    "    def call(self, query, value, mask):\n",
    "       \n",
    "        # From Eqn. (4), `W1@ht`.\n",
    "        w1_query = self.W1(query)\n",
    "       \n",
    "        # From Eqn. (4), `W2@hs`.\n",
    "        w2_key = self.W2(value)\n",
    "\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs = [w1_query, value, w2_key],\n",
    "            mask=[query_mask, value_mask],\n",
    "            return_attention_scores = True,\n",
    "        )\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch_size, query_seq_length, units):           (64, 2, 1024)\n",
      "Attention weights shape: (batch_size, query_seq_length, value_seq_length): (64, 2, 26)\n"
     ]
    }
   ],
   "source": [
    "# Later, the decoder will generate this attention query\n",
    "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
    "\n",
    "# Attend to the encoded tokens\n",
    "\n",
    "context_vector, attention_weights = attention_layer(\n",
    "    query=example_attention_query,\n",
    "    value=example_enc_output,\n",
    "    mask=(example_tokens != 0))\n",
    "\n",
    "print(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\n",
    "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Mask')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(attention_weights[:, 0, :])\n",
    "plt.title('Attention weights')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(example_tokens != 0)\n",
    "plt.title('Mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_slice = attention_weights[0, 0].numpy()\n",
    "attention_slice = attention_slice[attention_slice != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.dec_units = dec_units\n",
    "    self.output_vocab_size = output_vocab_size\n",
    "    self.embedding_dim = embedding_dim\n",
    "\n",
    "    # For Step 1. The embedding layer convets token IDs to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
    "                                               embedding_dim)\n",
    "\n",
    "    # For Step 2. The RNN keeps track of what's been generated so far.\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    # For step 3. The RNN output will be the query for the attention layer.\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    # For step 4. Eqn. (3): converting `ct` to `at`\n",
    "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
    "                                    use_bias=False)\n",
    "\n",
    "    # For step 5. This fully connected layer produces the logits for each\n",
    "    # output token.\n",
    "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "class DecoderInput(typing.NamedTuple):\n",
    "    new_tokens: Any\n",
    "    enc_output: Any\n",
    "    mask: Any\n",
    "\n",
    "class DecoderOutput(typing.NamedTuple):\n",
    "    logits: Any\n",
    "    attention_weights: Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self, inputs: DecoderInput, state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
    "\n",
    "  # Step 1. Lookup the embeddings\n",
    "  vectors = self.embedding(inputs.new_tokens)\n",
    " \n",
    "\n",
    "  # Step 2. Process one step with the RNN\n",
    "  rnn_output, state = self.gru(vectors, initial_state=state)\n",
    "\n",
    "  \n",
    "\n",
    "  # Step 3. Use the RNN output as the query for the attention over the\n",
    "  # encoder output.\n",
    "  context_vector, attention_weights = self.attention(\n",
    "      query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
    "  \n",
    "\n",
    "  # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
    "  #     [ct; ht] shape: (batch t, value_units + query_units)\n",
    "  context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
    "\n",
    "  # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
    "  attention_vector = self.Wc(context_and_rnn_output)\n",
    "\n",
    "  # Step 5. Generate logit predictions:\n",
    "  logits = self.fc(attention_vector)\n",
    "\n",
    "  return DecoderOutput(logits, attention_weights), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decoder.call = call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target sequence, and collect the \"[START]\" tokens\n",
    "example_output_tokens = output_text_processor(example_target_batch)\n",
    "\n",
    "start_index = output_text_processor.get_vocabulary().index('[start]')\n",
    "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: (batch_size, t, output_vocab_size) (64, 1, 2099)\n",
      "state shape: (batch_size, dec_units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Run the decoder\n",
    "dec_result, dec_state = decoder(\n",
    "    inputs = DecoderInput(new_tokens=first_token,\n",
    "                          enc_output=example_enc_output,\n",
    "                          mask=(example_tokens != 0)),\n",
    "    state = example_enc_state\n",
    ")\n",
    "\n",
    "print(f'logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\n",
    "print(f'state shape: (batch_size, dec_units) {dec_state.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['pilot'],\n",
       "       ['club\"'],\n",
       "       ['\"cleavant'],\n",
       "       [\"'full'\"],\n",
       "       ['city_name']], dtype='<U37')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(output_text_processor.get_vocabulary())\n",
    "first_word = vocab[sampled_token.numpy()]\n",
    "first_word[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_result, dec_state = decoder(\n",
    "    DecoderInput(sampled_token,\n",
    "                 example_enc_output,\n",
    "                 mask=(example_tokens != 0)),\n",
    "    state=dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['launch_year'],\n",
       "       ['genres;'],\n",
       "       ['card_number'],\n",
       "       ['dateundergoes'],\n",
       "       ['636']], dtype='<U37')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)\n",
    "first_word = vocab[sampled_token.numpy()]\n",
    "first_word[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        self.name = 'masked_loss'\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        # Calculate the loss for each item in the batch.\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "\n",
    "        # Mask off the losses on padding.\n",
    "        mask = tf.cast(y_true != 0, tf.float32)\n",
    "        loss *= mask\n",
    "\n",
    "        # Return the total.\n",
    "        return tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTranslator(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units,\n",
    "               input_text_processor,\n",
    "               output_text_processor, \n",
    "               use_tf_function=True):\n",
    "        super().__init__()\n",
    "        # Build the encoder and decoder\n",
    "        encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                        embedding_dim, units)\n",
    "        decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                        embedding_dim, units)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "        self.use_tf_function = use_tf_function\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_train_step(inputs)\n",
    "        else:\n",
    "         return self._train_step(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess(self, input_text, target_text):\n",
    "  \n",
    "\n",
    "  # Convert the text to token IDs\n",
    "  input_tokens = self.input_text_processor(input_text)\n",
    "  target_tokens = self.output_text_processor(target_text)\n",
    "  \n",
    "\n",
    "  # Convert IDs to masks.\n",
    "  input_mask = input_tokens != 0\n",
    " \n",
    "\n",
    "  target_mask = target_tokens != 0\n",
    "  \n",
    "\n",
    "  return input_tokens, input_mask, target_tokens, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._preprocess = _preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_step(self, inputs):\n",
    "    input_text, target_text = inputs  \n",
    "\n",
    "    (input_tokens, input_mask,target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
    "\n",
    "    max_target_length = tf.shape(target_tokens)[1]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Encode the input\n",
    "        enc_output, enc_state = self.encoder(input_tokens)\n",
    "        \n",
    "\n",
    "        # Initialize the decoder's state to the encoder's final state.\n",
    "        # This only works if the encoder and decoder have the same number of\n",
    "        # units.\n",
    "        dec_state = enc_state\n",
    "        loss = tf.constant(0.0)\n",
    "\n",
    "        for t in tf.range(max_target_length-1):\n",
    "            # Pass in two tokens from the target sequence:\n",
    "            # 1. The current input to the decoder.\n",
    "            # 2. The target for the decoder's next prediction.\n",
    "            new_tokens = target_tokens[:, t:t+2]\n",
    "            step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
    "                                                    enc_output, dec_state)\n",
    "            loss = loss + step_loss\n",
    "\n",
    "    # Average the loss over all non padding tokens.\n",
    "        average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
    "\n",
    "    # Apply an optimization step\n",
    "    variables = self.trainable_variables \n",
    "    gradients = tape.gradient(average_loss, variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    # Return a dict mapping metric names to current value\n",
    "    return {'batch_loss': average_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._train_step = _train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
    "    input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
    "\n",
    "    # Run the decoder one step.\n",
    "    decoder_input = DecoderInput(new_tokens=input_token,\n",
    "                                enc_output=enc_output,\n",
    "                                mask=input_mask)\n",
    "\n",
    "    dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
    "\n",
    "\n",
    "    # `self.loss` returns the total for non-padded tokens\n",
    "    y = target_token\n",
    "    y_pred = dec_result.logits\n",
    "    step_loss = self.loss(y, y_pred)\n",
    "\n",
    "    return step_loss, dec_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._loop_step = _loop_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = TrainTranslator(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    "    use_tf_function=False)\n",
    "\n",
    "# Configure the loss and optimizer\n",
    "translator.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=MaskedLoss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.649216319820633"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(output_text_processor.vocabulary_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.0313325>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.9751773>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.855935>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.4688644>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.1975126>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.910534>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.3237343>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.3402195>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.259539>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.0715237>}\n",
      "\n",
      "Wall time: 33.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for n in range(10):\n",
    "  print(translator.train_step([example_input_batch, example_target_batch]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
    "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
    "def _tf_train_step(self, inputs):\n",
    "  return self._train_step(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._tf_train_step = _tf_train_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.use_tf_function = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.91424>}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.train_step([example_input_batch, example_target_batch])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.9558828>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.8005328>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.7824287>}\n",
      "\n",
      "Wall time: 6.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for n in range(3):\n",
    "  print(translator.train_step([example_input_batch, example_target_batch]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = []\n",
    "# for n in range(100):\n",
    "#   print('.', end='')\n",
    "#   logs = translator.train_step([example_input_batch, example_target_batch])\n",
    "#   losses.append(logs['batch_loss'].numpy())\n",
    "\n",
    "# print()\n",
    "# plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_translator = TrainTranslator(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor)\n",
    "\n",
    "# Configure the loss and optimizer\n",
    "train_translator.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=MaskedLoss(),\n",
    ")\n",
    "\n",
    "# loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# train_translator = TrainTranslator(\n",
    "#     embedding_dim, units,\n",
    "#     input_text_processor=input_text_processor,\n",
    "#     output_text_processor=output_text_processor)\n",
    "\n",
    "# # Configure the loss and optimizer\n",
    "# train_translator.compile(\n",
    "#     optimizer=tf.optimizers.Adam(),\n",
    "#     loss=loss,\n",
    "#     metrics=['accuracy'],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLogs(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.logs = []\n",
    "\n",
    "    def on_train_batch_end(self, n, logs):\n",
    "        self.logs.append(logs[self.key])\n",
    "\n",
    "batch_loss = BatchLogs('batch_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "46/46 [==============================] - 140s 3s/step - batch_loss: 4.5901\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 152s 3s/step - batch_loss: 2.8263\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 152s 3s/step - batch_loss: 2.1858\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 155s 3s/step - batch_loss: 1.8859\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 156s 3s/step - batch_loss: 1.6298\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 156s 3s/step - batch_loss: 1.4095\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 160s 3s/step - batch_loss: 1.1777\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 163s 4s/step - batch_loss: 0.9738\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 183s 4s/step - batch_loss: 0.7967\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 434s 9s/step - batch_loss: 0.6378\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 469s 10s/step - batch_loss: 0.5073\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 493s 10s/step - batch_loss: 0.3956\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 433s 9s/step - batch_loss: 0.3143\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 436s 9s/step - batch_loss: 0.2482\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 434s 9s/step - batch_loss: 0.1920\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 471s 10s/step - batch_loss: 0.1535\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 501s 11s/step - batch_loss: 0.1132\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 503s 11s/step - batch_loss: 0.0873\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 18395s 409s/step - batch_loss: 0.0659\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 211s 5s/step - batch_loss: 0.0512\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 197s 4s/step - batch_loss: 0.0385\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 176s 4s/step - batch_loss: 0.0304\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 167s 4s/step - batch_loss: 0.0240\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 169s 4s/step - batch_loss: 0.0194\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 170s 4s/step - batch_loss: 0.0175\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 162s 4s/step - batch_loss: 0.0141\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 167s 4s/step - batch_loss: 0.0116\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 168s 4s/step - batch_loss: 0.0106\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 172s 4s/step - batch_loss: 0.0111\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 171s 4s/step - batch_loss: 0.0099\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - 171s 4s/step - batch_loss: 0.0106\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 169s 4s/step - batch_loss: 0.0122\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 170s 4s/step - batch_loss: 0.0213\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 162s 3s/step - batch_loss: 0.0488\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 165s 4s/step - batch_loss: 0.0782\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 163s 4s/step - batch_loss: 0.0783\n",
      "Epoch 37/50\n",
      "46/46 [==============================] - 166s 4s/step - batch_loss: 0.0563\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 165s 4s/step - batch_loss: 0.0377\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 166s 4s/step - batch_loss: 0.0240\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 162s 4s/step - batch_loss: 0.0131\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 165s 4s/step - batch_loss: 0.0088\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 176s 4s/step - batch_loss: 0.0056\n",
      "Epoch 43/50\n",
      "46/46 [==============================] - 174s 4s/step - batch_loss: 0.0045\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 177s 4s/step - batch_loss: 0.0045\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 180s 4s/step - batch_loss: 0.0038\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 181s 4s/step - batch_loss: 0.0027\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 173s 4s/step - batch_loss: 0.0026\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 168s 4s/step - batch_loss: 0.0021\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 171s 4s/step - batch_loss: 0.0023\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 169s 4s/step - batch_loss: 0.0021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23f15873048>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_translator.fit(dataset, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'CE/token')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAASFUlEQVR4nO3dfbRldV3H8fcnRuRBZUDHEXkQRJas0VLwLp8oU9AUUkFTly7LWYpNLa18yBK1lVqtAp+1WtYk2tDCR9Sgslo4gT2swgaEeJJmQlGIh1ERUBBFv/1x9vw43u7lnrn3nrPvzHm/1jrr7P3bv73P9+ddzoe99zm/napCkiSAn+i7AEnSymEoSJIaQ0GS1BgKkqTGUJAkNYaCJKkZWygk+XCSm5NcPtR2QJLzkmzt3vfv2pPkA0m2JfmvJMeMqy5J0vzGeabwl8CzZrWdCmyuqiOBzd06wAnAkd1rA/DBMdYlSZrH2EKhqv4Z+Nas5pOATd3yJuDkofYza+A/gNVJDhxXbZKkua2a8OetraobuuUbgbXd8kHA14f6Xde13cAsSTYwOJtg3333fdxRRx01vmolaTd00UUXfaOq1sy1bdKh0FRVJdnpOTaqaiOwEWBmZqa2bNmy7LVJ0u4sybXzbZv0t49u2nFZqHu/uWu/HjhkqN/BXZskaYImHQrnAuu75fXAOUPtL+u+hfRE4Nahy0ySpAkZ2+WjJB8Dngo8KMl1wFuB04BPJjkFuBZ4Udf9c8CJwDbgDuDl46pLkjS/sYVCVb1knk3Hz9G3gFePqxZJ0mj8RbMkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqegmFJK9LckWSy5N8LMleSQ5PcmGSbUk+kWTPPmqTpGk28VBIchDwG8BMVT0a2AN4MXA68N6qegRwC3DKpGuTpGnX1+WjVcDeSVYB+wA3AMcBZ3fbNwEn91OaJE2viYdCVV0PvAv4GoMwuBW4CPh2Vd3ddbsOOGiu/ZNsSLIlyZbt27dPomRJmhp9XD7aHzgJOBx4KLAv8KxR96+qjVU1U1Uza9asGVOVkjSd+rh89HTgK1W1vap+AHwGOBZY3V1OAjgYuL6H2iRpqvURCl8DnphknyQBjgeuBM4HXtD1WQ+c00NtkjTV+rincCGDG8oXA5d1NWwE3gi8Psk24IHAGZOuTZKm3aqFuyy/qnor8NZZzdcAj++hHElSx180S5IaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLU9BIKSVYnOTvJl5NcleRJSQ5Icl6Srd37/n3UJknTrK8zhfcD/1BVRwGPAa4CTgU2V9WRwOZuXZI0QRMPhST7AU8BzgCoqu9X1beBk4BNXbdNwMmTrk2Spl0fZwqHA9uBjyT5UpIPJdkXWFtVN3R9bgTWzrVzkg1JtiTZsn379gmVLEnToY9QWAUcA3ywqo4GvsusS0VVVUDNtXNVbayqmaqaWbNmzdiLlaRp0kcoXAdcV1UXdutnMwiJm5IcCNC939xDbZI01VaN2jHJk4HDhvepqjN39gOr6sYkX0/yyKq6GjgeuLJ7rQdO697P2dljS5KWZqRQSPJXwBHAJcAPu+YCdjoUOr8OnJVkT+Aa4OUMzlo+meQU4FrgRYs8tiRpkUY9U5gB1nXX+pesqi7pjjnb8ctxfEnS4ox6T+Fy4CHjLESS1L9RzxQeBFyZ5IvAXTsaq+q5Y6lKktSLUUPhbeMsQpK0MowUClX1hSQPA46sqs8n2QfYY7ylSZImbaR7Ckl+mcHvCf68azoI+Osx1SRJ6smoN5pfDRwL3AZQVVuBB4+rKElSP0YNhbuq6vs7VpKsYp5pKCRJu65RQ+ELSd4M7J3kGcCngL8ZX1mSpD6MGgqnMpjZ9DLgV4DPVdVbxlaVJKkXI38ltap+F/gLgCR7JDmrql46vtIkSZM26pnCIUneBNDNV/RpYOvYqpIk9WLUUHgF8JNdMPwt8IWqetvYqpIk9eJeLx8lOWZo9f0MfqfwbwxuPB9TVRePszhJ0mQtdE/h3bPWbwHWde0FHDeOoiRJ/bjXUKiqp02qEElS/0ad5mK/JO9JsqV7vTvJfuMuTpI0WaPeaP4wcDuDp6G9iMF0Fx8ZV1GSpH6M+juFI6rqF4bW357kkjHUI0nq0ahnCncm+ekdK0mOBe4cT0mSpL6Meqbwq8CZQ/cRbgHWj6ckSVJfRg2F26rqMUkeAFBVtyU5fIx1SZJ6MOrlo0/DIAyq6rau7ezxlCRJ6stCv2g+CngUsF+S5w9tegCw1zgLkyRN3kKXjx4JPBtYDTxnqP124JfHVJMkqScLhcI+wBuAjVX17xOoR5LUo4VC4VAGT1m7T5LNwN8DX6wqH8UpSbuhe73RXFWnV9VxwInApQym0L44yUeTvCzJ2kkUKUmajJG+klpVtwOf7V4kWQecAJwJPHNs1UmSJupezxSS/OLQ8rE7lqvqSuCuqjIQJGk3stDvFF4/tPzHs7a9YplrkST1bKFQyDzLc61LknZxC4VCzbM817okaRe30I3mo5L8F4OzgiO6Zbr1h4+1MknSxC0UCo8B1gJfn9V+CHDjWCqSJPVmoctH7wVuraprh1/Ard02SdJuZKFQWFtVl81u7NoOW8oHJ9kjyZeS/G23fniSC5NsS/KJJHsu5fiSpJ23UCisvpdtey/xs18DXDW0fjrw3qp6BIOH+JyyxONLknbSQqGwJcn/mw01ySuBixb7oUkOBn4e+FC3HuA47nlGwybg5MUeX5K0OAvdaH4t8NkkL+WeEJgB9gSet4TPfR/w28D9u/UHAt+uqru79euAg+baMckGYAPAoYceuoQSJEmzLTQh3k1V9WTg7cBXu9fbq+pJVbWobx8leTZwc1Ut6kyjqjZW1UxVzaxZs2Yxh5AkzWPUCfHOB85fps88FnhukhMZPL3tAcD7gdVJVnVnCwcD1y/T50mSRjTqM5qXTVW9qaoOrqrDgBcD/1RVL2UQOi/ouq0Hzpl0bZI07SYeCvfijcDrk2xjcI/hjJ7rkaSpM9Llo3GpqguAC7rla4DH91mPJE27lXSmIEnqmaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJaiYeCkkOSXJ+kiuTXJHkNV37AUnOS7K1e99/0rVJ0rTr40zhbuA3q2od8ETg1UnWAacCm6vqSGBzty5JmqCJh0JV3VBVF3fLtwNXAQcBJwGbum6bgJMnXZskTbte7ykkOQw4GrgQWFtVN3SbbgTWzrPPhiRbkmzZvn37ZAqVpCnRWygkuR/waeC1VXXb8LaqKqDm2q+qNlbVTFXNrFmzZgKVStL06CUUktyHQSCcVVWf6ZpvSnJgt/1A4OY+apOkadbHt48CnAFcVVXvGdp0LrC+W14PnDPp2iRp2q3q4TOPBX4JuCzJJV3bm4HTgE8mOQW4FnhRD7VJ0lSbeChU1b8CmWfz8ZOsRZL04/xFsySpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWpWVCgkeVaSq5NsS3Jq3/VI0rRZMaGQZA/gT4ETgHXAS5Ks67cqSZouKyYUgMcD26rqmqr6PvBx4KSea5KkqbKq7wKGHAR8fWj9OuAJszsl2QBs6Fa/k+TqCdS23B4EfKPvIiZs2sY8beMFx7wredh8G1ZSKIykqjYCG/uuYymSbKmqmb7rmKRpG/O0jRcc8+5iJV0+uh44ZGj94K5NkjQhKykU/hM4MsnhSfYEXgyc23NNkjRVVszlo6q6O8mvAf8I7AF8uKqu6LmscdmlL38t0rSNedrGC455t5Cq6rsGSdIKsZIuH0mSemYoSJIaQ2FMkhyQ5LwkW7v3/efpt77rszXJ+jm2n5vk8vFXvDRLGW+SfZL8XZIvJ7kiyWmTrX7nLDQdS5L7JvlEt/3CJIcNbXtT1351kmdOtPAlWOyYkzwjyUVJLuvej5t48Yu0lL9zt/3QJN9J8oaJFb0cqsrXGF7AO4BTu+VTgdPn6HMAcE33vn+3vP/Q9ucDHwUu73s84xwvsA/wtK7PnsC/ACf0PaZ5xrkH8D/Aw7taLwXWzerzKuDPuuUXA5/oltd1/e8LHN4dZ4++xzTmMR8NPLRbfjRwfd/jGfeYh7afDXwKeEPf49mZl2cK43MSsKlb3gScPEefZwLnVdW3quoW4DzgWQBJ7ge8HviD8Ze6LBY93qq6o6rOB6jBFCcXM/idyko0ynQsw/9bnA0cnyRd+8er6q6q+gqwrTveSrfoMVfVl6rqf7v2K4C9k9x3IlUvzVL+ziQ5GfgKgzHvUgyF8VlbVTd0yzcCa+foM9fUHgd1y78PvBu4Y2wVLq+ljheAJKuB5wCbx1DjclhwDMN9qupu4FbggSPuuxItZczDfgG4uKruGlOdy2nRY+7+g+6NwNsnUOeyWzG/U9gVJfk88JA5Nr1leKWqKsnI3/1N8ljgiKp63ezrlH0a13iHjr8K+Bjwgaq6ZnFVaiVK8ijgdODn+q5lAt4GvLeqvtOdOOxSDIUlqKqnz7ctyU1JDqyqG5IcCNw8R7frgacOrR8MXAA8CZhJ8lUGf6MHJ7mgqp5Kj8Y43h02Alur6n1Lr3ZsRpmOZUef67qg2w/45oj7rkRLGTNJDgY+C7ysqv5n/OUui6WM+QnAC5K8A1gN/CjJ96rqT8Ze9XLo+6bG7voC3smP33h9xxx9DmBw3XH/7vUV4IBZfQ5j17jRvKTxMrh38mngJ/oeywLjXMXgBvnh3HMD8lGz+ryaH78B+clu+VH8+I3ma9g1bjQvZcyru/7P73sckxrzrD5vYxe70dx7Abvri8H11M3AVuDzQ//4zQAfGur3CgY3HLcBL5/jOLtKKCx6vAz+K6yAq4BLutcr+x7TvYz1ROC/GXw75S1d2+8Bz+2W92LwrZNtwBeBhw/t+5Zuv6tZod+wWs4xA78DfHfo73oJ8OC+xzPuv/PQMXa5UHCaC0lS47ePJEmNoSBJagwFSVJjKEiSGkNBktQYClInyQ+TXJLk0iQXJ3nyAv1XJ3nVCMe9IMnID3dP8rHusbSvTfKSUfeTloOhIN3jzqp6bFU9BngT8EcL9F/NYKbM5XZYDSbM+1ngn8dwfGlehoI0twcAt8Bgxtokm7uzh8uS7Jgt8zTgiO7s4p1d3zd2fS6d9VyIFyb5YpL/TvIzc31gkrOSXAkcleQSBvME/V2SV45rkNJszn0k3WPv7h/jvYADgR0PhPke8Lyqui3Jg4D/SHIug+k8Hl1VjwVIcgKD6ZSfUFV3JDlg6NirqurxSU4E3gr8v3mkquqlSV4IHMpgKuZ3VdULxzFQaT6GgnSPO4f+gX8ScGaSRwMB/jDJU4AfMZgyea6pwZ8OfKSq7gCoqm8NbftM934Rg6lL5nMMg+lCforBfDvSRBkK0hyq6t+7s4I1DObAWQM8rqp+0M1eu9dOHnLHMwR+yBz/v+vOIP6QwQRsz+4+77tJjq+qpy1uFNLO856CNIckRzF4JOM3GUyJfHMXCE8DHtZ1ux24/9Bu5wEvT7JPd4zhy0f3qqo+BzyOweSHP8ngiV1HGwiaNM8UpHvsuKcAg0tG66vqh0nOAv4myWXAFuDLAFX1zST/luRy4O+r6re6ByRtSfJ94HPAm3fi848GLk2yJ3CfqrpteYYljc5ZUiVJjZePJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDX/B1+e7mZ3ei7cAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(batch_loss.logs)\n",
    "plt.ylim([0, 100])\n",
    "plt.xlabel('Batch #')\n",
    "plt.ylabel('CE/token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss.logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "    \n",
    "  def __init__(self, encoder, decoder, input_text_processor,\n",
    "               output_text_processor):\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.input_text_processor = input_text_processor\n",
    "    self.output_text_processor = output_text_processor\n",
    "\n",
    "    self.output_token_string_from_index = (\n",
    "        tf.keras.layers.StringLookup(\n",
    "            vocabulary=output_text_processor.get_vocabulary(),\n",
    "            mask_token='',\n",
    "            invert=True))\n",
    "\n",
    "    # The output should never generate padding, unknown, or start.\n",
    "    index_from_string = tf.keras.layers.StringLookup(\n",
    "        vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
    "    token_mask_ids = index_from_string(['', '[unk]', '[start]']).numpy()\n",
    "\n",
    "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
    "    token_mask[np.array(token_mask_ids)] = True\n",
    "    self.token_mask = token_mask\n",
    "\n",
    "    self.start_token = index_from_string(tf.constant('[start]'))\n",
    "    self.end_token = index_from_string(tf.constant('[end]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(\n",
    "    encoder=train_translator.encoder,\n",
    "    decoder=train_translator.decoder,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_text(self, result_tokens):\n",
    "    \n",
    "    result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
    "   \n",
    "\n",
    "    result_text = tf.strings.reduce_join(result_text_tokens,\n",
    "                                        axis=1, separator=' ')\n",
    "    \n",
    "\n",
    "    result_text = tf.strings.strip(result_text)\n",
    "    \n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "Translator.tokens_to_text = tokens_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'2007 pets_allowed_yn', b'technician_id stuid',\n",
       "       b'keyword; \"edmonton\"', b'ycard job', b'2003 \"howard\"'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_output_tokens = tf.random.uniform(\n",
    "    shape=[5, 2], minval=0, dtype=tf.int64,\n",
    "    maxval=output_text_processor.vocabulary_size())\n",
    "translator.tokens_to_text(example_output_tokens).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(self, logits, temperature):\n",
    "\n",
    "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
    "    logits = tf.where(self.token_mask, -np.inf, logits)\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        new_tokens = tf.argmax(logits, axis=-1)\n",
    "    else: \n",
    "        logits = tf.squeeze(logits, axis=1)\n",
    "        new_tokens = tf.random.categorical(logits/temperature,\n",
    "                                            num_samples=1)\n",
    "\n",
    "    return new_tokens\n",
    "\n",
    "Translator.sample = sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
       "array([[ 468],\n",
       "       [1165],\n",
       "       [2046],\n",
       "       [1803],\n",
       "       [1674]], dtype=int64)>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_logits = tf.random.normal([5, 1, output_text_processor.vocabulary_size()])\n",
    "example_output_tokens = translator.sample(example_logits, temperature=1.0)\n",
    "example_output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_unrolled(self,\n",
    "                       input_text, *,\n",
    "                       max_length=50,\n",
    "                       return_attention=True,\n",
    "                       temperature=1.0):\n",
    "  batch_size = tf.shape(input_text)[0]\n",
    "  input_tokens = self.input_text_processor(input_text)\n",
    "  enc_output, enc_state = self.encoder(input_tokens)\n",
    "\n",
    "  dec_state = enc_state\n",
    "  new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "\n",
    "  result_tokens = []\n",
    "  attention = []\n",
    "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "\n",
    "  for _ in range(max_length):\n",
    "    dec_input = DecoderInput(new_tokens=new_tokens,\n",
    "                             enc_output=enc_output,\n",
    "                             mask=(input_tokens!=0))\n",
    "\n",
    "    dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
    "\n",
    "    attention.append(dec_result.attention_weights)\n",
    "\n",
    "    new_tokens = self.sample(dec_result.logits, temperature)\n",
    "\n",
    "    # If a sequence produces an `end_token`, set it `done`\n",
    "    done = done | (new_tokens == self.end_token)\n",
    "    # Once a sequence is done it only produces 0-padding.\n",
    "    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
    "\n",
    "    # Collect the generated tokens\n",
    "    result_tokens.append(new_tokens)\n",
    "\n",
    "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "      break\n",
    "\n",
    "  # Convert the list of generates token ids to a list of strings.\n",
    "  result_tokens = tf.concat(result_tokens, axis=-1)\n",
    "  result_text = self.tokens_to_text(result_tokens)\n",
    "\n",
    "  if return_attention:\n",
    "    attention_stack = tf.concat(attention, axis=1)\n",
    "    return {'text': result_text, 'attention': attention_stack}\n",
    "  else:\n",
    "    return {'text': result_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "Translator.translate = translate_unrolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select count ( * ) from wrestler\n",
      "select sum ( gamesplayed ) from sportsinfo\n",
      "\n",
      "Wall time: 192 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_text = tf.constant([\n",
    "    'How many singers do we have?', # \"It's really cold here.\"\n",
    "    'What is the total number of singers?', # \"This is my life.\"\"\n",
    "])\n",
    "\n",
    "result = translator.translate(\n",
    "    input_text = input_text)\n",
    "\n",
    "print(result['text'][0].numpy().decode())\n",
    "print(result['text'][1].numpy().decode())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "24c7168f742075337832b93c1fbfd4355e111387279ce758eae35d9b42cad7f4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
