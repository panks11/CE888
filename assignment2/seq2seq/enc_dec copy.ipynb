{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "run_logdir = get_run_logdir()\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/\n",
    "# https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/custom/15844304.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Train_Data():\n",
    "    \n",
    "    '''\n",
    "    Train_Data class Loads the data from multiple Training JSON files in Pandas Dataframes\n",
    "    '''\n",
    "   \n",
    "    def __init__(self, path , filenames):\n",
    "        '''\n",
    "            Parameters\n",
    "            ------------\n",
    "            path: directory where English to SQL translation JSON are placed\n",
    "            filenames: Json file names containing Train data\n",
    "        '''\n",
    "        self.path = path\n",
    "        self.filenames = filenames\n",
    "        self.df_train = pd.DataFrame()\n",
    "        for f in self.filenames:\n",
    "            print(\"Reading file at path\", self.path+f)\n",
    "            try:\n",
    "                df = pd.read_json(self.path + f)\n",
    "                if len(self.df_train) == 0:\n",
    "                    self.df_train = df\n",
    "                else:\n",
    "                    self.df_train = self.df_train.append(df)\n",
    "                print(\"{} Rows in Total\".format(len(self.df_train)))\n",
    "            except Exception as e:\n",
    "                print(\"Got error while Reading file : \" , e)\n",
    "                \n",
    "    @property          \n",
    "    def questions(self):\n",
    "        ''' \n",
    "        Returns\n",
    "        ------------\n",
    "        Returns English Questions in Dataframe Rows as List\n",
    "        '''\n",
    "        return self.df_train.question.values.tolist()\n",
    "    \n",
    "    @property\n",
    "    def sql(self):\n",
    "        '''\n",
    "        Returns\n",
    "        ------------\n",
    "        Returns SQL in Dataframe Rows as List\n",
    "        '''\n",
    "        return self.df_train['query'].values.tolist()\n",
    "    \n",
    "    @property          \n",
    "    def question_tokens(self):\n",
    "        ''' \n",
    "        Returns\n",
    "        ------------\n",
    "        Returns English Question Tokens in Dataframe Rows as List\n",
    "        '''\n",
    "        \n",
    "        return self.df_train['question_toks'].values.tolist()\n",
    "    \n",
    "    @property\n",
    "    def sql_tokens(self):\n",
    "        ''' \n",
    "        Returns\n",
    "        ------------\n",
    "        Returns SQL Query Tokens in Dataframe Rows as List\n",
    "        '''\n",
    "        return self.df_train['query_toks'].values.tolist()\n",
    "    \n",
    "    def get_special_characters(self,list_of_text):\n",
    "        '''\n",
    "        Parameters\n",
    "        ------------\n",
    "        list_of_text: Input List of Text \n",
    "        Returns\n",
    "        ------------\n",
    "        Provides list of Special Characters in the text\n",
    "        '''\n",
    "        return list(set(Preprocess.special_char(''.join([''.join(ele) for ele in list_of_text]))))\n",
    "    \n",
    "    def get_vocab_size(self, list_of_text):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------\n",
    "        list_of_text: Input List of Text \n",
    "        \n",
    "        Returns\n",
    "        ------------\n",
    "        Vocabulary size or unique words in the corpus\n",
    "        \"\"\"\n",
    "        word_list = []\n",
    "        for sentence in list_of_text:\n",
    "            for word in sentence.split():\n",
    "                word = word.lower().strip()\n",
    "                if word not in word_list:\n",
    "                    word_list.append(word)\n",
    "        return len(word_list), word_list\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#################################\n",
    "#CONSTANTS\n",
    "\n",
    "EOS = '[end]'\n",
    "SOS = '[start]'  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#################################################################################################################\n",
    "    \n",
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "class Preprocess():\n",
    "    \n",
    "    '''\n",
    "    Preprocess class cleans and standardize the data, add SOS-EOS tags to the data\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,x , text):\n",
    "        \"\"\"\n",
    "            Parameters\n",
    "            ------------\n",
    "            text : Input string\n",
    "            Runs the text processing steps\n",
    "            \n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.processed_text = self.run_pipeline(text)\n",
    "    \n",
    "    def text_standardize(self, text):\n",
    "        \"\"\"\n",
    "            Parameters\n",
    "            ------------\n",
    "            text : Input string\n",
    "            \n",
    "            Returns\n",
    "            ------------\n",
    "            -   Unicode normalization using NFKD method\n",
    "            -   Lower Case text\n",
    "        \n",
    "        \"\"\"\n",
    "        # text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "        text = tf.strings.lower(text)\n",
    "        return text\n",
    "    \n",
    "    def text_whitespace(self, text):\n",
    "        \"\"\"\n",
    "            Parameters\n",
    "            ------------\n",
    "            text : Input string\n",
    "            \n",
    "            Returns\n",
    "            ------------\n",
    "            -   Remove $ and \\\\ special characters \n",
    "            -   Add space around punctations\n",
    "            -   Remove spaces around sentences\n",
    "        \n",
    "        \"\"\"\n",
    "        text = tf.strings.regex_replace(text, '[$\\\\\\\\]', '')\n",
    "        text = tf.strings.regex_replace(text, '[.?!,Â¿()*:@]', r' \\0 ')\n",
    "        text = tf.strings.strip(text)\n",
    "        return text\n",
    "    \n",
    "    def add_SOS_EOS(self, text):\n",
    "        \"\"\"\n",
    "            Parameters\n",
    "            ------------\n",
    "            text : Input string\n",
    "            \n",
    "            Returns\n",
    "            ------------\n",
    "            -   Add <SOS> and <EOS> tags to each sentence\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.x == 'i':\n",
    "            return text\n",
    "        if self.x == 'o':\n",
    "            text = tf.strings.join([text, EOS], separator=' ')\n",
    "            return text\n",
    "        if self.x == 's_o':\n",
    "            text = tf.strings.join([SOS,text], separator=' ')\n",
    "            return text\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    @classmethod\n",
    "    def special_char(cls, text):\n",
    "        \"\"\"\n",
    "            Parameters\n",
    "            ------------\n",
    "            text : Input string\n",
    "            \n",
    "            Returns\n",
    "            ------------\n",
    "            -   Special Characters found in Text using Regular Expression\n",
    "        \"\"\"\n",
    "        return re.findall(r'[\\W]',text.replace(' ',''))\n",
    "    \n",
    "    \n",
    "    def run_pipeline(self,text):\n",
    "        \"\"\"\n",
    "            Parameters\n",
    "            ------------\n",
    "            text : Input string\n",
    "            \n",
    "            Returns\n",
    "            ------------\n",
    "            Executes series of Text pre processing functions\n",
    "        \n",
    "        \"\"\"\n",
    "        text = self.text_standardize(text)\n",
    "        text = self.text_whitespace(text)\n",
    "        text = self.add_SOS_EOS(text)\n",
    "        self.text = text\n",
    "        return self.text\n",
    "    \n",
    "    \n",
    "class Features():\n",
    "    '''\n",
    "    Extracts text features from data\n",
    "    '''\n",
    "    def __init__(self,x):\n",
    "        self.x = x\n",
    "        \n",
    "    def tf_lower_and_split_punct(self,text):\n",
    "        \"\"\"\n",
    "            Parameters\n",
    "            ------------\n",
    "            text : Input string\n",
    "            \n",
    "            Returns\n",
    "            ------------\n",
    "            Standardized Text\n",
    "        \n",
    "        \"\"\"\n",
    "        return Preprocess(self.x, text).processed_text\n",
    "        \n",
    "    def vectorizor(self, document, max_vocab_size):\n",
    "        \"\"\"\n",
    "            Parameters\n",
    "            ------------\n",
    "            document : Collection of sentences\n",
    "            max_vocab_size : No of words in document used for TextVectorization\n",
    "            \n",
    "            Returns\n",
    "            ------------\n",
    "            TextVectorization object\n",
    "        \n",
    "        \"\"\"\n",
    "        text_processor = tf.keras.layers.TextVectorization( standardize = self.tf_lower_and_split_punct, max_tokens = max_vocab_size)\n",
    "        text_processor.adapt(document)\n",
    "        print(\"Sample Vocabulary\",text_processor.get_vocabulary()[:10])\n",
    "        return text_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file at path D:/DS/Learnin/Essex MS/CE888/git/CE888/assignment2/seq2seq/spider/train_spider.json\n",
      "7000 Rows in Total\n",
      "Reading file at path D:/DS/Learnin/Essex MS/CE888/git/CE888/assignment2/seq2seq/spider/train_others.json\n",
      "8659 Rows in Total\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SELECT count(*) FROM head WHERE age  >  56'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    del(o)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "o = Train_Data('D:/DS/Learnin/Essex MS/CE888/git/CE888/assignment2/seq2seq/spider/',['train_spider.json','train_others.json'])\n",
    "\n",
    "o.sql[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Vocabulary ['', '[UNK]', 'the', 'of', '?', '.', 'what', 'are', 'and', 'in']\n",
      "Sample Vocabulary ['', '[UNK]', '.', 't1', 't2', '=', 'select', 'from', 'as', '[END]']\n",
      "Sample Vocabulary ['', '[UNK]', '.', 't1', 't2', '=', 'select', 'from', 'as', '[START]']\n"
     ]
    }
   ],
   "source": [
    "input_text_processor = Features('i').vectorizor(o.questions , 7000)\n",
    "output_text_processor = Features('o').vectorizor(o.sql , 7000)\n",
    "output_shifted_text_processor = Features('s_o').vectorizor(o.sql , 7000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_input_len = input_text_processor(o.questions[:500]).shape[-1]\n",
    "max_input_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_out_len = output_text_processor(o.sql[:500]).shape[-1]\n",
    "max_out_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "emb = 'D:/DS/Learnin/Essex MS/CE888/embeddings/glove.6B.100d.txt'\n",
    "glove_file = open(emb, encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = input_text_processor.vocabulary_size()\n",
    "EMBEDDING_SIZE = 100\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_SIZE))\n",
    "for index,word in enumerate(input_text_processor.get_vocabulary()):\n",
    "    # if word:\n",
    "        embedding_vector = embeddings_dictionary.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "import numpy as np\n",
    "\n",
    "embedding_layer = Embedding(num_words, EMBEDDING_SIZE, weights=[embedding_matrix], input_length=max_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 80, 3821)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_of_input_sentences = len(o.questions[:500])\n",
    "num_words_output = output_text_processor.vocabulary_size()+1\n",
    "############################\n",
    "decoder_targets_one_hot = np.zeros((\n",
    "        len_of_input_sentences,\n",
    "        max_out_len,\n",
    "        num_words_output\n",
    "    ),\n",
    "    dtype='float32'\n",
    ")\n",
    "decoder_targets_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_NODES =256\n",
    "encoder_inputs_placeholder = Input(shape=(max_input_len,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = LSTM(LSTM_NODES, return_state=True)\n",
    "\n",
    "encoder_outputs, h, c = encoder(x)\n",
    "encoder_states = [h, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs_placeholder = Input(shape=(max_out_len,))\n",
    "\n",
    "decoder_embedding = Embedding(num_words_output, LSTM_NODES)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "decoder_lstm = LSTM(LSTM_NODES, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs_placeholder,\n",
    "  decoder_inputs_placeholder], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model_plot4a.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in c:\\users\\pankhuri\\.conda\\envs\\tf\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\pankhuri\\.conda\\envs\\tf\\lib\\site-packages (from pydot) (3.0.8)\n"
     ]
    }
   ],
   "source": [
    "# !pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=output_shifted_text_processor(o.sql[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(d):\n",
    "    # print(i)\n",
    "    for t, word in enumerate(d):\n",
    "        decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 16s 2s/step - loss: 5.9967 - accuracy: 0.1168 - val_loss: 3.4204 - val_accuracy: 0.8480\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 17s 2s/step - loss: 2.7025 - accuracy: 0.7618 - val_loss: 1.5185 - val_accuracy: 0.8480\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.7776 - accuracy: 0.7618 - val_loss: 1.3719 - val_accuracy: 0.8480\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.6164 - accuracy: 0.7635 - val_loss: 1.0992 - val_accuracy: 0.8480\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.4207 - accuracy: 0.7653 - val_loss: 0.9478 - val_accuracy: 0.8605\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 18s 2s/step - loss: 1.3050 - accuracy: 0.7743 - val_loss: 0.8658 - val_accuracy: 0.8605\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.2202 - accuracy: 0.7771 - val_loss: 0.8333 - val_accuracy: 0.8765\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.0983 - accuracy: 0.7849 - val_loss: 0.8274 - val_accuracy: 0.8813\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 19s 2s/step - loss: 1.0212 - accuracy: 0.7940 - val_loss: 0.8045 - val_accuracy: 0.8792\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 18s 2s/step - loss: 0.9792 - accuracy: 0.8068 - val_loss: 0.7761 - val_accuracy: 0.8850\n"
     ]
    }
   ],
   "source": [
    "r = model.fit(\n",
    "    [input_text_processor(o.questions[:500]), output_shifted_text_processor(o.sql[:500])],\n",
    "    decoder_targets_one_hot,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs_placeholder, encoder_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(LSTM_NODES,))\n",
    "decoder_state_input_c = Input(shape=(LSTM_NODES,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_states = [h, c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = Model(\n",
    "    [decoder_inputs_single] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9, 1]], dtype=int64)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_shifted_text_processor(['<sos>']).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = 1\n",
    "    eos = 9\n",
    "    output_sentence = []\n",
    "\n",
    "    for _ in range(max_out_len):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "        if eos == 10:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "\n",
    "        if idx > 0:\n",
    "            word = output_text_processor.get_vocabulary()[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "        target_seq[0, 0] = idx\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([9, 1], dtype=int64)>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_shifted_text_processor(EOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# i=input_text_processor(o.questions[:500])[2]\n",
    "# translation = translate_sentence(i)\n",
    "# print('-')\n",
    "# print('Input:', o.questions[2])\n",
    "# print('Response:', translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input: Show names for all aircrafts with distances more than the average.\n",
      "Response: [END] select select from from from from from from from\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(len(o.questions[:500]))\n",
    "input_seq = input_text_processor(o.questions[:500])[i:i+1]\n",
    "translation = translate_sentence(input_seq)\n",
    "print('-')\n",
    "print('Input:', o.questions[i])\n",
    "print('Response:', translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 79), dtype=int64, numpy=\n",
       "array([[ 783,   57,  191,    1,  190,   58, 1471,   94,   86, 3713, 3721,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0]], dtype=int64)>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_processor(o.sql[:500])[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(27,), dtype=int64, numpy=\n",
       "array([  30,    2, 1492,   64,   21,   12,    8,  267,    3,   29,   68,\n",
       "          5,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0], dtype=int64)>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_processor(o.questions[:500])[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LSTM_NODES =256\n",
    "NUM_SENTENCES = 20000\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = [x.decode('utf-8') for x in list(Preprocess('i',o.questions[:500]).processed_text.numpy())]\n",
    "output_sentences = [x.decode('utf-8') for x in list(Preprocess('o',o.sql[:500]).processed_text.numpy())]\n",
    "output_sentences_inputs = [x.decode('utf-8') for x in list(Preprocess('s_o',o.sql[:500]).processed_text.numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are the names of all stations that have more than 10 bikes available and are not located in san jose ?\n",
      "select t1 . name from station as t1 join status as t2 on t1 . id  =  t2 . station_id group by t2 . station_id having avg ( bikes_available )   >  10 except select name from station where city  =  \"san jose\" [end]\n",
      "[start] select t1 . name from station as t1 join status as t2 on t1 . id  =  t2 . station_id group by t2 . station_id having avg ( bikes_available )   >  10 except select name from station where city  =  \"san jose\"\n"
     ]
    }
   ],
   "source": [
    "print(input_sentences[172])\n",
    "print(output_sentences[172])\n",
    "print(output_sentences_inputs[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what are the names of all stations that have more than 10 bikes available and are not located in san jose ?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentences[172]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples input: 500\n",
      "num samples output: 500\n",
      "num samples output input: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"num samples input:\", len(input_sentences))\n",
    "print(\"num samples output:\", len(output_sentences))\n",
    "print(\"num samples output input:\", len(output_sentences_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the input: 559\n",
      "Length of longest sentence in input: 26\n"
     ]
    }
   ],
   "source": [
    "input_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "input_tokenizer.fit_on_texts(input_sentences)\n",
    "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
    "\n",
    "word2idx_inputs = input_tokenizer.word_index\n",
    "print('Total unique words in the input: %s' % len(word2idx_inputs))\n",
    "\n",
    "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
    "print(\"Length of longest sentence in input: %g\" % max_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the output: 334\n",
      "Length of longest sentence in the output: 80\n"
     ]
    }
   ],
   "source": [
    "output_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "output_tokenizer.fit_on_texts(output_sentences + output_sentences_inputs)\n",
    "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
    "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
    "\n",
    "word2idx_outputs = output_tokenizer.word_index\n",
    "print('Total unique words in the output: %s' % len(word2idx_outputs))\n",
    "\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
    "print(\"Length of longest sentence in the output: %g\" % max_out_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_sequences.shape: (500, 26)\n",
      "encoder_input_sequences[172]: [  0   0   0   0   0   3   4   1  12   2   7  34   9  11  27  18  97  94\n",
      " 107   5   4  41 120  10 138 233]\n"
     ]
    }
   ],
   "source": [
    "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
    "print(\"encoder_input_sequences.shape:\", encoder_input_sequences.shape)\n",
    "print(\"encoder_input_sequences[172]:\", encoder_input_sequences[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input_sequences.shape: (500, 80)\n",
      "decoder_input_sequences[172]: [ 12   2   4   1  19   3  32   9   4  16  40   9   5  18   4   1  31   6\n",
      "   5   1  50  20  10   5   1  50  37  29   7  79   8  25  98  95   2  19\n",
      "   3  32  13  26   6 121 184   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_input_sequences.shape:\", decoder_input_sequences.shape)\n",
    "print(\"decoder_input_sequences[172]:\", decoder_input_sequences[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx_outputs['[start]']\n",
    "word2idx_outputs['[end]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_SIZE))\n",
    "for word, index in word2idx_inputs.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.73132998 -0.66733998  0.74194002 -0.18158001  0.29214999  0.43759999\n",
      "  0.23856001 -0.24066    -0.23841    -0.38927001  1.59239995 -0.063928\n",
      "  0.54516     0.43784001 -0.70384002  0.28386     1.41639996 -0.093074\n",
      "  0.17941999  0.38483    -0.025013    0.03748     0.20721     0.12104\n",
      " -0.69294    -0.13064     0.30320999 -0.49383     0.15166    -0.19228999\n",
      "  0.71739    -0.18375     0.15871     1.19599998 -0.37094    -0.069602\n",
      " -0.57428998 -0.0034853  -0.2502     -0.63968998 -0.0029819  -0.11249\n",
      " -0.29874     0.073009   -0.16331001 -0.91198999  0.14664    -0.82380998\n",
      "  0.71890998 -0.53586     0.81794    -0.085717    0.06512     0.75410998\n",
      "  0.72373998 -2.01740003 -0.18441001  0.12034     1.44529998  0.95161998\n",
      " -0.21149001  0.63142002 -0.11256     0.10271     0.5273     -0.58968002\n",
      "  0.37007999  0.39436001  1.00010002 -0.40781999 -0.10999     0.52677\n",
      " -0.19093999  0.059869    0.18929     0.19038001 -0.089262   -0.50409001\n",
      " -0.89447999 -0.40553001  0.65720999  0.26298001  0.42081001  0.22235\n",
      " -1.00450003  0.17151999  0.38111001 -0.96912003 -0.13902     1.34990001\n",
      " -0.41690999 -0.06715    -0.25654    -0.037228   -0.056948   -0.1666\n",
      " -0.69937998 -0.011797    0.53068    -0.83100998]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix[539])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words, EMBEDDING_SIZE, weights=[embedding_matrix], input_length=max_input_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_targets_one_hot = np.zeros((\n",
    "        len(input_sentences),\n",
    "        max_out_len,\n",
    "        num_words_output\n",
    "    ),\n",
    "    dtype='float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 80, 335)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_targets_one_hot.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(output_input_integer_seq):\n",
    "    for t, word in enumerate(d):\n",
    "        decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs_placeholder = Input(shape=(max_input_len,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = LSTM(LSTM_NODES, return_state=True)\n",
    "\n",
    "encoder_outputs, h, c = encoder(x)\n",
    "encoder_states = [h, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs_placeholder = Input(shape=(max_out_len,))\n",
    "\n",
    "decoder_embedding = Embedding(num_words_output, LSTM_NODES)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "decoder_lstm = LSTM(LSTM_NODES, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs_placeholder,\n",
    "  decoder_inputs_placeholder], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 2s 299ms/step - loss: 0.2797 - accuracy: 0.1996 - val_loss: 0.3117 - val_accuracy: 0.1060\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 2s 303ms/step - loss: 0.2741 - accuracy: 0.2006 - val_loss: 0.3081 - val_accuracy: 0.1065\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 2s 281ms/step - loss: 0.2658 - accuracy: 0.2015 - val_loss: 0.3347 - val_accuracy: 0.1055\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 2s 280ms/step - loss: 0.2628 - accuracy: 0.2023 - val_loss: 0.2985 - val_accuracy: 0.1067\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 2s 275ms/step - loss: 0.2496 - accuracy: 0.2047 - val_loss: 0.3020 - val_accuracy: 0.1072\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 2s 279ms/step - loss: 0.2452 - accuracy: 0.2060 - val_loss: 0.3132 - val_accuracy: 0.1060\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 2s 284ms/step - loss: 0.2417 - accuracy: 0.2071 - val_loss: 0.3120 - val_accuracy: 0.1067\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 2s 303ms/step - loss: 0.2341 - accuracy: 0.2080 - val_loss: 0.3028 - val_accuracy: 0.1070\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 3s 320ms/step - loss: 0.2284 - accuracy: 0.2092 - val_loss: 0.2961 - val_accuracy: 0.1075\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 3s 335ms/step - loss: 0.2224 - accuracy: 0.2101 - val_loss: 0.2984 - val_accuracy: 0.1085\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 3s 349ms/step - loss: 0.2178 - accuracy: 0.2115 - val_loss: 0.3004 - val_accuracy: 0.1080\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 3s 346ms/step - loss: 0.2162 - accuracy: 0.2120 - val_loss: 0.2930 - val_accuracy: 0.1088\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 3s 345ms/step - loss: 0.2087 - accuracy: 0.2142 - val_loss: 0.2856 - val_accuracy: 0.1080\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 3s 342ms/step - loss: 0.2046 - accuracy: 0.2150 - val_loss: 0.2907 - val_accuracy: 0.1088\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 2s 292ms/step - loss: 0.1999 - accuracy: 0.2159 - val_loss: 0.2950 - val_accuracy: 0.1085\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 2s 287ms/step - loss: 0.1968 - accuracy: 0.2167 - val_loss: 0.2852 - val_accuracy: 0.1105\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 2s 296ms/step - loss: 0.1925 - accuracy: 0.2174 - val_loss: 0.2902 - val_accuracy: 0.1100\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 3s 315ms/step - loss: 0.1905 - accuracy: 0.2177 - val_loss: 0.2794 - val_accuracy: 0.1110\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 2s 309ms/step - loss: 0.1827 - accuracy: 0.2190 - val_loss: 0.2827 - val_accuracy: 0.1098\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 2s 302ms/step - loss: 0.1812 - accuracy: 0.2191 - val_loss: 0.2787 - val_accuracy: 0.1110\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 2s 283ms/step - loss: 0.1767 - accuracy: 0.2203 - val_loss: 0.2889 - val_accuracy: 0.1098\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 2s 280ms/step - loss: 0.1771 - accuracy: 0.2201 - val_loss: 0.2787 - val_accuracy: 0.1123\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 2s 284ms/step - loss: 0.1704 - accuracy: 0.2212 - val_loss: 0.2728 - val_accuracy: 0.1115\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 2s 272ms/step - loss: 0.1675 - accuracy: 0.2215 - val_loss: 0.2959 - val_accuracy: 0.1098\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 2s 278ms/step - loss: 0.1668 - accuracy: 0.2229 - val_loss: 0.2764 - val_accuracy: 0.1123\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 2s 289ms/step - loss: 0.1627 - accuracy: 0.2239 - val_loss: 0.2819 - val_accuracy: 0.1112\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 2s 297ms/step - loss: 0.1604 - accuracy: 0.2246 - val_loss: 0.2785 - val_accuracy: 0.1138\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 2s 294ms/step - loss: 0.1582 - accuracy: 0.2251 - val_loss: 0.2826 - val_accuracy: 0.1133\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 2s 296ms/step - loss: 0.1554 - accuracy: 0.2254 - val_loss: 0.2687 - val_accuracy: 0.1163\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 2s 300ms/step - loss: 0.1504 - accuracy: 0.2261 - val_loss: 0.2681 - val_accuracy: 0.1152\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 2s 295ms/step - loss: 0.1492 - accuracy: 0.2265 - val_loss: 0.2650 - val_accuracy: 0.1165\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 2s 299ms/step - loss: 0.1459 - accuracy: 0.2270 - val_loss: 0.2708 - val_accuracy: 0.1160\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 3s 349ms/step - loss: 0.1446 - accuracy: 0.2271 - val_loss: 0.2649 - val_accuracy: 0.1163\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 3s 382ms/step - loss: 0.1407 - accuracy: 0.2273 - val_loss: 0.2627 - val_accuracy: 0.1180\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 3s 384ms/step - loss: 0.1418 - accuracy: 0.2275 - val_loss: 0.2597 - val_accuracy: 0.1173\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 3s 318ms/step - loss: 0.1361 - accuracy: 0.2280 - val_loss: 0.2660 - val_accuracy: 0.1163\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 2s 312ms/step - loss: 0.1377 - accuracy: 0.2280 - val_loss: 0.2592 - val_accuracy: 0.1177\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 3s 320ms/step - loss: 0.1320 - accuracy: 0.2283 - val_loss: 0.2611 - val_accuracy: 0.1182\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 3s 316ms/step - loss: 0.1299 - accuracy: 0.2283 - val_loss: 0.2599 - val_accuracy: 0.1192\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 3s 312ms/step - loss: 0.1299 - accuracy: 0.2286 - val_loss: 0.2582 - val_accuracy: 0.1187\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 3s 311ms/step - loss: 0.1263 - accuracy: 0.2289 - val_loss: 0.2629 - val_accuracy: 0.1192\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 3s 314ms/step - loss: 0.1273 - accuracy: 0.2293 - val_loss: 0.2605 - val_accuracy: 0.1192\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 3s 316ms/step - loss: 0.1226 - accuracy: 0.2296 - val_loss: 0.2585 - val_accuracy: 0.1187\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 3s 315ms/step - loss: 0.1229 - accuracy: 0.2297 - val_loss: 0.2690 - val_accuracy: 0.1190\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 3s 324ms/step - loss: 0.1211 - accuracy: 0.2299 - val_loss: 0.2645 - val_accuracy: 0.1190\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 3s 316ms/step - loss: 0.1209 - accuracy: 0.2302 - val_loss: 0.2602 - val_accuracy: 0.1190\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 2s 312ms/step - loss: 0.1176 - accuracy: 0.2304 - val_loss: 0.2611 - val_accuracy: 0.1192\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 2s 307ms/step - loss: 0.1158 - accuracy: 0.2303 - val_loss: 0.2588 - val_accuracy: 0.1195\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 3s 311ms/step - loss: 0.1191 - accuracy: 0.2303 - val_loss: 0.2545 - val_accuracy: 0.1198\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 3s 326ms/step - loss: 0.1127 - accuracy: 0.2310 - val_loss: 0.2530 - val_accuracy: 0.1198\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 3s 326ms/step - loss: 0.1106 - accuracy: 0.2309 - val_loss: 0.2510 - val_accuracy: 0.1203\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 2s 301ms/step - loss: 0.1092 - accuracy: 0.2312 - val_loss: 0.2501 - val_accuracy: 0.1205\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 2s 292ms/step - loss: 0.1091 - accuracy: 0.2313 - val_loss: 0.2748 - val_accuracy: 0.1185\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 2s 292ms/step - loss: 0.1113 - accuracy: 0.2313 - val_loss: 0.2546 - val_accuracy: 0.1203\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 2s 298ms/step - loss: 0.1057 - accuracy: 0.2316 - val_loss: 0.2553 - val_accuracy: 0.1200\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 3s 317ms/step - loss: 0.1078 - accuracy: 0.2313 - val_loss: 0.2537 - val_accuracy: 0.1200\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 3s 352ms/step - loss: 0.1036 - accuracy: 0.2318 - val_loss: 0.2511 - val_accuracy: 0.1205\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 3s 377ms/step - loss: 0.1036 - accuracy: 0.2318 - val_loss: 0.2510 - val_accuracy: 0.1203\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 3s 302ms/step - loss: 0.1008 - accuracy: 0.2319 - val_loss: 0.2492 - val_accuracy: 0.1208\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 3s 311ms/step - loss: 0.1007 - accuracy: 0.2319 - val_loss: 0.2526 - val_accuracy: 0.1205\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 2s 298ms/step - loss: 0.1021 - accuracy: 0.2320 - val_loss: 0.2465 - val_accuracy: 0.1208\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 2s 302ms/step - loss: 0.0984 - accuracy: 0.2321 - val_loss: 0.2512 - val_accuracy: 0.1205\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 2s 298ms/step - loss: 0.0968 - accuracy: 0.2322 - val_loss: 0.2483 - val_accuracy: 0.1205\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 2s 298ms/step - loss: 0.0952 - accuracy: 0.2323 - val_loss: 0.2419 - val_accuracy: 0.1208\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 2s 298ms/step - loss: 0.0968 - accuracy: 0.2324 - val_loss: 0.2514 - val_accuracy: 0.1205\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 2s 296ms/step - loss: 0.0939 - accuracy: 0.2326 - val_loss: 0.2568 - val_accuracy: 0.1205\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 2s 301ms/step - loss: 0.0938 - accuracy: 0.2327 - val_loss: 0.2473 - val_accuracy: 0.1205\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 2s 296ms/step - loss: 0.0921 - accuracy: 0.2328 - val_loss: 0.2529 - val_accuracy: 0.1205\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 2s 295ms/step - loss: 0.0924 - accuracy: 0.2330 - val_loss: 0.2572 - val_accuracy: 0.1205\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 2s 294ms/step - loss: 0.0917 - accuracy: 0.2329 - val_loss: 0.2494 - val_accuracy: 0.1205\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 2s 298ms/step - loss: 0.0895 - accuracy: 0.2332 - val_loss: 0.2489 - val_accuracy: 0.1205\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 2s 298ms/step - loss: 0.0896 - accuracy: 0.2333 - val_loss: 0.2501 - val_accuracy: 0.1205\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 2s 294ms/step - loss: 0.0883 - accuracy: 0.2332 - val_loss: 0.2521 - val_accuracy: 0.1205\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 2s 296ms/step - loss: 0.0871 - accuracy: 0.2334 - val_loss: 0.2455 - val_accuracy: 0.1205\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 2s 309ms/step - loss: 0.0916 - accuracy: 0.2333 - val_loss: 0.2443 - val_accuracy: 0.1210\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 2s 293ms/step - loss: 0.0858 - accuracy: 0.2336 - val_loss: 0.2462 - val_accuracy: 0.1210\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 2s 291ms/step - loss: 0.0846 - accuracy: 0.2336 - val_loss: 0.2411 - val_accuracy: 0.1213\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 2s 293ms/step - loss: 0.0841 - accuracy: 0.2337 - val_loss: 0.2337 - val_accuracy: 0.1215\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 2s 292ms/step - loss: 0.0861 - accuracy: 0.2339 - val_loss: 0.2433 - val_accuracy: 0.1210\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 2s 301ms/step - loss: 0.0828 - accuracy: 0.2341 - val_loss: 0.2470 - val_accuracy: 0.1210\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 3s 330ms/step - loss: 0.0821 - accuracy: 0.2342 - val_loss: 0.2400 - val_accuracy: 0.1213\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 3s 357ms/step - loss: 0.0823 - accuracy: 0.2344 - val_loss: 0.2435 - val_accuracy: 0.1210\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 3s 370ms/step - loss: 0.0822 - accuracy: 0.2346 - val_loss: 0.2471 - val_accuracy: 0.1210\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 3s 358ms/step - loss: 0.0806 - accuracy: 0.2346 - val_loss: 0.2478 - val_accuracy: 0.1210\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 3s 337ms/step - loss: 0.0797 - accuracy: 0.2347 - val_loss: 0.2500 - val_accuracy: 0.1210\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 2s 298ms/step - loss: 0.0798 - accuracy: 0.2347 - val_loss: 0.2459 - val_accuracy: 0.1210\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 2s 298ms/step - loss: 0.0784 - accuracy: 0.2348 - val_loss: 0.2402 - val_accuracy: 0.1210\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 2s 303ms/step - loss: 0.0807 - accuracy: 0.2348 - val_loss: 0.2418 - val_accuracy: 0.1210\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 3s 316ms/step - loss: 0.0781 - accuracy: 0.2350 - val_loss: 0.2437 - val_accuracy: 0.1210\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 2s 302ms/step - loss: 0.0767 - accuracy: 0.2350 - val_loss: 0.2427 - val_accuracy: 0.1210\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 2s 294ms/step - loss: 0.0760 - accuracy: 0.2352 - val_loss: 0.2483 - val_accuracy: 0.1210\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 2s 299ms/step - loss: 0.0770 - accuracy: 0.2352 - val_loss: 0.2388 - val_accuracy: 0.1210\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 2s 300ms/step - loss: 0.0751 - accuracy: 0.2354 - val_loss: 0.2400 - val_accuracy: 0.1210\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 2s 294ms/step - loss: 0.0744 - accuracy: 0.2354 - val_loss: 0.2498 - val_accuracy: 0.1210\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 2s 298ms/step - loss: 0.0748 - accuracy: 0.2354 - val_loss: 0.2425 - val_accuracy: 0.1210\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 2s 295ms/step - loss: 0.0742 - accuracy: 0.2356 - val_loss: 0.2430 - val_accuracy: 0.1210\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 2s 301ms/step - loss: 0.0727 - accuracy: 0.2357 - val_loss: 0.2468 - val_accuracy: 0.1210\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 2s 298ms/step - loss: 0.0755 - accuracy: 0.2358 - val_loss: 0.2352 - val_accuracy: 0.1213\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 2s 296ms/step - loss: 0.0715 - accuracy: 0.2358 - val_loss: 0.2436 - val_accuracy: 0.1210\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 2s 297ms/step - loss: 0.0713 - accuracy: 0.2359 - val_loss: 0.2411 - val_accuracy: 0.1210\n"
     ]
    }
   ],
   "source": [
    "r = model.fit(\n",
    "    [encoder_input_sequences, decoder_input_sequences],\n",
    "    decoder_targets_one_hot,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs_placeholder, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(LSTM_NODES,))\n",
    "decoder_state_input_c = Input(shape=(LSTM_NODES,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
    "decoder_states = [h, c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_single] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_target = {v:k for k, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = word2idx_outputs['[start]']\n",
    "    eos = word2idx_outputs['[end]']\n",
    "    output_sentence = []\n",
    "\n",
    "    for _ in range(max_out_len):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "\n",
    "        if idx > 0:\n",
    "            word = idx2word_target[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "        target_seq[0, 0] = idx\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input: for each city ,  list their names in decreasing order by their highest station latitude .\n",
      "Response: [start] select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(len(input_sentences))\n",
    "input_seq = encoder_input_sequences[i:i+1]\n",
    "translation = translate_sentence(input_seq)\n",
    "print('-')\n",
    "print('Input:', input_sentences[i])\n",
    "print('Response:', translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "24c7168f742075337832b93c1fbfd4355e111387279ce758eae35d9b42cad7f4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
