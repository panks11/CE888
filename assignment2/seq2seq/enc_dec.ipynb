{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "run_logdir = get_run_logdir()\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Train_Data():\n",
    "    \n",
    "    '''\n",
    "    Train_Data class Loads the data from multiple Training JSON files in Pandas Dataframes\n",
    "    '''\n",
    "   \n",
    "    def __init__(self, path , filenames):\n",
    "        '''\n",
    "            Parameters\n",
    "            ------------\n",
    "            path: directory where English to SQL translation JSON are placed\n",
    "            filenames: Json file names containing Train data\n",
    "        '''\n",
    "        self.path = path\n",
    "        self.filenames = filenames\n",
    "        self.df_train = pd.DataFrame()\n",
    "        for f in self.filenames:\n",
    "            print(\"Reading file at path\", self.path+f)\n",
    "            try:\n",
    "                df = pd.read_json(self.path + f)\n",
    "                if len(self.df_train) == 0:\n",
    "                    self.df_train = df\n",
    "                else:\n",
    "                    self.df_train = self.df_train.append(df)\n",
    "                print(\"{} Rows in Total\".format(len(self.df_train)))\n",
    "            except Exception as e:\n",
    "                print(\"Got error while Reading file : \" , e)\n",
    "                \n",
    "    @property          \n",
    "    def questions(self):\n",
    "        ''' \n",
    "        Returns\n",
    "        ------------\n",
    "        Returns English Questions in Dataframe Rows as List\n",
    "        '''\n",
    "        return self.df_train.question.values.tolist()\n",
    "    \n",
    "    @property\n",
    "    def sql(self):\n",
    "        '''\n",
    "        Returns\n",
    "        ------------\n",
    "        Returns SQL in Dataframe Rows as List\n",
    "        '''\n",
    "        return self.df_train['query'].values.tolist()\n",
    "    \n",
    "    @property          \n",
    "    def question_tokens(self):\n",
    "        ''' \n",
    "        Returns\n",
    "        ------------\n",
    "        Returns English Question Tokens in Dataframe Rows as List\n",
    "        '''\n",
    "        \n",
    "        return self.df_train['question_toks'].values.tolist()\n",
    "    \n",
    "    @property\n",
    "    def sql_tokens(self):\n",
    "        ''' \n",
    "        Returns\n",
    "        ------------\n",
    "        Returns SQL Query Tokens in Dataframe Rows as List\n",
    "        '''\n",
    "        return self.df_train['query_toks'].values.tolist()\n",
    "    \n",
    "    def get_special_characters(self,list_of_text):\n",
    "        '''\n",
    "        Parameters\n",
    "        ------------\n",
    "        list_of_text: Input List of Text \n",
    "        Returns\n",
    "        ------------\n",
    "        Provides list of Special Characters in the text\n",
    "        '''\n",
    "        return list(set(Preprocess.special_char(''.join([''.join(ele) for ele in list_of_text]))))\n",
    "    \n",
    "    def get_vocab_size(self, list_of_text):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------\n",
    "        list_of_text: Input List of Text \n",
    "        \n",
    "        Returns\n",
    "        ------------\n",
    "        Vocabulary size or unique words in the corpus\n",
    "        \"\"\"\n",
    "        word_list = []\n",
    "        for sentence in list_of_text:\n",
    "            for word in sentence.split():\n",
    "                word = word.lower().strip()\n",
    "                if word not in word_list:\n",
    "                    word_list.append(word)\n",
    "        return len(word_list), word_list\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#################################\n",
    "#CONSTANTS\n",
    "\n",
    "EOS = '[END]'\n",
    "SOS = '[START]'  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#################################################################################################################\n",
    "    \n",
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "class Preprocess():\n",
    "    \n",
    "    '''\n",
    "    Preprocess class cleans and standardize the data, add SOS-EOS tags to the data\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,text):\n",
    "        \"\"\"\n",
    "            Parameters\n",
    "            ------------\n",
    "            text : Input string\n",
    "            Runs the text processing steps\n",
    "            \n",
    "        \"\"\"\n",
    "        self.processed_text = self.run_pipeline(text)\n",
    "    \n",
    "    def text_standardize(self, text):\n",
    "        \"\"\"\n",
    "            Parameters\n",
    "            ------------\n",
    "            text : Input string\n",
    "            \n",
    "            Returns\n",
    "            ------------\n",
    "            -   Unicode normalization using NFKD method\n",
    "            -   Lower Case text\n",
    "        \n",
    "        \"\"\"\n",
    "        text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "        text = tf.strings.lower(text)\n",
    "        return text\n",
    "    \n",
    "    def text_whitespace(self, text):\n",
    "        \"\"\"\n",
    "            Parameters\n",
    "            ------------\n",
    "            text : Input string\n",
    "            \n",
    "            Returns\n",
    "            ------------\n",
    "            -   Remove $ and \\\\ special characters \n",
    "            -   Add space around punctations\n",
    "            -   Remove spaces around sentences\n",
    "        \n",
    "        \"\"\"\n",
    "        text = tf.strings.regex_replace(text, '[$\\\\\\\\]', '')\n",
    "        text = tf.strings.regex_replace(text, '[.?!,Â¿()*:@]', r' \\0 ')\n",
    "        text = tf.strings.strip(text)\n",
    "        return text\n",
    "    \n",
    "    def add_SOS_EOS(self, text):\n",
    "        \"\"\"\n",
    "            Parameters\n",
    "            ------------\n",
    "            text : Input string\n",
    "            \n",
    "            Returns\n",
    "            ------------\n",
    "            -   Add <SOS> and <EOS> tags to each sentence\n",
    "        \n",
    "        \"\"\"\n",
    "        text = tf.strings.join([SOS, text, EOS], separator=' ')\n",
    "        return text\n",
    "    \n",
    "    @classmethod\n",
    "    def special_char(cls, text):\n",
    "        \"\"\"\n",
    "            Parameters\n",
    "            ------------\n",
    "            text : Input string\n",
    "            \n",
    "            Returns\n",
    "            ------------\n",
    "            -   Special Characters found in Text using Regular Expression\n",
    "        \"\"\"\n",
    "        return re.findall(r'[\\W]',text.replace(' ',''))\n",
    "    \n",
    "    \n",
    "    def run_pipeline(self,text):\n",
    "        \"\"\"\n",
    "            Parameters\n",
    "            ------------\n",
    "            text : Input string\n",
    "            \n",
    "            Returns\n",
    "            ------------\n",
    "            Executes series of Text pre processing functions\n",
    "        \n",
    "        \"\"\"\n",
    "        text = self.text_standardize(text)\n",
    "        text = self.text_whitespace(text)\n",
    "        text = self.add_SOS_EOS(text)\n",
    "        self.text = text\n",
    "        return self.text\n",
    "    \n",
    "    \n",
    "class Features():\n",
    "    '''\n",
    "    Extracts text features from data\n",
    "    '''\n",
    "    def tf_lower_and_split_punct(self,text):\n",
    "        \"\"\"\n",
    "            Parameters\n",
    "            ------------\n",
    "            text : Input string\n",
    "            \n",
    "            Returns\n",
    "            ------------\n",
    "            Standardized Text\n",
    "        \n",
    "        \"\"\"\n",
    "        return Preprocess(text).processed_text\n",
    "        \n",
    "    def vectorizor(self, document, max_vocab_size):\n",
    "        \"\"\"\n",
    "            Parameters\n",
    "            ------------\n",
    "            document : Collection of sentences\n",
    "            max_vocab_size : No of words in document used for TextVectorization\n",
    "            \n",
    "            Returns\n",
    "            ------------\n",
    "            TextVectorization object\n",
    "        \n",
    "        \"\"\"\n",
    "        text_processor = tf.keras.layers.TextVectorization( standardize = self.tf_lower_and_split_punct, max_tokens = max_vocab_size)\n",
    "        text_processor.adapt(document)\n",
    "        print(\"Sample Vocabulary\",text_processor.get_vocabulary()[:10])\n",
    "        return text_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file at path D:/DS/Learnin/Essex MS/CE888/CE888/assignment/seq2seq/spider/train_spider.json\n",
      "7000 Rows in Total\n",
      "Reading file at path D:/DS/Learnin/Essex MS/CE888/CE888/assignment/seq2seq/spider/train_others.json\n",
      "8659 Rows in Total\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SELECT count(*) FROM head WHERE age  >  56'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    del(o)\n",
    "except:\n",
    "    pass\n",
    "o = Train_Data('D:/DS/Learnin/Essex MS/CE888/CE888/assignment/seq2seq/spider/',['train_spider.json','train_others.json'])\n",
    "\n",
    "o.sql[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Vocabulary ['', '[UNK]', 'the', '[START]', '[END]', 'of', '?', '.', 'what', 'are']\n",
      "Sample Vocabulary ['', '[UNK]', '.', 't1', 't2', '=', 'select', 'from', 'as', '[START]']\n"
     ]
    }
   ],
   "source": [
    "max_vocab_size = o.get_vocab_size(o.questions)[0]\n",
    "\n",
    "input_text_processor = Features().vectorizor(o.questions , max_vocab_size)\n",
    "output_text_processor = Features().vectorizor(o.sql , max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = o.questions\n",
    "targ = o.sql\n",
    "\n",
    "BUFFER_SIZE = len(inp)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open(r'embeddings./glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 6, 51) (100000, 3, 51) (100000, 3, 51)\n",
      "3125/3125 [==============================] - 79s 22ms/step - loss: 0.6465 - accuracy: 0.7962\n",
      "Accuracy: 99.00%\n",
      "X=[47, 49, 5, 49, 38, 39] y=[5, 49, 47], yhat=[5, 49, 47]\n",
      "X=[5, 20, 23, 21, 11, 39] y=[23, 20, 5], yhat=[23, 20, 5]\n",
      "X=[50, 21, 46, 25, 29, 24] y=[46, 21, 50], yhat=[46, 21, 50]\n",
      "X=[10, 17, 1, 5, 37, 10] y=[1, 17, 10], yhat=[1, 17, 10]\n",
      "X=[31, 35, 34, 33, 12, 43] y=[34, 35, 31], yhat=[34, 35, 31]\n",
      "X=[2, 43, 48, 43, 5, 18] y=[48, 43, 2], yhat=[43, 48, 2]\n",
      "X=[17, 4, 2, 12, 31, 7] y=[2, 4, 17], yhat=[2, 4, 17]\n",
      "X=[30, 40, 23, 36, 6, 14] y=[23, 40, 30], yhat=[23, 40, 30]\n",
      "X=[34, 1, 23, 16, 18, 7] y=[23, 1, 34], yhat=[23, 1, 34]\n",
      "X=[32, 23, 19, 45, 23, 37] y=[19, 23, 32], yhat=[19, 23, 32]\n"
     ]
    }
   ],
   "source": [
    "# from random import randint\n",
    "# import numpy as np\n",
    "# from numpy import array\n",
    "# from numpy import argmax\n",
    "# from numpy import array_equal\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from keras.models import Model\n",
    "# from keras.layers import Input\n",
    "# from keras.layers import LSTM\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# # generate a sequence of random integers\n",
    "# def generate_sequence(length, n_unique):\n",
    "# \treturn [randint(1, n_unique-1) for _ in range(length)]\n",
    "\n",
    "# # prepare data for the LSTM\n",
    "# def get_dataset(n_in, n_out, cardinality, n_samples):\n",
    "# \tX1, X2, y = list(), list(), list()\n",
    "# \tfor _ in range(n_samples):\n",
    "# \t\tsource = generate_sequence(n_in, cardinality)\n",
    "#         # define padded target sequence\n",
    "# \t\ttarget = source[:n_out]\n",
    "# \t\ttarget.reverse()\n",
    "#         # create padded input target sequence\n",
    "# \t\ttarget_in = [0] + target[:-1]\n",
    "#         # encode\n",
    "# \t\tsrc_encoded = to_categorical([source], num_classes=cardinality)\n",
    "# \t\ttar_encoded = to_categorical([target], num_classes=cardinality)\n",
    "# \t\ttar2_encoded = to_categorical([target_in], num_classes=cardinality)\n",
    "#         # store\n",
    "# \t\tX1.append(src_encoded)\n",
    "# \t\tX2.append(tar2_encoded)\n",
    "# \t\ty.append(tar_encoded)\n",
    "# \tX1 = np.squeeze(array(X1), axis=1)\n",
    "# \tX2 = np.squeeze(array(X2), axis=1)\n",
    "# \ty = np.squeeze(array(y), axis=1)\n",
    "# \treturn array(X1), array(X2), array(y)\n",
    "\n",
    "# # returns train, inference_encoder and inference_decoder models\n",
    "# def define_models(n_input, n_output, n_units):\n",
    "# \t# define training encoder\n",
    "# \tencoder_inputs = Input(shape=(None, n_input))\n",
    "# \tencoder = LSTM(n_units, return_state=True)\n",
    "# \tencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# \tencoder_states = [state_h, state_c]\n",
    "# \t# define training decoder\n",
    "# \tdecoder_inputs = Input(shape=(None, n_output))\n",
    "# \tdecoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "# \tdecoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "# \tdecoder_dense = Dense(n_output, activation='softmax')\n",
    "# \tdecoder_outputs = decoder_dense(decoder_outputs)\n",
    "# \tmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "# \t# define inference encoder\n",
    "# \tencoder_model = Model(encoder_inputs, encoder_states)\n",
    "# \t# define inference decoder\n",
    "# \tdecoder_state_input_h = Input(shape=(n_units,))\n",
    "# \tdecoder_state_input_c = Input(shape=(n_units,))\n",
    "# \tdecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "# \tdecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# \tdecoder_states = [state_h, state_c]\n",
    "# \tdecoder_outputs = decoder_dense(decoder_outputs)\n",
    "# \tdecoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "# \t# return all models\n",
    "# \treturn model, encoder_model, decoder_model\n",
    "\n",
    "# # generate target given source sequence\n",
    "# def predict_sequence(infenc, infdec, source, n_steps, cardinality):\n",
    "# \t# encode\n",
    "# \tstate = infenc.predict(source)\n",
    "# \t# start of sequence input\n",
    "# \ttarget_seq = array([0.0 for _ in range(cardinality)]).reshape(1, 1, cardinality)\n",
    "# \t# collect predictions\n",
    "# \toutput = list()\n",
    "# \tfor t in range(n_steps):\n",
    "# \t\t# predict next char\n",
    "# \t\tyhat, h, c = infdec.predict([target_seq] + state)\n",
    "# \t\t# store prediction\n",
    "# \t\toutput.append(yhat[0,0,:])\n",
    "# \t\t# update state\n",
    "# \t\tstate = [h, c]\n",
    "# \t\t# update target sequence\n",
    "# \t\ttarget_seq = yhat\n",
    "# \treturn array(output)\n",
    "\n",
    "# # decode a one hot encoded string\n",
    "# def one_hot_decode(encoded_seq):\n",
    "# \treturn [argmax(vector) for vector in encoded_seq]\n",
    "\n",
    "# # configure problem\n",
    "# n_features = 50 + 1\n",
    "# n_steps_in = 6\n",
    "# n_steps_out = 3\n",
    "# # define model\n",
    "# train, infenc, infdec = define_models(n_features, n_features, 128)\n",
    "# train.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# # generate training dataset\n",
    "# X1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 100000)\n",
    "# print(X1.shape,X2.shape,y.shape)\n",
    "# # train model\n",
    "# train.fit([X1, X2], y, epochs=1,callbacks=[tensorboard_cb])\n",
    "# # evaluate LSTM\n",
    "# total, correct = 100, 0\n",
    "# for _ in range(total):\n",
    "# \tX1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 1)\n",
    "# \ttarget = predict_sequence(infenc, infdec, X1, n_steps_out, n_features)\n",
    "# \tif array_equal(one_hot_decode(y[0]), one_hot_decode(target)):\n",
    "# \t\tcorrect += 1\n",
    "# print('Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))\n",
    "# # spot check some examples\n",
    "# for _ in range(10):\n",
    "# \tX1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 1)\n",
    "# \ttarget = predict_sequence(infenc, infdec, X1, n_steps_out, n_features)\n",
    "# \tprint('X=%s y=%s, yhat=%s' % (one_hot_decode(X1[0]), one_hot_decode(y[0]), one_hot_decode(target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "24c7168f742075337832b93c1fbfd4355e111387279ce758eae35d9b42cad7f4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
