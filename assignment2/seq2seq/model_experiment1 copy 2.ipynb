{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import tensorflow as tf\n",
    "import typing\n",
    "from typing import Any, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file at path D:/DS/Learnin/Essex MS/CE888/git/CE888/assignment2/seq2seq/spider/train_spider.json\n",
      "7000 Rows in Total\n",
      "Reading file at path D:/DS/Learnin/Essex MS/CE888/git/CE888/assignment2/seq2seq/spider/train_others.json\n",
      "8659 Rows in Total\n",
      "Filter Easy Queries\n",
      "Splittin the Train and Test data\n",
      "(727, 7)\n",
      "Data for Training (2908, 7)\n",
      "Data for Testing (727, 7)\n",
      "Sample Vocabulary ['', '[UNK]', 'the', '[start]', '[end]', 'of', '?', '.', 'what', 'are']\n",
      "Sample Vocabulary ['', '[UNK]', 'select', 'from', '[start]', '[end]', 'where', ')', '(', '=']\n"
     ]
    }
   ],
   "source": [
    "handlr = data.Train_Data('D:/DS/Learnin/Essex MS/CE888/git/CE888/assignment2/seq2seq/spider/',['train_spider.json','train_others.json'])\n",
    "input_text_processor = data.Features().vectorizor(handlr.questions , data.Max_Vocab_Size)\n",
    "output_text_processor = data.Features().vectorizor(handlr.sql , data.Max_Vocab_Size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the List of Questions in inp and List of Queries as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = handlr.questions\n",
    "targ = handlr.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(inp)\n",
    "BATCH_SIZE = data.BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = data.embedding_dim\n",
    "units = data.units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "\n",
    "        # The embedding layer converts tokens to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
    "                                                embedding_dim)\n",
    "\n",
    "        # The GRU RNN layer processes those vectors sequentially.\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                    # Return the sequence and state\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, tokens, state=None):\n",
    "        \n",
    "\n",
    "\n",
    "        # 2. The embedding layer looks up the embedding for each token.\n",
    "        vectors = self.embedding(tokens)\n",
    "\n",
    "\n",
    "        # 3. The GRU processes the embedding sequence.\n",
    "        #    output shape: (batch, s, enc_units)\n",
    "        #    state shape: (batch, enc_units)\n",
    "        output, state = self.gru(vectors, initial_state=state)\n",
    "\n",
    "\n",
    "        # 4. Returns the new sequence and its state.\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # For Eqn. (4), the  Bahdanau attention\n",
    "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "        self.attention = tf.keras.layers.AdditiveAttention()\n",
    "\n",
    "    def call(self, query, value, mask):\n",
    "       \n",
    "        # From Eqn. (4), `W1@ht`.\n",
    "        w1_query = self.W1(query)\n",
    "       \n",
    "        # From Eqn. (4), `W2@hs`.\n",
    "        w2_key = self.W2(value)\n",
    "\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs = [w1_query, value, w2_key],\n",
    "            mask=[query_mask, value_mask],\n",
    "            return_attention_scores = True,\n",
    "        )\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.dec_units = dec_units\n",
    "    self.output_vocab_size = output_vocab_size\n",
    "    self.embedding_dim = embedding_dim\n",
    "\n",
    "    # For Step 1. The embedding layer convets token IDs to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
    "                                               embedding_dim)\n",
    "\n",
    "    # For Step 2. The RNN keeps track of what's been generated so far.\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    # For step 3. The RNN output will be the query for the attention layer.\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    # For step 4. Eqn. (3): converting `ct` to `at`\n",
    "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
    "                                    use_bias=False)\n",
    "\n",
    "    # For step 5. This fully connected layer produces the logits for each\n",
    "    # output token.\n",
    "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderInput(typing.NamedTuple):\n",
    "    new_tokens: Any\n",
    "    enc_output: Any\n",
    "    mask: Any\n",
    "\n",
    "class DecoderOutput(typing.NamedTuple):\n",
    "    logits: Any\n",
    "    attention_weights: Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self, inputs: DecoderInput, state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
    "\n",
    "  # Step 1. Lookup the embeddings\n",
    "  vectors = self.embedding(inputs.new_tokens)\n",
    " \n",
    "\n",
    "  # Step 2. Process one step with the RNN\n",
    "  rnn_output, state = self.gru(vectors, initial_state=state)\n",
    "\n",
    "  \n",
    "\n",
    "  # Step 3. Use the RNN output as the query for the attention over the\n",
    "  # encoder output.\n",
    "  context_vector, attention_weights = self.attention(\n",
    "      query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
    "  \n",
    "\n",
    "  # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
    "  #     [ct; ht] shape: (batch t, value_units + query_units)\n",
    "  context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
    "\n",
    "  # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
    "  attention_vector = self.Wc(context_and_rnn_output)\n",
    "\n",
    "  # Step 5. Generate logit predictions:\n",
    "  logits = self.fc(attention_vector)\n",
    "\n",
    "  return DecoderOutput(logits, attention_weights), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decoder.call = call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        self.name = 'masked_loss'\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        # Calculate the loss for each item in the batch.\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "\n",
    "        # Mask off the losses on padding.\n",
    "        mask = tf.cast(y_true != 0, tf.float32)\n",
    "        loss *= mask\n",
    "\n",
    "        # Return the total.\n",
    "        return tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTranslator(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units,\n",
    "               input_text_processor,\n",
    "               output_text_processor, \n",
    "               use_tf_function=True):\n",
    "        super().__init__()\n",
    "        # Build the encoder and decoder\n",
    "        encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                        embedding_dim, units)\n",
    "        decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                        embedding_dim, units)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "        self.use_tf_function = use_tf_function\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_train_step(inputs)\n",
    "        else:\n",
    "         return self._train_step(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess(self, input_text, target_text):\n",
    "  \n",
    "\n",
    "  # Convert the text to token IDs\n",
    "  input_tokens = self.input_text_processor(input_text)\n",
    "  target_tokens = self.output_text_processor(target_text)\n",
    "  \n",
    "\n",
    "  # Convert IDs to masks.\n",
    "  input_mask = input_tokens != 0\n",
    " \n",
    "\n",
    "  target_mask = target_tokens != 0\n",
    "  \n",
    "\n",
    "  return input_tokens, input_mask, target_tokens, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._preprocess = _preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_step(self, inputs):\n",
    "    input_text, target_text = inputs  \n",
    "\n",
    "    (input_tokens, input_mask,target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
    "\n",
    "    max_target_length = tf.shape(target_tokens)[1]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Encode the input\n",
    "        enc_output, enc_state = self.encoder(input_tokens)\n",
    "        \n",
    "\n",
    "        # Initialize the decoder's state to the encoder's final state.\n",
    "        # This only works if the encoder and decoder have the same number of\n",
    "        # units.\n",
    "        dec_state = enc_state\n",
    "        loss = tf.constant(0.0)\n",
    "\n",
    "        for t in tf.range(max_target_length-1):\n",
    "            # Pass in two tokens from the target sequence:\n",
    "            # 1. The current input to the decoder.\n",
    "            # 2. The target for the decoder's next prediction.\n",
    "            new_tokens = target_tokens[:, t:t+2]\n",
    "            step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
    "                                                    enc_output, dec_state)\n",
    "            loss = loss + step_loss\n",
    "\n",
    "    # Average the loss over all non padding tokens.\n",
    "        average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
    "\n",
    "    # Apply an optimization step\n",
    "    variables = self.trainable_variables \n",
    "    gradients = tape.gradient(average_loss, variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    # Return a dict mapping metric names to current value\n",
    "    return {'batch_loss': average_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._train_step = _train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
    "    input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
    "\n",
    "    # Run the decoder one step.\n",
    "    decoder_input = DecoderInput(new_tokens=input_token,\n",
    "                                enc_output=enc_output,\n",
    "                                mask=input_mask)\n",
    "\n",
    "    dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
    "\n",
    "\n",
    "    # `self.loss` returns the total for non-padded tokens\n",
    "    y = target_token\n",
    "    y_pred = dec_result.logits\n",
    "    # print(y_pred.as_numpy_iterator())\n",
    "    step_loss = self.loss(y, y_pred)\n",
    "\n",
    "    return step_loss, dec_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._loop_step = _loop_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = TrainTranslator(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    "    use_tf_function=False)\n",
    "\n",
    "# Configure the loss and optimizer\n",
    "translator.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=MaskedLoss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
    "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
    "def _tf_train_step(self, inputs):\n",
    "  return self._train_step(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._tf_train_step = _tf_train_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.use_tf_function = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_translator = TrainTranslator(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor)\n",
    "\n",
    "# Configure the loss and optimizer\n",
    "train_translator.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=MaskedLoss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLogs(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.logs = []\n",
    "\n",
    "    def on_train_batch_end(self, n, logs):\n",
    "        self.logs.append(logs[self.key])\n",
    "\n",
    "batch_loss = BatchLogs('batch_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "def get_run_logdir():\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "46/46 [==============================] - 126s 3s/step - batch_loss: 4.8429\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 117s 3s/step - batch_loss: 3.0781\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 118s 3s/step - batch_loss: 2.3434\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 118s 3s/step - batch_loss: 2.0450\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 122s 3s/step - batch_loss: 1.8140\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 120s 3s/step - batch_loss: 1.5988\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 127s 3s/step - batch_loss: 1.4064\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 120s 3s/step - batch_loss: 1.2233\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 3720s 83s/step - batch_loss: 1.0401\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 120s 3s/step - batch_loss: 0.8761\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 121s 3s/step - batch_loss: 0.7218\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 118s 3s/step - batch_loss: 0.5983\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 122s 3s/step - batch_loss: 0.4929\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 123s 3s/step - batch_loss: 0.3994\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 123s 3s/step - batch_loss: 0.3214\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 121s 3s/step - batch_loss: 0.2550\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 119s 3s/step - batch_loss: 0.1980\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 121s 3s/step - batch_loss: 0.1534\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 118s 3s/step - batch_loss: 0.1228\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 117s 3s/step - batch_loss: 0.0960\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 120s 3s/step - batch_loss: 0.0757\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 130s 3s/step - batch_loss: 0.0602\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 124s 3s/step - batch_loss: 0.0468\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 125s 3s/step - batch_loss: 0.0387\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 126s 3s/step - batch_loss: 0.0313\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 125s 3s/step - batch_loss: 0.0269\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 133s 3s/step - batch_loss: 0.0210\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 156s 3s/step - batch_loss: 0.0163\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 160s 3s/step - batch_loss: 0.0157\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 158s 3s/step - batch_loss: 0.0136\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - 159s 3s/step - batch_loss: 0.0136\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 158s 3s/step - batch_loss: 0.0112\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 159s 3s/step - batch_loss: 0.0118\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 152s 3s/step - batch_loss: 0.0137\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 160s 3s/step - batch_loss: 0.0295\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 158s 3s/step - batch_loss: 0.0599\n",
      "Epoch 37/50\n",
      "46/46 [==============================] - 158s 3s/step - batch_loss: 0.0767\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 156s 3s/step - batch_loss: 0.0621\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 158s 3s/step - batch_loss: 0.0423\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 158s 3s/step - batch_loss: 0.0258\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 157s 3s/step - batch_loss: 0.0194\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 159s 3s/step - batch_loss: 0.0114\n",
      "Epoch 43/50\n",
      "46/46 [==============================] - 158s 3s/step - batch_loss: 0.0087\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 156s 3s/step - batch_loss: 0.0061\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 152s 3s/step - batch_loss: 0.0054\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 150s 3s/step - batch_loss: 0.0059\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 149s 3s/step - batch_loss: 0.0063\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 149s 3s/step - batch_loss: 0.0066\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 147s 3s/step - batch_loss: 0.0055\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 151s 3s/step - batch_loss: 0.0043\n"
     ]
    }
   ],
   "source": [
    "history = train_translator.fit(dataset, epochs=50, callbacks=[batch_loss,tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Batch Loss')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(batch_loss.logs)\n",
    "plt.ylim([0, 3])\n",
    "plt.xlabel('Batch Number')\n",
    "plt.ylabel('Batch Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.051624774932861,\n",
       " 7.048401832580566,\n",
       " 6.908292770385742,\n",
       " 6.5229058265686035,\n",
       " 5.454949855804443,\n",
       " 8.64066219329834,\n",
       " 7.918764114379883,\n",
       " 6.329209327697754,\n",
       " 6.27166223526001,\n",
       " 5.726656436920166,\n",
       " 5.531042098999023,\n",
       " 5.152456283569336,\n",
       " 4.808164596557617,\n",
       " 4.759221076965332,\n",
       " 4.648735523223877,\n",
       " 4.560144424438477,\n",
       " 4.630853176116943,\n",
       " 4.651496887207031,\n",
       " 4.443292617797852,\n",
       " 4.563898086547852,\n",
       " 4.47241735458374,\n",
       " 4.380863666534424,\n",
       " 4.285150527954102,\n",
       " 4.366373062133789,\n",
       " 4.561182975769043,\n",
       " 4.338625431060791,\n",
       " 4.387815952301025,\n",
       " 4.203274250030518,\n",
       " 4.27259635925293,\n",
       " 4.176522731781006,\n",
       " 4.298745155334473,\n",
       " 4.23585844039917,\n",
       " 4.14679479598999,\n",
       " 4.146397113800049,\n",
       " 4.241480350494385,\n",
       " 4.063497543334961,\n",
       " 4.253663063049316,\n",
       " 3.9712228775024414,\n",
       " 3.9840433597564697,\n",
       " 3.925319194793701,\n",
       " 4.13038444519043,\n",
       " 4.067673206329346,\n",
       " 3.9402806758880615,\n",
       " 3.7709476947784424,\n",
       " 3.8467326164245605,\n",
       " 3.763396739959717,\n",
       " 3.69238018989563,\n",
       " 3.643933057785034,\n",
       " 3.531745433807373,\n",
       " 3.4368627071380615,\n",
       " 3.4303975105285645,\n",
       " 3.5579473972320557,\n",
       " 3.6255416870117188,\n",
       " 3.5068695545196533,\n",
       " 3.490436315536499,\n",
       " 3.4168033599853516,\n",
       " 3.5039985179901123,\n",
       " 3.3774209022521973,\n",
       " 3.3960986137390137,\n",
       " 3.5094428062438965,\n",
       " 3.135533332824707,\n",
       " 3.309101104736328,\n",
       " 3.1752829551696777,\n",
       " 3.2324225902557373,\n",
       " 3.1904232501983643,\n",
       " 3.0934290885925293,\n",
       " 2.8982834815979004,\n",
       " 3.1242523193359375,\n",
       " 3.190814971923828,\n",
       " 3.014552354812622,\n",
       " 2.9887614250183105,\n",
       " 3.0563604831695557,\n",
       " 3.1925244331359863,\n",
       " 2.9073517322540283,\n",
       " 2.8539087772369385,\n",
       " 2.9996352195739746,\n",
       " 2.977917432785034,\n",
       " 2.831576347351074,\n",
       " 2.7299654483795166,\n",
       " 2.822000741958618,\n",
       " 2.874418258666992,\n",
       " 2.8638932704925537,\n",
       " 2.791552782058716,\n",
       " 2.731879711151123,\n",
       " 2.6780924797058105,\n",
       " 2.537353754043579,\n",
       " 2.7489469051361084,\n",
       " 2.717994213104248,\n",
       " 2.5865862369537354,\n",
       " 2.4587998390197754,\n",
       " 2.441784381866455,\n",
       " 2.69832706451416,\n",
       " 2.4378323554992676,\n",
       " 2.457986354827881,\n",
       " 2.4854865074157715,\n",
       " 2.3156301975250244,\n",
       " 2.4093945026397705,\n",
       " 2.301816701889038,\n",
       " 2.5151305198669434,\n",
       " 2.5341639518737793,\n",
       " 2.3576467037200928,\n",
       " 2.3985772132873535,\n",
       " 2.4469923973083496,\n",
       " 2.3080599308013916,\n",
       " 2.2945268154144287,\n",
       " 2.2551984786987305,\n",
       " 2.4736666679382324,\n",
       " 2.3946774005889893,\n",
       " 2.3712804317474365,\n",
       " 2.3143422603607178,\n",
       " 2.4656178951263428,\n",
       " 2.2270095348358154,\n",
       " 2.525722026824951,\n",
       " 2.4246609210968018,\n",
       " 2.3723793029785156,\n",
       " 2.282543182373047,\n",
       " 2.445937395095825,\n",
       " 2.361478805541992,\n",
       " 2.2264907360076904,\n",
       " 2.5216352939605713,\n",
       " 2.408628225326538,\n",
       " 2.190324544906616,\n",
       " 2.234771966934204,\n",
       " 2.25732684135437,\n",
       " 2.2799696922302246,\n",
       " 2.455310344696045,\n",
       " 2.3536295890808105,\n",
       " 2.215470552444458,\n",
       " 2.1868293285369873,\n",
       " 2.267967939376831,\n",
       " 2.319284439086914,\n",
       " 2.162916421890259,\n",
       " 2.240229845046997,\n",
       " 2.4806878566741943,\n",
       " 2.3074958324432373,\n",
       " 2.2273755073547363,\n",
       " 2.074861526489258,\n",
       " 2.276249408721924,\n",
       " 1.9488705396652222,\n",
       " 2.1239850521087646,\n",
       " 1.9990288019180298,\n",
       " 1.889897108078003,\n",
       " 2.14319109916687,\n",
       " 2.0398330688476562,\n",
       " 2.067553758621216,\n",
       " 2.0954113006591797,\n",
       " 2.0056092739105225,\n",
       " 2.1443872451782227,\n",
       " 2.162108898162842,\n",
       " 1.985644817352295,\n",
       " 2.1074023246765137,\n",
       " 1.9865168333053589,\n",
       " 2.1268200874328613,\n",
       " 2.041024684906006,\n",
       " 2.087085485458374,\n",
       " 2.039360284805298,\n",
       " 2.1259491443634033,\n",
       " 2.1703591346740723,\n",
       " 2.0869126319885254,\n",
       " 2.0548510551452637,\n",
       " 1.980600118637085,\n",
       " 2.0380427837371826,\n",
       " 2.001473903656006,\n",
       " 2.2692368030548096,\n",
       " 1.9667103290557861,\n",
       " 2.008185625076294,\n",
       " 1.9754520654678345,\n",
       " 1.8982901573181152,\n",
       " 1.9466170072555542,\n",
       " 2.0510525703430176,\n",
       " 2.0709831714630127,\n",
       " 2.1195321083068848,\n",
       " 2.017657518386841,\n",
       " 1.9124394655227661,\n",
       " 2.0557780265808105,\n",
       " 2.287841558456421,\n",
       " 1.9971774816513062,\n",
       " 1.9994862079620361,\n",
       " 2.0403523445129395,\n",
       " 1.9307752847671509,\n",
       " 2.033874988555908,\n",
       " 1.9935120344161987,\n",
       " 2.015242099761963,\n",
       " 2.0356404781341553,\n",
       " 1.7283456325531006,\n",
       " 1.8624980449676514,\n",
       " 1.8527350425720215,\n",
       " 1.711526870727539,\n",
       " 1.8296966552734375,\n",
       " 1.8585727214813232,\n",
       " 1.7112946510314941,\n",
       " 1.6382309198379517,\n",
       " 1.9909218549728394,\n",
       " 1.7861560583114624,\n",
       " 1.8450349569320679,\n",
       " 1.8470007181167603,\n",
       " 1.7707558870315552,\n",
       " 1.8229273557662964,\n",
       " 1.7755335569381714,\n",
       " 1.8939050436019897,\n",
       " 1.7601773738861084,\n",
       " 1.856889009475708,\n",
       " 1.7575782537460327,\n",
       " 1.8509339094161987,\n",
       " 1.9187074899673462,\n",
       " 1.905036449432373,\n",
       " 1.7632808685302734,\n",
       " 1.6712902784347534,\n",
       " 1.8645873069763184,\n",
       " 1.8326363563537598,\n",
       " 1.7435534000396729,\n",
       " 1.7899315357208252,\n",
       " 1.7874224185943604,\n",
       " 1.753300428390503,\n",
       " 1.7311211824417114,\n",
       " 1.8559999465942383,\n",
       " 1.8080331087112427,\n",
       " 1.7742323875427246,\n",
       " 1.637384295463562,\n",
       " 1.8562533855438232,\n",
       " 1.8559024333953857,\n",
       " 1.9412785768508911,\n",
       " 1.7478795051574707,\n",
       " 1.7149080038070679,\n",
       " 1.8415920734405518,\n",
       " 1.9372756481170654,\n",
       " 1.7782694101333618,\n",
       " 1.7865188121795654,\n",
       " 1.9753525257110596,\n",
       " 1.9183980226516724,\n",
       " 1.535911202430725,\n",
       " 1.5254918336868286,\n",
       " 1.513451337814331,\n",
       " 1.6815814971923828,\n",
       " 1.599242925643921,\n",
       " 1.5550888776779175,\n",
       " 1.6003737449645996,\n",
       " 1.5078834295272827,\n",
       " 1.5292760133743286,\n",
       " 1.558767557144165,\n",
       " 1.5449789762496948,\n",
       " 1.5600905418395996,\n",
       " 1.597561240196228,\n",
       " 1.4954694509506226,\n",
       " 1.6183034181594849,\n",
       " 1.7860307693481445,\n",
       " 1.5990619659423828,\n",
       " 1.689997673034668,\n",
       " 1.6035895347595215,\n",
       " 1.5527533292770386,\n",
       " 1.5130529403686523,\n",
       " 1.5259721279144287,\n",
       " 1.6043235063552856,\n",
       " 1.6697112321853638,\n",
       " 1.5646063089370728,\n",
       " 1.6903473138809204,\n",
       " 1.673935890197754,\n",
       " 1.655457615852356,\n",
       " 1.6279916763305664,\n",
       " 1.596014380455017,\n",
       " 1.7549299001693726,\n",
       " 1.7020223140716553,\n",
       " 1.5744253396987915,\n",
       " 1.6904270648956299,\n",
       " 1.6061413288116455,\n",
       " 1.6372058391571045,\n",
       " 1.6274616718292236,\n",
       " 1.596655011177063,\n",
       " 1.732862114906311,\n",
       " 1.6250718832015991,\n",
       " 1.6061519384384155,\n",
       " 1.6173030138015747,\n",
       " 1.5517208576202393,\n",
       " 1.6145495176315308,\n",
       " 1.47500741481781,\n",
       " 1.4768366813659668,\n",
       " 1.4185749292373657,\n",
       " 1.4144346714019775,\n",
       " 1.3506592512130737,\n",
       " 1.3346470594406128,\n",
       " 1.4945242404937744,\n",
       " 1.3804845809936523,\n",
       " 1.414238691329956,\n",
       " 1.442403793334961,\n",
       " 1.3501724004745483,\n",
       " 1.4823848009109497,\n",
       " 1.3846240043640137,\n",
       " 1.4835790395736694,\n",
       " 1.3424530029296875,\n",
       " 1.2785531282424927,\n",
       " 1.3505316972732544,\n",
       " 1.3644691705703735,\n",
       " 1.3084080219268799,\n",
       " 1.5720608234405518,\n",
       " 1.4038795232772827,\n",
       " 1.4434010982513428,\n",
       " 1.3626855611801147,\n",
       " 1.2672162055969238,\n",
       " 1.356968879699707,\n",
       " 1.500495433807373,\n",
       " 1.386539101600647,\n",
       " 1.5787705183029175,\n",
       " 1.2495999336242676,\n",
       " 1.4833011627197266,\n",
       " 1.4278945922851562,\n",
       " 1.4811551570892334,\n",
       " 1.380323052406311,\n",
       " 1.486452579498291,\n",
       " 1.4838310480117798,\n",
       " 1.4010258913040161,\n",
       " 1.308529019355774,\n",
       " 1.360767126083374,\n",
       " 1.407540202140808,\n",
       " 1.514272928237915,\n",
       " 1.4264826774597168,\n",
       " 1.4767063856124878,\n",
       " 1.438524603843689,\n",
       " 1.389096736907959,\n",
       " 1.3531962633132935,\n",
       " 1.3997550010681152,\n",
       " 1.497363567352295,\n",
       " 1.3342583179473877,\n",
       " 1.1452313661575317,\n",
       " 1.188179612159729,\n",
       " 1.1413742303848267,\n",
       " 1.1686633825302124,\n",
       " 1.2851141691207886,\n",
       " 1.2689927816390991,\n",
       " 1.199784517288208,\n",
       " 1.2495794296264648,\n",
       " 1.2472258806228638,\n",
       " 1.2150070667266846,\n",
       " 1.21939218044281,\n",
       " 1.1555640697479248,\n",
       " 1.1376718282699585,\n",
       " 1.2098137140274048,\n",
       " 1.2752580642700195,\n",
       " 1.3254090547561646,\n",
       " 1.1942002773284912,\n",
       " 1.108488917350769,\n",
       " 1.2351075410842896,\n",
       " 1.1853125095367432,\n",
       " 1.2733263969421387,\n",
       " 1.1893163919448853,\n",
       " 1.217692494392395,\n",
       " 1.3396055698394775,\n",
       " 1.2315016984939575,\n",
       " 1.2705720663070679,\n",
       " 1.2229901552200317,\n",
       " 1.1902716159820557,\n",
       " 1.1646612882614136,\n",
       " 1.2173364162445068,\n",
       " 1.2136372327804565,\n",
       " 1.265838623046875,\n",
       " 1.465065598487854,\n",
       " 1.202816367149353,\n",
       " 1.214009165763855,\n",
       " 1.205299735069275,\n",
       " 1.2358487844467163,\n",
       " 1.1033824682235718,\n",
       " 1.2707871198654175,\n",
       " 1.2364696264266968,\n",
       " 1.278661847114563,\n",
       " 1.1369030475616455,\n",
       " 1.1954164505004883,\n",
       " 1.2777612209320068,\n",
       " 1.3177798986434937,\n",
       " 1.201509714126587,\n",
       " 1.0401334762573242,\n",
       " 0.988813579082489,\n",
       " 0.991867184638977,\n",
       " 1.0374749898910522,\n",
       " 1.1105901002883911,\n",
       " 0.9615745544433594,\n",
       " 1.0676616430282593,\n",
       " 1.0492393970489502,\n",
       " 1.0433343648910522,\n",
       " 1.0787776708602905,\n",
       " 1.0230023860931396,\n",
       " 0.9789096713066101,\n",
       " 1.0181487798690796,\n",
       " 1.1333534717559814,\n",
       " 0.8662722706794739,\n",
       " 1.0546185970306396,\n",
       " 1.019917368888855,\n",
       " 1.0841894149780273,\n",
       " 1.0406211614608765,\n",
       " 1.0130864381790161,\n",
       " 1.018904685974121,\n",
       " 1.025574803352356,\n",
       " 1.0241367816925049,\n",
       " 1.0174049139022827,\n",
       " 1.0863301753997803,\n",
       " 1.085526943206787,\n",
       " 0.9485918879508972,\n",
       " 0.9649778604507446,\n",
       " 1.2197566032409668,\n",
       " 1.033518671989441,\n",
       " 1.1157227754592896,\n",
       " 1.1102473735809326,\n",
       " 0.9988913536071777,\n",
       " 1.0803581476211548,\n",
       " 1.0484647750854492,\n",
       " 1.007044792175293,\n",
       " 1.0919865369796753,\n",
       " 1.0514713525772095,\n",
       " 1.0108752250671387,\n",
       " 1.1383610963821411,\n",
       " 1.0617270469665527,\n",
       " 0.9682461619377136,\n",
       " 1.0273792743682861,\n",
       " 1.0517380237579346,\n",
       " 1.0211727619171143,\n",
       " 1.0376155376434326,\n",
       " 0.7820769548416138,\n",
       " 0.8872270584106445,\n",
       " 0.8612021803855896,\n",
       " 0.8158429861068726,\n",
       " 0.8364415764808655,\n",
       " 0.8602896332740784,\n",
       " 0.845181941986084,\n",
       " 0.9300476312637329,\n",
       " 0.8524247407913208,\n",
       " 0.84316486120224,\n",
       " 0.8481221199035645,\n",
       " 0.9090186357498169,\n",
       " 0.8226470947265625,\n",
       " 0.9215888977050781,\n",
       " 0.8491026163101196,\n",
       " 0.9372493624687195,\n",
       " 0.8910750150680542,\n",
       " 0.8382367491722107,\n",
       " 0.9078782200813293,\n",
       " 0.8561793565750122,\n",
       " 0.8813492059707642,\n",
       " 0.8807578086853027,\n",
       " 0.9541789889335632,\n",
       " 0.8498193621635437,\n",
       " 0.8321474194526672,\n",
       " 0.7491528987884521,\n",
       " 0.9138508439064026,\n",
       " 0.9071987271308899,\n",
       " 0.8348522186279297,\n",
       " 0.8675856590270996,\n",
       " 0.8660946488380432,\n",
       " 0.940664529800415,\n",
       " 0.8609709739685059,\n",
       " 0.8753692507743835,\n",
       " 0.9627483487129211,\n",
       " 0.9060858488082886,\n",
       " 0.851643443107605,\n",
       " 0.8549141883850098,\n",
       " 0.8952471613883972,\n",
       " 0.9115374684333801,\n",
       " 0.900132417678833,\n",
       " 0.847864031791687,\n",
       " 0.8891981244087219,\n",
       " 0.8375675082206726,\n",
       " 0.8197135925292969,\n",
       " 0.9943735003471375,\n",
       " 0.6060716509819031,\n",
       " 0.6422661542892456,\n",
       " 0.6511958241462708,\n",
       " 0.6658895015716553,\n",
       " 0.679561197757721,\n",
       " 0.7158704400062561,\n",
       " 0.6646244525909424,\n",
       " 0.726377010345459,\n",
       " 0.7441176772117615,\n",
       " 0.7224432826042175,\n",
       " 0.7696289420127869,\n",
       " 0.7078956365585327,\n",
       " 0.7707728147506714,\n",
       " 0.7101083993911743,\n",
       " 0.7750182747840881,\n",
       " 0.6924654841423035,\n",
       " 0.7112720608711243,\n",
       " 0.7395608425140381,\n",
       " 0.7877795696258545,\n",
       " 0.6717522740364075,\n",
       " 0.7563508152961731,\n",
       " 0.7162786722183228,\n",
       " 0.7172192931175232,\n",
       " 0.6818839311599731,\n",
       " 0.7754424810409546,\n",
       " 0.7201747894287109,\n",
       " 0.840723991394043,\n",
       " 0.7588136196136475,\n",
       " 0.7308091521263123,\n",
       " 0.7338045239448547,\n",
       " 0.749742865562439,\n",
       " 0.698084831237793,\n",
       " 0.748115062713623,\n",
       " 0.7005419731140137,\n",
       " 0.6630058288574219,\n",
       " 0.7714604139328003,\n",
       " 0.6861684918403625,\n",
       " 0.6653071641921997,\n",
       " 0.7461867332458496,\n",
       " 0.7320912480354309,\n",
       " 0.7682809829711914,\n",
       " 0.7486746311187744,\n",
       " 0.7598220109939575,\n",
       " 0.8322468400001526,\n",
       " 0.7931525111198425,\n",
       " 0.6388705372810364,\n",
       " 0.6155565977096558,\n",
       " 0.5768947601318359,\n",
       " 0.546116828918457,\n",
       " 0.5597176551818848,\n",
       " 0.5253536105155945,\n",
       " 0.5912841558456421,\n",
       " 0.5858394503593445,\n",
       " 0.5635307431221008,\n",
       " 0.5689256191253662,\n",
       " 0.6190176010131836,\n",
       " 0.650532066822052,\n",
       " 0.6515794992446899,\n",
       " 0.5602166056632996,\n",
       " 0.5212726593017578,\n",
       " 0.5386244654655457,\n",
       " 0.6356822848320007,\n",
       " 0.5701479315757751,\n",
       " 0.6262850165367126,\n",
       " 0.7060710191726685,\n",
       " 0.6745471358299255,\n",
       " 0.5404924750328064,\n",
       " 0.5654136538505554,\n",
       " 0.5847092866897583,\n",
       " 0.6306657195091248,\n",
       " 0.5804941058158875,\n",
       " 0.6380680203437805,\n",
       " 0.6539947986602783,\n",
       " 0.5542131066322327,\n",
       " 0.6348975896835327,\n",
       " 0.584627628326416,\n",
       " 0.5997937917709351,\n",
       " 0.5937380194664001,\n",
       " 0.6584171056747437,\n",
       " 0.6206854581832886,\n",
       " 0.5955963134765625,\n",
       " 0.5789535641670227,\n",
       " 0.578291654586792,\n",
       " 0.6325463056564331,\n",
       " 0.6912416815757751,\n",
       " 0.6248672604560852,\n",
       " 0.5741423964500427,\n",
       " 0.6286318302154541,\n",
       " 0.677492082118988,\n",
       " 0.6042484045028687,\n",
       " 0.5721051096916199,\n",
       " 0.5167369842529297,\n",
       " 0.46814653277397156,\n",
       " 0.4396958351135254,\n",
       " 0.45660245418548584,\n",
       " 0.47453975677490234,\n",
       " 0.5314374566078186,\n",
       " 0.47082245349884033,\n",
       " 0.4659220278263092,\n",
       " 0.4994382858276367,\n",
       " 0.4356791377067566,\n",
       " 0.5058021545410156,\n",
       " 0.45632535219192505,\n",
       " 0.6048591732978821,\n",
       " 0.5073041915893555,\n",
       " 0.5258936882019043,\n",
       " 0.47715967893600464,\n",
       " 0.586896538734436,\n",
       " 0.44438788294792175,\n",
       " 0.49599960446357727,\n",
       " 0.4426088035106659,\n",
       " 0.5004734396934509,\n",
       " 0.43714457750320435,\n",
       " 0.5115941762924194,\n",
       " 0.4780704975128174,\n",
       " 0.5139389634132385,\n",
       " 0.4781871438026428,\n",
       " 0.4901900887489319,\n",
       " 0.546367883682251,\n",
       " 0.5454166531562805,\n",
       " 0.48476672172546387,\n",
       " 0.5107749104499817,\n",
       " 0.5603461861610413,\n",
       " 0.5325469970703125,\n",
       " 0.4807659089565277,\n",
       " 0.4479934275150299,\n",
       " 0.44569531083106995,\n",
       " 0.5199058055877686,\n",
       " 0.5493770837783813,\n",
       " 0.46240538358688354,\n",
       " 0.47028419375419617,\n",
       " 0.4971160590648651,\n",
       " 0.5048755407333374,\n",
       " 0.46021753549575806,\n",
       " 0.49180251359939575,\n",
       " 0.544113278388977,\n",
       " 0.46735426783561707,\n",
       " 0.47315752506256104,\n",
       " 0.35804224014282227,\n",
       " 0.3662608861923218,\n",
       " 0.44594743847846985,\n",
       " 0.41259002685546875,\n",
       " 0.33821800351142883,\n",
       " 0.36996808648109436,\n",
       " 0.35864341259002686,\n",
       " 0.3765327036380768,\n",
       " 0.36227527260780334,\n",
       " 0.3691508173942566,\n",
       " 0.4362126588821411,\n",
       " 0.3631206750869751,\n",
       " 0.37267276644706726,\n",
       " 0.4214928448200226,\n",
       " 0.3909929394721985,\n",
       " 0.4220268428325653,\n",
       " 0.36787745356559753,\n",
       " 0.40402281284332275,\n",
       " 0.42902687191963196,\n",
       " 0.45008552074432373,\n",
       " 0.3358616828918457,\n",
       " 0.37502580881118774,\n",
       " 0.43408578634262085,\n",
       " 0.39326098561286926,\n",
       " 0.3946205973625183,\n",
       " 0.34800881147384644,\n",
       " 0.3761940896511078,\n",
       " 0.3928402066230774,\n",
       " 0.4021502733230591,\n",
       " 0.41385939717292786,\n",
       " 0.42661890387535095,\n",
       " 0.4160657227039337,\n",
       " 0.4058545231819153,\n",
       " 0.4118621051311493,\n",
       " 0.412914901971817,\n",
       " 0.518338680267334,\n",
       " 0.4287538230419159,\n",
       " 0.42595404386520386,\n",
       " 0.4077165722846985,\n",
       " 0.44512245059013367,\n",
       " 0.39076557755470276,\n",
       " 0.4654123783111572,\n",
       " 0.3709292709827423,\n",
       " 0.37131017446517944,\n",
       " 0.4176793098449707,\n",
       " 0.38704752922058105,\n",
       " 0.3400688171386719,\n",
       " 0.35889580845832825,\n",
       " 0.2897503674030304,\n",
       " 0.298482745885849,\n",
       " 0.30907654762268066,\n",
       " 0.248632550239563,\n",
       " 0.30112984776496887,\n",
       " 0.31646910309791565,\n",
       " 0.32143932580947876,\n",
       " 0.28359028697013855,\n",
       " 0.34164881706237793,\n",
       " 0.28976690769195557,\n",
       " 0.2777842879295349,\n",
       " 0.3395008444786072,\n",
       " 0.3119281828403473,\n",
       " 0.2975405156612396,\n",
       " 0.2916087806224823,\n",
       " 0.3154858350753784,\n",
       " 0.3409556746482849,\n",
       " 0.29892539978027344,\n",
       " 0.325128436088562,\n",
       " 0.33080339431762695,\n",
       " 0.3064567446708679,\n",
       " 0.3434503674507141,\n",
       " 0.3672501742839813,\n",
       " 0.324136346578598,\n",
       " 0.3019908666610718,\n",
       " 0.34068867564201355,\n",
       " 0.298820823431015,\n",
       " 0.3307231664657593,\n",
       " 0.3400646448135376,\n",
       " 0.2843112349510193,\n",
       " 0.3158112168312073,\n",
       " 0.3607209026813507,\n",
       " 0.30930620431900024,\n",
       " 0.31376031041145325,\n",
       " 0.3739597797393799,\n",
       " 0.3283163011074066,\n",
       " 0.36024728417396545,\n",
       " 0.3344534635543823,\n",
       " 0.3488830029964447,\n",
       " 0.30355170369148254,\n",
       " 0.3299176096916199,\n",
       " 0.3105131983757019,\n",
       " 0.3713945150375366,\n",
       " 0.33922824263572693,\n",
       " 0.22336241602897644,\n",
       " 0.2360774576663971,\n",
       " 0.22253942489624023,\n",
       " 0.23170018196105957,\n",
       " 0.20286917686462402,\n",
       " 0.24696898460388184,\n",
       " 0.23915432393550873,\n",
       " 0.2569713592529297,\n",
       " 0.27730685472488403,\n",
       " 0.26713883876800537,\n",
       " 0.26064857840538025,\n",
       " 0.24786409735679626,\n",
       " 0.2752135097980499,\n",
       " 0.2227199226617813,\n",
       " 0.2825482189655304,\n",
       " 0.2618219554424286,\n",
       " 0.242655411362648,\n",
       " 0.23937347531318665,\n",
       " 0.2540149688720703,\n",
       " 0.24638040363788605,\n",
       " 0.2778407335281372,\n",
       " 0.3086299002170563,\n",
       " 0.24525441229343414,\n",
       " 0.2425277829170227,\n",
       " 0.24632999300956726,\n",
       " 0.2520732581615448,\n",
       " 0.2618217170238495,\n",
       " 0.24796979129314423,\n",
       " 0.2289082407951355,\n",
       " 0.2534090578556061,\n",
       " 0.2644352316856384,\n",
       " 0.2604921758174896,\n",
       " 0.23752501606941223,\n",
       " 0.30606985092163086,\n",
       " 0.2709723114967346,\n",
       " 0.23474232852458954,\n",
       " 0.3029481768608093,\n",
       " 0.26607462763786316,\n",
       " 0.20935094356536865,\n",
       " 0.23531387746334076,\n",
       " 0.2688525915145874,\n",
       " 0.29768750071525574,\n",
       " 0.24164490401744843,\n",
       " 0.25630444288253784,\n",
       " 0.2925727367401123,\n",
       " 0.2698366940021515,\n",
       " 0.18987315893173218,\n",
       " 0.16700077056884766,\n",
       " 0.22121316194534302,\n",
       " 0.17961470782756805,\n",
       " 0.19682593643665314,\n",
       " 0.17305588722229004,\n",
       " 0.1584433615207672,\n",
       " 0.21718475222587585,\n",
       " 0.18489199876785278,\n",
       " 0.19668181240558624,\n",
       " 0.19273151457309723,\n",
       " 0.20211297273635864,\n",
       " 0.19016462564468384,\n",
       " 0.1953018456697464,\n",
       " 0.1781027764081955,\n",
       " 0.1933722347021103,\n",
       " 0.18111012876033783,\n",
       " 0.19671379029750824,\n",
       " 0.2017238587141037,\n",
       " 0.19837333261966705,\n",
       " 0.21861407160758972,\n",
       " 0.1786155253648758,\n",
       " 0.2004946619272232,\n",
       " 0.19297374784946442,\n",
       " 0.168813556432724,\n",
       " 0.24441833794116974,\n",
       " 0.19716694951057434,\n",
       " 0.19279512763023376,\n",
       " 0.20685334503650665,\n",
       " 0.21231135725975037,\n",
       " 0.19136299192905426,\n",
       " 0.17993634939193726,\n",
       " 0.1894383281469345,\n",
       " 0.1835183948278427,\n",
       " 0.18277306854724884,\n",
       " 0.195730522274971,\n",
       " 0.21931041777133942,\n",
       " 0.20997820794582367,\n",
       " 0.24028468132019043,\n",
       " 0.24424518644809723,\n",
       " 0.19518540799617767,\n",
       " 0.23069365322589874,\n",
       " 0.24900543689727783,\n",
       " 0.18347017467021942,\n",
       " 0.20767401158809662,\n",
       " 0.1887432187795639,\n",
       " 0.13020649552345276,\n",
       " 0.15684741735458374,\n",
       " 0.14676865935325623,\n",
       " 0.1368359923362732,\n",
       " 0.1527252197265625,\n",
       " 0.1353883594274521,\n",
       " 0.16024059057235718,\n",
       " 0.1712166666984558,\n",
       " 0.15354779362678528,\n",
       " 0.13776558637619019,\n",
       " 0.16900929808616638,\n",
       " 0.15076884627342224,\n",
       " 0.16248995065689087,\n",
       " 0.1368570476770401,\n",
       " 0.15642693638801575,\n",
       " 0.14944128692150116,\n",
       " 0.1698388159275055,\n",
       " 0.1803298145532608,\n",
       " 0.14179806411266327,\n",
       " 0.16302113234996796,\n",
       " 0.12470337003469467,\n",
       " 0.16941632330417633,\n",
       " 0.16163557767868042,\n",
       " 0.15144498646259308,\n",
       " 0.13613660633563995,\n",
       " 0.15580515563488007,\n",
       " 0.17110654711723328,\n",
       " 0.1414238065481186,\n",
       " 0.13575486838817596,\n",
       " 0.1714363694190979,\n",
       " 0.13536851108074188,\n",
       " 0.16779416799545288,\n",
       " 0.15886612236499786,\n",
       " 0.1423337459564209,\n",
       " 0.1603117436170578,\n",
       " 0.15067681670188904,\n",
       " 0.15650077164173126,\n",
       " 0.15174171328544617,\n",
       " 0.17904405295848846,\n",
       " 0.19216148555278778,\n",
       " 0.1690872758626938,\n",
       " 0.14796458184719086,\n",
       " 0.1567162275314331,\n",
       " 0.17094293236732483,\n",
       " 0.13999155163764954,\n",
       " 0.12530195713043213,\n",
       " 0.08709749579429626,\n",
       " 0.10638059675693512,\n",
       " 0.09155071526765823,\n",
       " 0.12501801550388336,\n",
       " 0.1157815232872963,\n",
       " 0.10762307047843933,\n",
       " 0.11938294768333435,\n",
       " 0.12807407975196838,\n",
       " 0.11996911466121674,\n",
       " 0.13632386922836304,\n",
       " 0.12825092673301697,\n",
       " 0.11061662435531616,\n",
       " 0.12254523485898972,\n",
       " 0.10326611995697021,\n",
       " 0.10421637445688248,\n",
       " 0.12950211763381958,\n",
       " 0.13923147320747375,\n",
       " 0.1149134561419487,\n",
       " 0.12917059659957886,\n",
       " 0.13198146224021912,\n",
       " 0.12478919327259064,\n",
       " 0.1169690415263176,\n",
       " 0.13211728632450104,\n",
       " 0.11836855113506317,\n",
       " 0.11311608552932739,\n",
       " 0.10447042435407639,\n",
       " 0.11733793467283249,\n",
       " 0.15347401797771454,\n",
       " 0.13347500562667847,\n",
       " 0.11639124900102615,\n",
       " 0.12655895948410034,\n",
       " 0.1261157989501953,\n",
       " 0.12129944562911987,\n",
       " 0.10643107444047928,\n",
       " 0.136415034532547,\n",
       " 0.116417795419693,\n",
       " 0.16374868154525757,\n",
       " 0.12168525159358978,\n",
       " 0.13212135434150696,\n",
       " 0.11671800911426544,\n",
       " 0.10499731451272964,\n",
       " 0.15526974201202393,\n",
       " 0.17014893889427185,\n",
       " 0.12686994671821594,\n",
       " 0.13715772330760956,\n",
       " 0.11514867842197418,\n",
       " 0.06966620683670044,\n",
       " 0.08021767437458038,\n",
       " 0.08146627247333527,\n",
       " 0.08787454664707184,\n",
       " 0.08999686688184738,\n",
       " 0.08844436705112457,\n",
       " 0.07464811205863953,\n",
       " 0.10082685947418213,\n",
       " 0.086742103099823,\n",
       " 0.0809723362326622,\n",
       " 0.10152210295200348,\n",
       " 0.10384954512119293,\n",
       " 0.0976659506559372,\n",
       " 0.10430818796157837,\n",
       " 0.0834665447473526,\n",
       " 0.08702436089515686,\n",
       " 0.12424042820930481,\n",
       " 0.0762983039021492,\n",
       " 0.09628339111804962,\n",
       " 0.10595032572746277,\n",
       " 0.09036505222320557,\n",
       " 0.08863317966461182,\n",
       " 0.07705289870500565,\n",
       " 0.12811379134655,\n",
       " 0.08663631975650787,\n",
       " 0.10591764003038406,\n",
       " 0.08487275242805481,\n",
       " 0.09231065958738327,\n",
       " 0.09331534057855606,\n",
       " 0.10335766524076462,\n",
       " 0.11026023328304291,\n",
       " 0.09129251539707184,\n",
       " 0.10266480594873428,\n",
       " 0.09763427823781967,\n",
       " 0.10658043622970581,\n",
       " 0.08985824137926102,\n",
       " 0.09677929431200027,\n",
       " 0.1099269911646843,\n",
       " 0.10947459936141968,\n",
       " 0.10082772374153137,\n",
       " 0.09080665558576584,\n",
       " 0.08579132705926895,\n",
       " 0.10366754978895187,\n",
       " 0.11018858104944229,\n",
       " 0.10933475196361542,\n",
       " 0.11309868842363358,\n",
       " 0.06266819685697556,\n",
       " 0.06876122206449509,\n",
       " 0.06935232877731323,\n",
       " 0.08144248276948929,\n",
       " 0.06804515421390533,\n",
       " 0.09669020026922226,\n",
       " 0.0803757756948471,\n",
       " 0.06451163440942764,\n",
       " 0.06661234050989151,\n",
       " 0.0707564651966095,\n",
       " 0.0796087384223938,\n",
       " 0.08691960573196411,\n",
       " 0.06948728114366531,\n",
       " 0.058838117867708206,\n",
       " 0.07023841142654419,\n",
       " 0.07613259553909302,\n",
       " 0.066282719373703,\n",
       " 0.09135723114013672,\n",
       " 0.08671200275421143,\n",
       " 0.06773321330547333,\n",
       " 0.08195102959871292,\n",
       " 0.0683397650718689,\n",
       " 0.0673523098230362,\n",
       " 0.06171059235930443,\n",
       " 0.06308432668447495,\n",
       " 0.07566051930189133,\n",
       " 0.07111886143684387,\n",
       " 0.06827061623334885,\n",
       " 0.054829392582178116,\n",
       " 0.07826936990022659,\n",
       " 0.0987461730837822,\n",
       " 0.08977311104536057,\n",
       " 0.06814117729663849,\n",
       " 0.08736352622509003,\n",
       " 0.07271470874547958,\n",
       " 0.0779830813407898,\n",
       " 0.08031505346298218,\n",
       " 0.06828225404024124,\n",
       " 0.08291908353567123,\n",
       " 0.07634249329566956,\n",
       " 0.07662085443735123,\n",
       " 0.07722622901201248,\n",
       " 0.09256313741207123,\n",
       " 0.08063061535358429,\n",
       " 0.09139800071716309,\n",
       " 0.0826098844408989,\n",
       " 0.04661701247096062,\n",
       " 0.05606332793831825,\n",
       " 0.04206058755517006,\n",
       " 0.060659218579530716,\n",
       " 0.052132636308670044,\n",
       " 0.050632815808057785,\n",
       " 0.061092592775821686,\n",
       " 0.0513133630156517,\n",
       " 0.05562075972557068,\n",
       " 0.04656831920146942,\n",
       " 0.06659554690122604,\n",
       " 0.055995479226112366,\n",
       " 0.07898817211389542,\n",
       " 0.05520697683095932,\n",
       " 0.05301579833030701,\n",
       " 0.05767161026597023,\n",
       " 0.05279683694243431,\n",
       " 0.07279936224222183,\n",
       " 0.06931841373443604,\n",
       " 0.05346936732530594,\n",
       " 0.08698362112045288,\n",
       " 0.05588797479867935,\n",
       " 0.06455879658460617,\n",
       " 0.07351373136043549,\n",
       " 0.05989997833967209,\n",
       " 0.051755622029304504,\n",
       " 0.06000736728310585,\n",
       " 0.08421657979488373,\n",
       " 0.06506482511758804,\n",
       " 0.05577487125992775,\n",
       " 0.06094928830862045,\n",
       " 0.05369436740875244,\n",
       " 0.05391847714781761,\n",
       " 0.05162756145000458,\n",
       " ...]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss.logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "    \n",
    "  def __init__(self, encoder, decoder, input_text_processor,\n",
    "               output_text_processor):\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.input_text_processor = input_text_processor\n",
    "    self.output_text_processor = output_text_processor\n",
    "\n",
    "    self.output_token_string_from_index = (\n",
    "        tf.keras.layers.StringLookup(\n",
    "            vocabulary=output_text_processor.get_vocabulary(),\n",
    "            mask_token='',\n",
    "            invert=True))\n",
    "\n",
    "    # The output should never generate padding, unknown, or start.\n",
    "    index_from_string = tf.keras.layers.StringLookup(\n",
    "        vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
    "    token_mask_ids = index_from_string(['', '[unk]', '[start]']).numpy()\n",
    "\n",
    "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
    "    token_mask[np.array(token_mask_ids)] = True\n",
    "    self.token_mask = token_mask\n",
    "\n",
    "    self.start_token = index_from_string(tf.constant('[start]'))\n",
    "    self.end_token = index_from_string(tf.constant('[end]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "translator = Translator(\n",
    "    encoder=train_translator.encoder,\n",
    "    decoder=train_translator.decoder,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_text(self, result_tokens):\n",
    "    \n",
    "    result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
    "   \n",
    "\n",
    "    result_text = tf.strings.reduce_join(result_text_tokens,\n",
    "                                        axis=1, separator=' ')\n",
    "    \n",
    "\n",
    "    result_text = tf.strings.strip(result_text)\n",
    "    \n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "Translator.tokens_to_text = tokens_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_output_tokens = tf.random.uniform(\n",
    "#     shape=[5, 2], minval=0, dtype=tf.int64,\n",
    "#     maxval=output_text_processor.vocabulary_size())\n",
    "# translator.tokens_to_text(example_output_tokens).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(self, logits, temperature):\n",
    "\n",
    "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
    "    logits = tf.where(self.token_mask, -np.inf, logits)\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        new_tokens = tf.argmax(logits, axis=-1)\n",
    "    else: \n",
    "        logits = tf.squeeze(logits, axis=1)\n",
    "        new_tokens = tf.random.categorical(logits/temperature,\n",
    "                                            num_samples=1)\n",
    "\n",
    "    return new_tokens\n",
    "\n",
    "Translator.sample = sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
       "array([[ 661],\n",
       "       [ 323],\n",
       "       [ 738],\n",
       "       [ 911],\n",
       "       [1866]], dtype=int64)>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_logits = tf.random.normal([5, 1, output_text_processor.vocabulary_size()])\n",
    "example_output_tokens = translator.sample(example_logits, temperature=1.0)\n",
    "example_output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_unrolled(self,\n",
    "                       input_text, *,\n",
    "                       max_length=50,\n",
    "                       return_attention=True,\n",
    "                       temperature=1.0):\n",
    "  batch_size = tf.shape(input_text)[0]\n",
    "  input_tokens = self.input_text_processor(input_text)\n",
    "  enc_output, enc_state = self.encoder(input_tokens)\n",
    "\n",
    "  dec_state = enc_state\n",
    "  new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "\n",
    "  result_tokens = []\n",
    "  attention = []\n",
    "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "\n",
    "  for _ in range(max_length):\n",
    "    dec_input = DecoderInput(new_tokens=new_tokens,\n",
    "                             enc_output=enc_output,\n",
    "                             mask=(input_tokens!=0))\n",
    "\n",
    "    dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
    "\n",
    "    attention.append(dec_result.attention_weights)\n",
    "\n",
    "    new_tokens = self.sample(dec_result.logits, temperature)\n",
    "\n",
    "    # If a sequence produces an `end_token`, set it `done`\n",
    "    done = done | (new_tokens == self.end_token)\n",
    "    # Once a sequence is done it only produces 0-padding.\n",
    "    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
    "\n",
    "    # Collect the generated tokens\n",
    "    result_tokens.append(new_tokens)\n",
    "\n",
    "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "      break\n",
    "\n",
    "  # Convert the list of generates token ids to a list of strings.\n",
    "  result_tokens = tf.concat(result_tokens, axis=-1)\n",
    "  result_text = self.tokens_to_text(result_tokens)\n",
    "\n",
    "  if return_attention:\n",
    "    attention_stack = tf.concat(attention, axis=1)\n",
    "    return {'text': result_text, 'attention': attention_stack}\n",
    "  else:\n",
    "    return {'text': result_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "Translator.translate = translate_unrolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select count ( * ) from event\n",
      "sum select count ( * ) from department )\n",
      "\n",
      "Wall time: 245 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_text = tf.constant([\n",
    "    'How many singers do we have?', # \"It's really cold here.\"\n",
    "    'What is the total number of singers?', # \"This is my life.\"\"\n",
    "])\n",
    "\n",
    "result = translator.translate(\n",
    "    input_text = input_text)\n",
    "\n",
    "print(result['text'][0].numpy().decode())\n",
    "print(result['text'][1].numpy().decode())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select count ( * ) from team_franchise where active = 'y';\n",
      "select count ( * ) from products where product_category_code = \"spices\" and typical_buying_price > 1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = list(handlr.df_test['question'].values)\n",
    "\n",
    "input_text = tf.constant(test)\n",
    "\n",
    "result = translator.translate(\n",
    "    input_text = input_text)\n",
    "\n",
    "print(result['text'][0].numpy().decode())\n",
    "print(result['text'][1].numpy().decode())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = handlr.df_test['query'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 33.56 %\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(test)):\n",
    "    # print(test[i])\n",
    "    # print(\"Predicted\")\n",
    "    pred = result['text'][i].numpy().decode().lower().replace(' ','')\n",
    "    # print(pred)\n",
    "    # print(\"Actual\")\n",
    "    act = query[i].lower().replace(' ','')\n",
    "    # print(act)\n",
    "    if pred == act:\n",
    "        count = count +1\n",
    "print(\"Accuracy is {:.2f} %\".format(count*100/len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "24c7168f742075337832b93c1fbfd4355e111387279ce758eae35d9b42cad7f4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
