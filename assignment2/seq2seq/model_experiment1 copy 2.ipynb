{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import tensorflow as tf\n",
    "import typing\n",
    "from typing import Any, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file at path D:/DS/Learnin/Essex MS/CE888/git/CE888/assignment2/seq2seq/spider/train_spider.json\n",
      "7000 Rows in Total\n",
      "Reading file at path D:/DS/Learnin/Essex MS/CE888/git/CE888/assignment2/seq2seq/spider/train_others.json\n",
      "8659 Rows in Total\n",
      "Filter Easy Queries\n",
      "Splittin the Train and Test data\n",
      "(727, 7)\n",
      "Data for Training (2908, 7)\n",
      "Data for Testing (727, 7)\n",
      "Sample Vocabulary ['', '[UNK]', 'the', '[start]', '[end]', 'of', '?', '.', 'what', 'are']\n",
      "Sample Vocabulary ['', '[UNK]', 'select', 'from', '[start]', '[end]', 'where', ')', '(', '=']\n"
     ]
    }
   ],
   "source": [
    "handlr = data.Train_Data('D:/DS/Learnin/Essex MS/CE888/git/CE888/assignment2/seq2seq/spider/',['train_spider.json','train_others.json'])\n",
    "input_text_processor = data.Features().vectorizor(handlr.questions , data.Max_Vocab_Size)\n",
    "output_text_processor = data.Features().vectorizor(handlr.sql , data.Max_Vocab_Size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the List of Questions in inp and List of Queries as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = handlr.questions\n",
    "targ = handlr.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(inp)\n",
    "BATCH_SIZE = data.BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = data.embedding_dim\n",
    "units = data.units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "\n",
    "        # The embedding layer converts tokens to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
    "                                                embedding_dim)\n",
    "\n",
    "        # The GRU RNN layer processes those vectors sequentially.\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                    # Return the sequence and state\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, tokens, state=None):\n",
    "        \n",
    "\n",
    "\n",
    "        # 2. The embedding layer looks up the embedding for each token.\n",
    "        vectors = self.embedding(tokens)\n",
    "\n",
    "\n",
    "        # 3. The GRU processes the embedding sequence.\n",
    "        #    output shape: (batch, s, enc_units)\n",
    "        #    state shape: (batch, enc_units)\n",
    "        output, state = self.gru(vectors, initial_state=state)\n",
    "\n",
    "\n",
    "        # 4. Returns the new sequence and its state.\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # For Eqn. (4), the  Bahdanau attention\n",
    "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "        self.attention = tf.keras.layers.AdditiveAttention()\n",
    "\n",
    "    def call(self, query, value, mask):\n",
    "       \n",
    "        # From Eqn. (4), `W1@ht`.\n",
    "        w1_query = self.W1(query)\n",
    "       \n",
    "        # From Eqn. (4), `W2@hs`.\n",
    "        w2_key = self.W2(value)\n",
    "\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs = [w1_query, value, w2_key],\n",
    "            mask=[query_mask, value_mask],\n",
    "            return_attention_scores = True,\n",
    "        )\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.dec_units = dec_units\n",
    "    self.output_vocab_size = output_vocab_size\n",
    "    self.embedding_dim = embedding_dim\n",
    "\n",
    "    # For Step 1. The embedding layer convets token IDs to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
    "                                               embedding_dim)\n",
    "\n",
    "    # For Step 2. The RNN keeps track of what's been generated so far.\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    # For step 3. The RNN output will be the query for the attention layer.\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    # For step 4. Eqn. (3): converting `ct` to `at`\n",
    "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
    "                                    use_bias=False)\n",
    "\n",
    "    # For step 5. This fully connected layer produces the logits for each\n",
    "    # output token.\n",
    "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderInput(typing.NamedTuple):\n",
    "    new_tokens: Any\n",
    "    enc_output: Any\n",
    "    mask: Any\n",
    "\n",
    "class DecoderOutput(typing.NamedTuple):\n",
    "    logits: Any\n",
    "    attention_weights: Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self, inputs: DecoderInput, state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
    "\n",
    "  # Step 1. Lookup the embeddings\n",
    "  vectors = self.embedding(inputs.new_tokens)\n",
    " \n",
    "\n",
    "  # Step 2. Process one step with the RNN\n",
    "  rnn_output, state = self.gru(vectors, initial_state=state)\n",
    "\n",
    "  \n",
    "\n",
    "  # Step 3. Use the RNN output as the query for the attention over the\n",
    "  # encoder output.\n",
    "  context_vector, attention_weights = self.attention(\n",
    "      query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
    "  \n",
    "\n",
    "  # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
    "  #     [ct; ht] shape: (batch t, value_units + query_units)\n",
    "  context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
    "\n",
    "  # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
    "  attention_vector = self.Wc(context_and_rnn_output)\n",
    "\n",
    "  # Step 5. Generate logit predictions:\n",
    "  logits = self.fc(attention_vector)\n",
    "\n",
    "  return DecoderOutput(logits, attention_weights), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decoder.call = call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        self.name = 'masked_loss'\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        # Calculate the loss for each item in the batch.\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "\n",
    "        # Mask off the losses on padding.\n",
    "        mask = tf.cast(y_true != 0, tf.float32)\n",
    "        loss *= mask\n",
    "\n",
    "        # Return the total.\n",
    "        return tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTranslator(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units,\n",
    "               input_text_processor,\n",
    "               output_text_processor, \n",
    "               use_tf_function=True):\n",
    "        super().__init__()\n",
    "        # Build the encoder and decoder\n",
    "        encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                        embedding_dim, units)\n",
    "        decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                        embedding_dim, units)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "        self.use_tf_function = use_tf_function\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_train_step(inputs)\n",
    "        else:\n",
    "         return self._train_step(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess(self, input_text, target_text):\n",
    "  \n",
    "\n",
    "  # Convert the text to token IDs\n",
    "  input_tokens = self.input_text_processor(input_text)\n",
    "  target_tokens = self.output_text_processor(target_text)\n",
    "  \n",
    "\n",
    "  # Convert IDs to masks.\n",
    "  input_mask = input_tokens != 0\n",
    " \n",
    "\n",
    "  target_mask = target_tokens != 0\n",
    "  \n",
    "\n",
    "  return input_tokens, input_mask, target_tokens, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._preprocess = _preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_step(self, inputs):\n",
    "    input_text, target_text = inputs  \n",
    "\n",
    "    (input_tokens, input_mask,target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
    "\n",
    "    max_target_length = tf.shape(target_tokens)[1]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Encode the input\n",
    "        enc_output, enc_state = self.encoder(input_tokens)\n",
    "        \n",
    "\n",
    "        # Initialize the decoder's state to the encoder's final state.\n",
    "        # This only works if the encoder and decoder have the same number of\n",
    "        # units.\n",
    "        dec_state = enc_state\n",
    "        loss = tf.constant(0.0)\n",
    "\n",
    "        for t in tf.range(max_target_length-1):\n",
    "            # Pass in two tokens from the target sequence:\n",
    "            # 1. The current input to the decoder.\n",
    "            # 2. The target for the decoder's next prediction.\n",
    "            new_tokens = target_tokens[:, t:t+2]\n",
    "            step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
    "                                                    enc_output, dec_state)\n",
    "            loss = loss + step_loss\n",
    "\n",
    "    # Average the loss over all non padding tokens.\n",
    "        average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
    "\n",
    "    # Apply an optimization step\n",
    "    variables = self.trainable_variables \n",
    "    gradients = tape.gradient(average_loss, variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    # Return a dict mapping metric names to current value\n",
    "    return {'batch_loss': average_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._train_step = _train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
    "    input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
    "\n",
    "    # Run the decoder one step.\n",
    "    decoder_input = DecoderInput(new_tokens=input_token,\n",
    "                                enc_output=enc_output,\n",
    "                                mask=input_mask)\n",
    "\n",
    "    dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
    "\n",
    "\n",
    "    # `self.loss` returns the total for non-padded tokens\n",
    "    y = target_token\n",
    "    y_pred = dec_result.logits\n",
    "    # print(y_pred.as_numpy_iterator())\n",
    "    step_loss = self.loss(y, y_pred)\n",
    "\n",
    "    return step_loss, dec_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._loop_step = _loop_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = TrainTranslator(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    "    use_tf_function=False)\n",
    "\n",
    "# Configure the loss and optimizer\n",
    "translator.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=MaskedLoss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
    "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
    "def _tf_train_step(self, inputs):\n",
    "  return self._train_step(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._tf_train_step = _tf_train_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.use_tf_function = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_translator = TrainTranslator(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor)\n",
    "\n",
    "# Configure the loss and optimizer\n",
    "train_translator.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=MaskedLoss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLogs(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.logs = []\n",
    "\n",
    "    def on_train_batch_end(self, n, logs):\n",
    "        self.logs.append(logs[self.key])\n",
    "\n",
    "batch_loss = BatchLogs('batch_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "def get_run_logdir():\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Pankhuri\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Pankhuri\\AppData\\Local\\Temp\\ipykernel_25076\\1866338966.py\", line 4, in _tf_train_step  *\n        return self._train_step(inputs)\n    File \"C:\\Users\\Pankhuri\\AppData\\Local\\Temp\\ipykernel_25076\\97723317.py\", line 24, in _train_step  *\n        step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n    File \"C:\\Users\\Pankhuri\\AppData\\Local\\Temp\\ipykernel_25076\\51902675.py\", line 15, in _loop_step  *\n        text.metrics.rouge_l(y, y_pred)\n    File \"C:\\Users\\Pankhuri\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow_text\\python\\metrics\\text_similarity_metric_ops.py\", line 75, in rouge_l  *\n        raise ValueError('hypotheses must be a RaggedTensor')\n\n    ValueError: hypotheses must be a RaggedTensor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25076\\2524458281.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_translator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtensorboard_cb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1148\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Pankhuri\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Pankhuri\\AppData\\Local\\Temp\\ipykernel_25076\\1866338966.py\", line 4, in _tf_train_step  *\n        return self._train_step(inputs)\n    File \"C:\\Users\\Pankhuri\\AppData\\Local\\Temp\\ipykernel_25076\\97723317.py\", line 24, in _train_step  *\n        step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n    File \"C:\\Users\\Pankhuri\\AppData\\Local\\Temp\\ipykernel_25076\\51902675.py\", line 15, in _loop_step  *\n        text.metrics.rouge_l(y, y_pred)\n    File \"C:\\Users\\Pankhuri\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow_text\\python\\metrics\\text_similarity_metric_ops.py\", line 75, in rouge_l  *\n        raise ValueError('hypotheses must be a RaggedTensor')\n\n    ValueError: hypotheses must be a RaggedTensor\n"
     ]
    }
   ],
   "source": [
    "history = train_translator.fit(dataset, epochs=50, callbacks=[batch_loss,tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Batch Loss')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(batch_loss.logs)\n",
    "plt.ylim([0, 3])\n",
    "plt.xlabel('Batch Number')\n",
    "plt.ylabel('Batch Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss.logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "    \n",
    "  def __init__(self, encoder, decoder, input_text_processor,\n",
    "               output_text_processor):\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.input_text_processor = input_text_processor\n",
    "    self.output_text_processor = output_text_processor\n",
    "\n",
    "    self.output_token_string_from_index = (\n",
    "        tf.keras.layers.StringLookup(\n",
    "            vocabulary=output_text_processor.get_vocabulary(),\n",
    "            mask_token='',\n",
    "            invert=True))\n",
    "\n",
    "    # The output should never generate padding, unknown, or start.\n",
    "    index_from_string = tf.keras.layers.StringLookup(\n",
    "        vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
    "    token_mask_ids = index_from_string(['', '[unk]', '[start]']).numpy()\n",
    "\n",
    "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
    "    token_mask[np.array(token_mask_ids)] = True\n",
    "    self.token_mask = token_mask\n",
    "\n",
    "    self.start_token = index_from_string(tf.constant('[start]'))\n",
    "    self.end_token = index_from_string(tf.constant('[end]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(\n",
    "    encoder=train_translator.encoder,\n",
    "    decoder=train_translator.decoder,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_text(self, result_tokens):\n",
    "    \n",
    "    result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
    "   \n",
    "\n",
    "    result_text = tf.strings.reduce_join(result_text_tokens,\n",
    "                                        axis=1, separator=' ')\n",
    "    \n",
    "\n",
    "    result_text = tf.strings.strip(result_text)\n",
    "    \n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "Translator.tokens_to_text = tokens_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'2007 pets_allowed_yn', b'technician_id stuid',\n",
       "       b'keyword; \"edmonton\"', b'ycard job', b'2003 \"howard\"'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_output_tokens = tf.random.uniform(\n",
    "    shape=[5, 2], minval=0, dtype=tf.int64,\n",
    "    maxval=output_text_processor.vocabulary_size())\n",
    "translator.tokens_to_text(example_output_tokens).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(self, logits, temperature):\n",
    "\n",
    "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
    "    logits = tf.where(self.token_mask, -np.inf, logits)\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        new_tokens = tf.argmax(logits, axis=-1)\n",
    "    else: \n",
    "        logits = tf.squeeze(logits, axis=1)\n",
    "        new_tokens = tf.random.categorical(logits/temperature,\n",
    "                                            num_samples=1)\n",
    "\n",
    "    return new_tokens\n",
    "\n",
    "Translator.sample = sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
       "array([[ 468],\n",
       "       [1165],\n",
       "       [2046],\n",
       "       [1803],\n",
       "       [1674]], dtype=int64)>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_logits = tf.random.normal([5, 1, output_text_processor.vocabulary_size()])\n",
    "example_output_tokens = translator.sample(example_logits, temperature=1.0)\n",
    "example_output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_unrolled(self,\n",
    "                       input_text, *,\n",
    "                       max_length=50,\n",
    "                       return_attention=True,\n",
    "                       temperature=1.0):\n",
    "  batch_size = tf.shape(input_text)[0]\n",
    "  input_tokens = self.input_text_processor(input_text)\n",
    "  enc_output, enc_state = self.encoder(input_tokens)\n",
    "\n",
    "  dec_state = enc_state\n",
    "  new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "\n",
    "  result_tokens = []\n",
    "  attention = []\n",
    "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "\n",
    "  for _ in range(max_length):\n",
    "    dec_input = DecoderInput(new_tokens=new_tokens,\n",
    "                             enc_output=enc_output,\n",
    "                             mask=(input_tokens!=0))\n",
    "\n",
    "    dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
    "\n",
    "    attention.append(dec_result.attention_weights)\n",
    "\n",
    "    new_tokens = self.sample(dec_result.logits, temperature)\n",
    "\n",
    "    # If a sequence produces an `end_token`, set it `done`\n",
    "    done = done | (new_tokens == self.end_token)\n",
    "    # Once a sequence is done it only produces 0-padding.\n",
    "    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
    "\n",
    "    # Collect the generated tokens\n",
    "    result_tokens.append(new_tokens)\n",
    "\n",
    "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "      break\n",
    "\n",
    "  # Convert the list of generates token ids to a list of strings.\n",
    "  result_tokens = tf.concat(result_tokens, axis=-1)\n",
    "  result_text = self.tokens_to_text(result_tokens)\n",
    "\n",
    "  if return_attention:\n",
    "    attention_stack = tf.concat(attention, axis=1)\n",
    "    return {'text': result_text, 'attention': attention_stack}\n",
    "  else:\n",
    "    return {'text': result_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "Translator.translate = translate_unrolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select count ( * ) from wrestler\n",
      "select sum ( gamesplayed ) from sportsinfo\n",
      "\n",
      "Wall time: 192 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_text = tf.constant([\n",
    "    'How many singers do we have?', # \"It's really cold here.\"\n",
    "    'What is the total number of singers?', # \"This is my life.\"\"\n",
    "])\n",
    "\n",
    "result = translator.translate(\n",
    "    input_text = input_text)\n",
    "\n",
    "print(result['text'][0].numpy().decode())\n",
    "print(result['text'][1].numpy().decode())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "24c7168f742075337832b93c1fbfd4355e111387279ce758eae35d9b42cad7f4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
