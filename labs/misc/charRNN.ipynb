{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://homl.info/shakespeare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  tensorflow import keras\n",
    "fpath = keras.utils.get_file(\"shakespeare.txt\",url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fpath) as fp:\n",
    "    s_text = fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(s_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index)\n",
    "max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_size = tokenizer.document_count\n",
    "d_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([s_text]))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_s = d_size*90//100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "data = tf.data.Dataset.from_tensor_slices(encoded[:t_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "w_l =n_steps+1\n",
    "dataset = data.window(w_l,shift=1,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.sequences_to_texts([[19,5,8,7,2,0,18,5,2,5,35]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 5, 8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 21, 1, 19, 3, 8, 1, 0, 16, 1, 0, 22, 8, 3, 18, 1, 1, 12, 0, 4, 9, 15, 0, 19, 13, 8, 2, 6, 1, 8, 17, 0, 6, 1, 4, 8, 0, 14, 1, 0, 7, 22, 1, 4, 24, 26, 10, 10, 4, 11, 11, 23, 10, 7, 22, 1, 4, 24, 17, 0, 7, 22, 1, 4, 24, 26, 10, 10, 19, 5, 8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 15, 3, 13, 0]\n",
      "[5, 8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 21, 1, 19, 3, 8, 1, 0, 16, 1, 0, 22, 8, 3, 18, 1, 1, 12, 0, 4, 9, 15, 0, 19, 13, 8, 2, 6, 1, 8, 17, 0, 6, 1, 4, 8, 0, 14, 1, 0, 7, 22, 1, 4, 24, 26, 10, 10, 4, 11, 11, 23, 10, 7, 22, 1, 4, 24, 17, 0, 7, 22, 1, 4, 24, 26, 10, 10, 19, 5, 8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 15, 3, 13, 0, 4]\n",
      "[8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 21, 1, 19, 3, 8, 1, 0, 16, 1, 0, 22, 8, 3, 18, 1, 1, 12, 0, 4, 9, 15, 0, 19, 13, 8, 2, 6, 1, 8, 17, 0, 6, 1, 4, 8, 0, 14, 1, 0, 7, 22, 1, 4, 24, 26, 10, 10, 4, 11, 11, 23, 10, 7, 22, 1, 4, 24, 17, 0, 7, 22, 1, 4, 24, 26, 10, 10, 19, 5, 8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 15, 3, 13, 0, 4, 8]\n"
     ]
    }
   ],
   "source": [
    "for i in dataset.take(3):\n",
    "    print(list(i.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflat = dataset.flat_map(lambda window:window.batch(w_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 2  5 35  1  9 23 10 15  3 13  0], shape=(11,), dtype=int32)\n",
      "tf.Tensor([ 5 35  1  9 23 10 15  3 13  0  4], shape=(11,), dtype=int32)\n",
      "tf.Tensor([35  1  9 23 10 15  3 13  0  4  8], shape=(11,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for i in dflat.take(3):\n",
    "    print(i[90:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dflat = dflat.shuffle(1000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflat = dflat.map(lambda windows:(windows[:,:-1],windows[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 100), dtype=int32, numpy=\n",
      "array([[ 3,  9,  1, ..., 18,  5,  2],\n",
      "       [15,  5,  1, ..., 21, 13,  2],\n",
      "       [13,  2,  6, ..., 16,  6,  3],\n",
      "       ...,\n",
      "       [ 0,  5,  2, ...,  1, 12,  0],\n",
      "       [ 0, 22,  1, ...,  3, 16,  9],\n",
      "       [ 4,  9, 18, ...,  3,  8,  0]])>, <tf.Tensor: shape=(32, 100), dtype=int32, numpy=\n",
      "array([[ 9,  1, 23, ...,  5,  2,  5],\n",
      "       [ 5,  1, 11, ..., 13,  2,  0],\n",
      "       [ 2,  6,  3, ...,  6,  3, 11],\n",
      "       ...,\n",
      "       [ 5,  2,  0, ..., 12,  0, 22],\n",
      "       [22,  1,  3, ..., 16,  9,  0],\n",
      "       [ 9, 18,  1, ...,  8,  0,  2]])>)\n",
      "(<tf.Tensor: shape=(32, 100), dtype=int32, numpy=\n",
      "array([[26,  0,  8, ..., 24,  9,  3],\n",
      "       [ 0,  4, 16, ...,  2,  6,  1],\n",
      "       [ 1, 14,  0, ..., 20,  1,  8],\n",
      "       ...,\n",
      "       [ 3,  0, 22, ..., 24,  1,  7],\n",
      "       [ 3,  0, 14, ...,  0, 18,  5],\n",
      "       [ 1,  7,  3, ...,  4,  5, 13]])>, <tf.Tensor: shape=(32, 100), dtype=int32, numpy=\n",
      "array([[ 0,  8,  1, ...,  9,  3, 16],\n",
      "       [ 4, 16,  4, ...,  6,  1,  0],\n",
      "       [14,  0, 11, ...,  1,  8,  0],\n",
      "       ...,\n",
      "       [ 0, 22,  4, ...,  1,  7, 17],\n",
      "       [ 0, 14,  3, ..., 18,  5,  2],\n",
      "       [ 7,  3, 11, ...,  5, 13,  7]])>)\n"
     ]
    }
   ],
   "source": [
    "for i in dflat.take(2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.one_hot([1,2,4],depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflat = dflat.map(lambda X_b,Y_b: (tf.one_hot(X_b,depth=max_id),Y_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 100, 39), dtype=float32, numpy=\n",
      "array([[[1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>, <tf.Tensor: shape=(32, 100), dtype=int32, numpy=\n",
      "array([[19,  3,  8, ...,  5, 14,  0],\n",
      "       [ 0, 19,  3, ...,  7,  0, 14],\n",
      "       [ 4,  8,  5, ...,  1,  0, 21],\n",
      "       ...,\n",
      "       [ 3, 11, 25, ...,  2, 17,  0],\n",
      "       [ 1, 11,  5, ...,  1, 15,  0],\n",
      "       [ 4,  2,  0, ...,  1,  8,  1]])>)\n",
      "(<tf.Tensor: shape=(32, 100, 39), dtype=float32, numpy=\n",
      "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.]]], dtype=float32)>, <tf.Tensor: shape=(32, 100), dtype=int32, numpy=\n",
      "array([[ 0,  3, 16, ...,  1,  9, 23],\n",
      "       [16,  4, 15, ...,  1,  0, 22],\n",
      "       [ 8,  1, 25, ...,  5,  8,  7],\n",
      "       ...,\n",
      "       [17,  0,  4, ...,  1, 23,  0],\n",
      "       [ 3,  0,  2, ...,  0,  3, 13],\n",
      "       [ 4, 16,  4, ...,  6,  1,  0]])>)\n"
     ]
    }
   ],
   "source": [
    "for i in dflat.take(2):\n",
    "    print(i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflat = dflat.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, Y_batch in dflat.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "31368/31368 [==============================] - 3253s 104ms/step - loss: 1.6328\n",
      "Epoch 2/3\n",
      "31368/31368 [==============================] - 5425s 173ms/step - loss: 1.5685\n",
      "Epoch 3/3\n",
      "31368/31368 [==============================] - 3398s 108ms/step - loss: 1.5487\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     #dropout=0.2, recurrent_dropout=0.2),\n",
    "                     dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     #dropout=0.2, recurrent_dropout=0.2),\n",
    "                     dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dflat, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = preprocess(\"How are you\")\n",
    "Y_pred = np.argmax(model(X_new), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = preprocess([\"How are yo\"])\n",
    "#Y_pred = model.predict_classes(X_new)\n",
    "Y_pred = np.argmax(model(X_new), axis=-1)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.0459199e-04, 3.2932534e-05, 1.4920449e-03, 1.9692015e-05,\n",
       "        1.8722432e-04, 1.7949014e-05, 2.9811654e-05, 2.0848440e-05,\n",
       "        4.8933463e-04, 1.7941782e-04, 1.8850882e-05, 2.0682439e-06,\n",
       "        1.1006093e-05, 9.9567324e-01, 3.3501750e-05, 9.6338154e-05,\n",
       "        3.0981025e-04, 1.6488614e-04, 5.7076562e-05, 3.9632064e-06,\n",
       "        9.1000911e-06, 1.6556985e-06, 6.1151047e-07, 2.7117196e-05,\n",
       "        1.2799128e-06, 3.5007508e-04, 1.2869040e-04, 5.3823887e-06,\n",
       "        1.1158750e-05, 2.1245422e-04, 3.4638349e-06, 2.4673222e-06,\n",
       "        4.2870617e-08, 4.8004026e-08, 1.2040691e-07, 1.6494181e-06,\n",
       "        2.2848929e-09, 5.8693522e-11, 2.8754110e-11]], dtype=float32)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_new)[0,-1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10, 39), dtype=float32, numpy=\n",
       "array([[[3.76836918e-02, 4.27791715e-01, 9.95732471e-03, 1.01372473e-01,\n",
       "         2.13540092e-01, 9.34411734e-02, 2.11075624e-03, 6.65976433e-03,\n",
       "         5.99742169e-03, 2.27317121e-03, 3.56663438e-03, 5.39182685e-03,\n",
       "         2.90621724e-03, 1.54922921e-02, 1.48994348e-03, 2.98371091e-02,\n",
       "         9.17096331e-04, 1.53627349e-02, 6.64529915e-04, 1.94426475e-03,\n",
       "         1.20241055e-03, 1.88748367e-04, 7.27309962e-04, 4.87992307e-03,\n",
       "         7.58843904e-04, 4.71356208e-04, 5.29873371e-03, 3.10155726e-03,\n",
       "         8.84147827e-04, 1.94265228e-03, 1.64149306e-03, 3.06330592e-04,\n",
       "         4.59894582e-05, 8.91273012e-05, 1.66856280e-05, 4.32945817e-05,\n",
       "         9.30368174e-07, 9.28278752e-08, 8.31766513e-08],\n",
       "        [2.36906242e-02, 1.23658115e-02, 1.87193379e-02, 2.74189543e-02,\n",
       "         5.80734946e-03, 3.09599284e-03, 8.31784506e-04, 2.87892856e-02,\n",
       "         2.13397294e-01, 8.90928358e-02, 1.29531277e-03, 7.58156255e-02,\n",
       "         4.65186406e-03, 2.89359421e-01, 8.51517618e-02, 4.41361160e-04,\n",
       "         8.35554302e-02, 4.10742685e-03, 3.09588085e-03, 1.67070772e-03,\n",
       "         4.44036588e-04, 7.81662762e-04, 8.06768890e-03, 1.78880536e-03,\n",
       "         3.50069284e-04, 1.03080058e-02, 2.71923793e-03, 1.58506061e-03,\n",
       "         2.34790510e-04, 9.48054774e-04, 1.32300556e-04, 1.21784899e-04,\n",
       "         3.66221902e-05, 4.57994611e-06, 4.36172340e-05, 7.95666638e-05,\n",
       "         3.24118545e-08, 2.10622453e-09, 9.73278347e-10],\n",
       "        [5.56958020e-01, 3.44435908e-02, 1.09805567e-02, 2.95836804e-03,\n",
       "         3.44475498e-03, 1.36539107e-02, 1.20463478e-03, 2.27113843e-01,\n",
       "         2.66632112e-03, 4.73697111e-02, 9.05327126e-03, 1.76740196e-02,\n",
       "         2.71602459e-02, 1.09338442e-04, 1.75036886e-03, 1.14537521e-04,\n",
       "         1.27502193e-04, 1.33410683e-02, 1.96675144e-04, 1.17136736e-03,\n",
       "         6.15940022e-04, 6.60177262e-04, 2.11320701e-04, 4.39969031e-03,\n",
       "         2.37529143e-03, 3.82725178e-04, 1.72757590e-03, 1.32066319e-02,\n",
       "         1.61120866e-03, 1.60432886e-03, 8.94575729e-04, 7.83029885e-04,\n",
       "         1.04553037e-05, 2.72717921e-06, 1.81082237e-06, 2.04694170e-05,\n",
       "         1.25819064e-10, 2.72888673e-11, 9.04244301e-12],\n",
       "        [2.35227053e-03, 8.89728963e-03, 1.60632432e-01, 3.49643491e-02,\n",
       "         5.24642058e-02, 1.14839993e-01, 9.08223540e-02, 7.23029003e-02,\n",
       "         2.15466488e-02, 1.26565948e-01, 7.45747064e-04, 5.75538632e-03,\n",
       "         3.19304653e-02, 4.90385713e-03, 7.64519572e-02, 7.74290189e-02,\n",
       "         5.25500849e-02, 1.24962637e-04, 7.36690778e-03, 2.23005507e-02,\n",
       "         7.66168162e-03, 1.55263487e-02, 5.32007590e-03, 1.29989785e-04,\n",
       "         8.87423928e-04, 1.10752112e-03, 4.30535583e-05, 6.24823326e-04,\n",
       "         1.45379990e-05, 5.49487013e-05, 1.31656907e-05, 1.43414927e-05,\n",
       "         3.52784805e-03, 8.38428386e-05, 3.91137764e-05, 3.91673211e-06,\n",
       "         1.04967874e-08, 3.73873120e-11, 1.16986993e-11],\n",
       "        [5.49313962e-01, 1.35157304e-03, 2.60470547e-02, 1.10864767e-03,\n",
       "         3.02306522e-04, 1.80556381e-03, 4.60842671e-03, 8.70075598e-02,\n",
       "         2.08819620e-02, 1.83899254e-01, 2.42478144e-03, 5.32179065e-02,\n",
       "         2.86807609e-03, 1.15335884e-03, 4.01418796e-03, 2.80180015e-03,\n",
       "         7.55933626e-03, 7.59989896e-04, 9.11292341e-03, 1.73164308e-02,\n",
       "         5.21361688e-03, 3.12401121e-03, 1.00633036e-02, 1.94274005e-04,\n",
       "         6.08028669e-04, 7.98327965e-04, 1.96739202e-04, 1.48579350e-03,\n",
       "         1.79041715e-04, 2.73319136e-04, 1.28817737e-05, 7.41662188e-06,\n",
       "         6.68946723e-06, 1.45869699e-05, 2.65705719e-04, 1.16370848e-06,\n",
       "         6.60710997e-09, 4.79087568e-11, 2.36133699e-11],\n",
       "        [4.23119823e-03, 8.72961462e-01, 5.72870160e-03, 2.02836711e-02,\n",
       "         1.13911619e-02, 3.77625972e-02, 7.12042674e-04, 1.04102446e-02,\n",
       "         7.37216882e-03, 6.73370305e-05, 3.51315466e-05, 2.95655639e-03,\n",
       "         1.52308890e-03, 6.55572687e-04, 4.07715794e-03, 9.95021686e-03,\n",
       "         1.53449678e-03, 3.26734706e-04, 1.64214452e-03, 3.83898674e-04,\n",
       "         3.90635018e-04, 8.93600882e-05, 3.08677263e-04, 5.21568400e-05,\n",
       "         8.02990806e-04, 3.80527391e-03, 2.02194875e-04, 2.73452151e-05,\n",
       "         4.34748945e-05, 9.28664522e-05, 2.01792755e-05, 9.09718219e-05,\n",
       "         4.63346041e-05, 1.63249606e-05, 1.91617460e-06, 3.70702082e-06,\n",
       "         2.67665723e-10, 4.20456031e-10, 1.88639118e-10],\n",
       "        [9.23877478e-01, 3.01580597e-03, 2.65778828e-04, 1.29553358e-04,\n",
       "         8.02536611e-04, 3.50225833e-03, 2.44287163e-04, 8.63813516e-03,\n",
       "         5.70623524e-05, 2.12210696e-03, 8.51369835e-03, 9.93790018e-05,\n",
       "         1.49952379e-04, 2.36632277e-05, 6.63391897e-04, 1.43656880e-05,\n",
       "         3.77498829e-04, 1.31301191e-02, 2.52887694e-04, 1.48019768e-04,\n",
       "         1.11685004e-05, 1.45039130e-05, 4.84126758e-05, 3.45027004e-03,\n",
       "         2.65727995e-05, 3.17157479e-04, 2.02756170e-02, 1.93380145e-03,\n",
       "         1.59590913e-03, 2.25245743e-03, 1.39389420e-03, 2.61063455e-03,\n",
       "         1.86638863e-06, 7.05666616e-06, 4.40352596e-06, 2.82711990e-05,\n",
       "         2.20292923e-10, 2.90937968e-11, 6.22415444e-12],\n",
       "        [2.38905032e-03, 9.54456069e-03, 1.55273929e-01, 1.36156240e-02,\n",
       "         9.17269140e-02, 9.02100876e-02, 4.75122556e-02, 7.71673098e-02,\n",
       "         1.54183200e-02, 7.32384771e-02, 6.87377527e-04, 7.66820600e-03,\n",
       "         2.69914102e-02, 3.22264922e-03, 3.86578329e-02, 4.62785773e-02,\n",
       "         6.66991696e-02, 1.04069411e-04, 1.53047973e-02, 5.28440587e-02,\n",
       "         8.31192732e-02, 3.25000994e-02, 2.98220627e-02, 3.61826460e-05,\n",
       "         9.00673028e-03, 4.05046763e-03, 3.36721350e-05, 6.06309739e-04,\n",
       "         9.64055107e-06, 3.88310509e-05, 2.03714008e-05, 3.40815313e-05,\n",
       "         5.70322666e-03, 4.00038727e-04, 5.50262375e-05, 9.38604444e-06,\n",
       "         3.18448379e-08, 4.45724957e-10, 2.04232090e-10],\n",
       "        [1.00636086e-03, 1.08590156e-01, 4.97190631e-05, 8.82359982e-01,\n",
       "         2.59289169e-03, 2.70577474e-03, 2.61933223e-04, 1.80304472e-04,\n",
       "         5.80829219e-04, 6.18106715e-05, 2.01950006e-05, 3.97261174e-05,\n",
       "         1.79489798e-05, 6.96693023e-04, 1.79178533e-05, 2.97839928e-04,\n",
       "         1.85288118e-05, 1.06286760e-04, 1.19694905e-05, 2.48744363e-05,\n",
       "         9.00258601e-06, 5.45438779e-05, 4.65341600e-06, 2.07093526e-05,\n",
       "         2.09807718e-06, 3.61552447e-05, 1.18735858e-04, 2.07438661e-05,\n",
       "         4.65237135e-05, 9.19011381e-06, 5.53934342e-06, 2.20725178e-06,\n",
       "         3.41328291e-06, 1.65336326e-06, 2.28578920e-05, 2.33159572e-08,\n",
       "         1.75454517e-07, 3.89310528e-10, 2.09338574e-10],\n",
       "        [4.04591992e-04, 3.29325339e-05, 1.49204489e-03, 1.96920155e-05,\n",
       "         1.87224316e-04, 1.79490144e-05, 2.98116538e-05, 2.08484398e-05,\n",
       "         4.89334634e-04, 1.79417824e-04, 1.88508820e-05, 2.06824393e-06,\n",
       "         1.10060928e-05, 9.95673239e-01, 3.35017503e-05, 9.63381535e-05,\n",
       "         3.09810246e-04, 1.64886136e-04, 5.70765624e-05, 3.96320638e-06,\n",
       "         9.10009112e-06, 1.65569850e-06, 6.11510472e-07, 2.71171957e-05,\n",
       "         1.27991279e-06, 3.50075075e-04, 1.28690401e-04, 5.38238874e-06,\n",
       "         1.11587497e-05, 2.12454222e-04, 3.46383490e-06, 2.46732225e-06,\n",
       "         4.28706173e-08, 4.80040256e-08, 1.20406909e-07, 1.64941810e-06,\n",
       "         2.28489294e-09, 5.86935223e-11, 2.87541102e-11]]], dtype=float32)>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:t_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 5, 8, 7, 2, 0, 18, 5, 2, 5]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.take(10).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.window(w_l, shift=n_steps, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 5, 8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 21, 1, 19, 3, 8, 1, 0, 16, 1, 0, 22, 8, 3, 18, 1, 1, 12, 0, 4, 9, 15, 0, 19, 13, 8, 2, 6, 1, 8, 17, 0, 6, 1, 4, 8, 0, 14, 1, 0, 7, 22, 1, 4, 24, 26, 10, 10, 4, 11, 11, 23, 10, 7, 22, 1, 4, 24, 17, 0, 7, 22, 1, 4, 24, 26, 10, 10, 19, 5, 8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 15, 3, 13, 0]\n",
      "[0, 4, 8, 1, 0, 4, 11, 11, 0, 8, 1, 7, 3, 11, 25, 1, 12, 0, 8, 4, 2, 6, 1, 8, 0, 2, 3, 0, 12, 5, 1, 0, 2, 6, 4, 9, 0, 2, 3, 0, 19, 4, 14, 5, 7, 6, 29, 10, 10, 4, 11, 11, 23, 10, 8, 1, 7, 3, 11, 25, 1, 12, 26, 0, 8, 1, 7, 3, 11, 25, 1, 12, 26, 10, 10, 19, 5, 8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 19, 5, 8, 7, 2, 17, 0, 15, 3, 13, 0]\n",
      "[0, 24, 9, 3, 16, 0, 18, 4, 5, 13, 7, 0, 14, 4, 8, 18, 5, 13, 7, 0, 5, 7, 0, 18, 6, 5, 1, 19, 0, 1, 9, 1, 14, 15, 0, 2, 3, 0, 2, 6, 1, 0, 22, 1, 3, 22, 11, 1, 26, 10, 10, 4, 11, 11, 23, 10, 16, 1, 0, 24, 9, 3, 16, 27, 2, 17, 0, 16, 1, 0, 24, 9, 3, 16, 27, 2, 26, 10, 10, 19, 5, 8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 11, 1, 2, 0, 13, 7, 0]\n"
     ]
    }
   ],
   "source": [
    "for i in dataset.take(3):\n",
    "    print(list(i.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(w_l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[19  5  8  7  2  0 18  5  2  5 35  1  9 23 10 21  1 19  3  8  1  0 16  1\n",
      "  0 22  8  3 18  1  1 12  0  4  9 15  0 19 13  8  2  6  1  8 17  0  6  1\n",
      "  4  8  0 14  1  0  7 22  1  4 24 26 10 10  4 11 11 23 10  7 22  1  4 24\n",
      " 17  0  7 22  1  4 24 26 10 10 19  5  8  7  2  0 18  5  2  5 35  1  9 23\n",
      " 10 15  3 13  0], shape=(101,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[ 0  4  8  1  0  4 11 11  0  8  1  7  3 11 25  1 12  0  8  4  2  6  1  8\n",
      "  0  2  3  0 12  5  1  0  2  6  4  9  0  2  3  0 19  4 14  5  7  6 29 10\n",
      " 10  4 11 11 23 10  8  1  7  3 11 25  1 12 26  0  8  1  7  3 11 25  1 12\n",
      " 26 10 10 19  5  8  7  2  0 18  5  2  5 35  1  9 23 10 19  5  8  7  2 17\n",
      "  0 15  3 13  0], shape=(101,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[ 0 24  9  3 16  0 18  4  5 13  7  0 14  4  8 18  5 13  7  0  5  7  0 18\n",
      "  6  5  1 19  0  1  9  1 14 15  0  2  3  0  2  6  1  0 22  1  3 22 11  1\n",
      " 26 10 10  4 11 11 23 10 16  1  0 24  9  3 16 27  2 17  0 16  1  0 24  9\n",
      "  3 16 27  2 26 10 10 19  5  8  7  2  0 18  5  2  5 35  1  9 23 10 11  1\n",
      "  2  0 13  7  0], shape=(101,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for i in dataset.take(3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 1s 0us/step\n",
      "17473536/17464789 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n",
      "1654784/1641221 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "word_index = keras.datasets.imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pankhuri\\.conda\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Pankhuri\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Size...: 100%|██████████| 80/80 [00:15<00:00,  5.17 MiB/s]rl]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:15<00:00, 15.47s/ url]\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\Pankhuri\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = info.splits[\"train\"].num_examples\n",
    "test_size = info.splits[\"test\"].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for X_batch, y_batch in datasets[\"train\"].batch(3).take(1):\n",
    "#     print(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it. ...\n",
      "Label: 0 = Negative\n",
      "\n",
      "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all. ...\n",
      "Label: 0 = Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in datasets[\"train\"].batch(2).take(1):\n",
    "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
    "        print(\"Review:\", review.decode(\"utf-8\"), \"...\")\n",
    "        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_batch.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess(X_batch, y_batch)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_batch = tf.strings.substr(X_batch, 0, 5)\n",
    "# X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
    "# X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "# X_batch = tf.strings.split(X_batch)\n",
    "# X_batch.to_tensor(default_value=b\"<pad>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 53), dtype=string, numpy=\n",
       " array([[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie',\n",
       "         b\"Don't\", b'be', b'lured', b'in', b'by', b'Christopher',\n",
       "         b'Walken', b'or', b'Michael', b'Ironside', b'Both', b'are',\n",
       "         b'great', b'actors', b'but', b'this', b'must', b'simply', b'be',\n",
       "         b'their', b'worst', b'role', b'in', b'history', b'Even',\n",
       "         b'their', b'great', b'acting', b'could', b'not', b'redeem',\n",
       "         b'this', b\"movie's\", b'ridiculous', b'storyline', b'This',\n",
       "         b'movie', b'is', b'an', b'early', b'nineties', b'US',\n",
       "         b'propaganda', b'pi', b'<pad>', b'<pad>', b'<pad>'],\n",
       "        [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep',\n",
       "         b'during', b'films', b'but', b'this', b'is', b'usually', b'due',\n",
       "         b'to', b'a', b'combination', b'of', b'things', b'including',\n",
       "         b'really', b'tired', b'being', b'warm', b'and', b'comfortable',\n",
       "         b'on', b'the', b'sette', b'and', b'having', b'just', b'eaten',\n",
       "         b'a', b'lot', b'However', b'on', b'this', b'occasion', b'I',\n",
       "         b'fell', b'asleep', b'because', b'the', b'film', b'was',\n",
       "         b'rubbish', b'The', b'plot', b'development', b'was', b'constant',\n",
       "         b'Cons']], dtype=object)>,\n",
       " <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0], dtype=int64)>)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53893"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "truncated_vocabulary = [word for word, count in vocabulary.most_common()[:vocab_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "12\n",
      "11\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}\n",
    "for word in b\"This movie was faaaaaantastic\".split():\n",
    "    print(word_to_id.get(word) or vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(word_to_id.get(b'Thicdcs') or 111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10053]], dtype=int64)>"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 60)\n",
      "(32,)\n",
      "(32, 60)\n",
      "(32,)\n",
      "(32, 63)\n",
      "(32,)\n",
      "(32, 61)\n",
      "(32,)\n",
      "(32, 63)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in train_set.take(5):\n",
    "    print(X_batch.shape)\n",
    "    print(y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 69s 81ms/step - loss: 0.5357 - accuracy: 0.7249\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 60s 76ms/step - loss: 0.3434 - accuracy: 0.8582\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 63s 81ms/step - loss: 0.1883 - accuracy: 0.9342\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 64s 81ms/step - loss: 0.1436 - accuracy: 0.9492\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 62s 79ms/step - loss: 0.1179 - accuracy: 0.9559\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size + num_oov_buckets"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "24c7168f742075337832b93c1fbfd4355e111387279ce758eae35d9b42cad7f4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
