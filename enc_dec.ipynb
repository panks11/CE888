{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/\n",
    "# https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/custom/15844304.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class Train_Data:\n",
    "\n",
    "    \"\"\"\n",
    "    Train_Data class Loads the data from multiple Training JSON files in Pandas Dataframes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, filenames):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------\n",
    "        path: directory where English to SQL translation JSON are placed\n",
    "        filenames: Json file names containing Train data\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.filenames = filenames\n",
    "        self.df_train = pd.DataFrame()\n",
    "        for f in self.filenames:\n",
    "            print(\"Reading file at path\", self.path + f)\n",
    "            try:\n",
    "                df = pd.read_json(self.path + f)\n",
    "                if len(self.df_train) == 0:\n",
    "                    self.df_train = df\n",
    "                else:\n",
    "                    self.df_train = self.df_train.append(df)\n",
    "                print(\"{} Rows in Total\".format(len(self.df_train)))\n",
    "            except Exception as e:\n",
    "                print(\"Got error while Reading file : \", e)\n",
    "\n",
    "    @property\n",
    "    def questions(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        ------------\n",
    "        Returns English Questions in Dataframe Rows as List\n",
    "        \"\"\"\n",
    "        return self.df_train.question.values.tolist()\n",
    "\n",
    "    @property\n",
    "    def sql(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        ------------\n",
    "        Returns SQL in Dataframe Rows as List\n",
    "        \"\"\"\n",
    "        return self.df_train[\"query\"].values.tolist()\n",
    "\n",
    "    @property\n",
    "    def question_tokens(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        ------------\n",
    "        Returns English Question Tokens in Dataframe Rows as List\n",
    "        \"\"\"\n",
    "\n",
    "        return self.df_train[\"question_toks\"].values.tolist()\n",
    "\n",
    "    @property\n",
    "    def sql_tokens(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        ------------\n",
    "        Returns SQL Query Tokens in Dataframe Rows as List\n",
    "        \"\"\"\n",
    "        return self.df_train[\"query_toks\"].values.tolist()\n",
    "\n",
    "    def get_special_characters(self, list_of_text):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------\n",
    "        list_of_text: Input List of Text\n",
    "        Returns\n",
    "        ------------\n",
    "        Provides list of Special Characters in the text\n",
    "        \"\"\"\n",
    "        return list(\n",
    "            set(\n",
    "                Preprocess.special_char(\"\".join([\"\".join(ele) for ele in list_of_text]))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_vocab_size(self, list_of_text):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------\n",
    "        list_of_text: Input List of Text\n",
    "\n",
    "        Returns\n",
    "        ------------\n",
    "        Vocabulary size or unique words in the corpus\n",
    "        \"\"\"\n",
    "        word_list = []\n",
    "        for sentence in list_of_text:\n",
    "            for word in sentence.split():\n",
    "                word = word.lower().strip()\n",
    "                if word not in word_list:\n",
    "                    word_list.append(word)\n",
    "        return len(word_list), word_list\n",
    "\n",
    "\n",
    "#################################\n",
    "# CONSTANTS\n",
    "\n",
    "EOS = \"[END]\"\n",
    "SOS = \"[START]\"\n",
    "\n",
    "\n",
    "#################################################################################################################\n",
    "\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "\n",
    "class Preprocess:\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocess class cleans and standardize the data, add SOS-EOS tags to the data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------\n",
    "        text : Input string\n",
    "        Runs the text processing steps\n",
    "\n",
    "        \"\"\"\n",
    "        self.processed_text = self.run_pipeline(text)\n",
    "\n",
    "    def text_standardize(self, text):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------\n",
    "        text : Input string\n",
    "\n",
    "        Returns\n",
    "        ------------\n",
    "        -   Unicode normalization using NFKD method\n",
    "        -   Lower Case text\n",
    "\n",
    "        \"\"\"\n",
    "        text = tf_text.normalize_utf8(text, \"NFKD\")\n",
    "        text = tf.strings.lower(text)\n",
    "        return text\n",
    "\n",
    "    def text_whitespace(self, text):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------\n",
    "        text : Input string\n",
    "\n",
    "        Returns\n",
    "        ------------\n",
    "        -   Remove $ and \\\\ special characters\n",
    "        -   Add space around punctations\n",
    "        -   Remove spaces around sentences\n",
    "\n",
    "        \"\"\"\n",
    "        text = tf.strings.regex_replace(text, \"[$\\\\\\\\]\", \"\")\n",
    "        text = tf.strings.regex_replace(text, \"[.?!,Â¿()*:@]\", r\" \\0 \")\n",
    "        text = tf.strings.strip(text)\n",
    "        return text\n",
    "\n",
    "    def add_SOS_EOS(self, text):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------\n",
    "        text : Input string\n",
    "\n",
    "        Returns\n",
    "        ------------\n",
    "        -   Add <SOS> and <EOS> tags to each sentence\n",
    "\n",
    "        \"\"\"\n",
    "        text = tf.strings.join([SOS, text, EOS], separator=\" \")\n",
    "        return text\n",
    "\n",
    "    @classmethod\n",
    "    def special_char(cls, text):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------\n",
    "        text : Input string\n",
    "\n",
    "        Returns\n",
    "        ------------\n",
    "        -   Special Characters found in Text using Regular Expression\n",
    "        \"\"\"\n",
    "        return re.findall(r\"[\\W]\", text.replace(\" \", \"\"))\n",
    "\n",
    "    def run_pipeline(self, text):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------\n",
    "        text : Input string\n",
    "\n",
    "        Returns\n",
    "        ------------\n",
    "        Executes series of Text pre processing functions\n",
    "\n",
    "        \"\"\"\n",
    "        text = self.text_standardize(text)\n",
    "        text = self.text_whitespace(text)\n",
    "        text = self.add_SOS_EOS(text)\n",
    "        self.text = text\n",
    "        return self.text\n",
    "\n",
    "\n",
    "class Features:\n",
    "    \"\"\"\n",
    "    Extracts text features from data\n",
    "    \"\"\"\n",
    "\n",
    "    def tf_lower_and_split_punct(self, text):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------\n",
    "        text : Input string\n",
    "\n",
    "        Returns\n",
    "        ------------\n",
    "        Standardized Text\n",
    "\n",
    "        \"\"\"\n",
    "        return Preprocess(text).processed_text\n",
    "\n",
    "    def vectorizor(self, document, max_vocab_size):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------\n",
    "        document : Collection of sentences\n",
    "        max_vocab_size : No of words in document used for TextVectorization\n",
    "\n",
    "        Returns\n",
    "        ------------\n",
    "        TextVectorization object\n",
    "\n",
    "        \"\"\"\n",
    "        text_processor = tf.keras.layers.TextVectorization(\n",
    "            standardize=self.tf_lower_and_split_punct, max_tokens=max_vocab_size\n",
    "        )\n",
    "        text_processor.adapt(document)\n",
    "        print(\"Sample Vocabulary\", text_processor.get_vocabulary()[:10])\n",
    "        return text_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file at path C:/Users/Lenovo/Downloads/pankhuri/CE888/assignment2/seq2seq/spider/train_spider.json\n",
      "7000 Rows in Total\n",
      "Reading file at path C:/Users/Lenovo/Downloads/pankhuri/CE888/assignment2/seq2seq/spider/train_others.json\n",
      "8659 Rows in Total\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SELECT count(*) FROM head WHERE age  >  56'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    del o\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "o = Train_Data(\n",
    "    \"C:/Users/Lenovo/Downloads/pankhuri/CE888/assignment2/seq2seq/spider/\",\n",
    "    [\"train_spider.json\", \"train_others.json\"],\n",
    ")\n",
    "\n",
    "o.sql[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Vocabulary ['', '[UNK]', 'the', '[START]', '[END]', 'of', '?', '.', 'what', 'are']\n",
      "Sample Vocabulary ['', '[UNK]', '.', 't1', 't2', '=', 'select', 'from', 'as', '[START]']\n"
     ]
    }
   ],
   "source": [
    "max_input_vocab_size = o.get_vocab_size(o.questions)[0]\n",
    "max_output_vocab_size = o.get_vocab_size(o.sql)[0]\n",
    "\n",
    "input_text_processor = Features().vectorizor(o.questions, max_input_vocab_size)\n",
    "output_text_processor = Features().vectorizor(o.sql, max_output_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = o.questions\n",
    "targ = o.sql\n",
    "\n",
    "BUFFER_SIZE = len(inp)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "emb = \"C:/Users/Lenovo/Downloads/pankhuri/CE888/assignment2/embedding/glove.6B.100d.txt\"\n",
    "glove_file = open(emb, encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype=\"float32\")\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_processor.get_vocabulary()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dictionary.get(input_text_processor.get_vocabulary()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = max_input_vocab_size\n",
    "EMBEDDING_SIZE = 100\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_SIZE))\n",
    "for index, word in enumerate(input_text_processor.get_vocabulary()):\n",
    "    # if word:\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194  , -0.24487001,  0.72812003, -0.39961001,  0.083172  ,\n",
       "        0.043953  , -0.39140999,  0.3344    , -0.57545   ,  0.087459  ,\n",
       "        0.28786999, -0.06731   ,  0.30906001, -0.26383999, -0.13231   ,\n",
       "       -0.20757   ,  0.33395001, -0.33848   , -0.31742999, -0.48335999,\n",
       "        0.1464    , -0.37303999,  0.34577   ,  0.052041  ,  0.44946   ,\n",
       "       -0.46970999,  0.02628   , -0.54154998, -0.15518001, -0.14106999,\n",
       "       -0.039722  ,  0.28277001,  0.14393   ,  0.23464   , -0.31020999,\n",
       "        0.086173  ,  0.20397   ,  0.52623999,  0.17163999, -0.082378  ,\n",
       "       -0.71787   , -0.41531   ,  0.20334999, -0.12763   ,  0.41367   ,\n",
       "        0.55186999,  0.57907999, -0.33476999, -0.36559001, -0.54856998,\n",
       "       -0.062892  ,  0.26583999,  0.30204999,  0.99774998, -0.80480999,\n",
       "       -3.0243001 ,  0.01254   , -0.36941999,  2.21670008,  0.72201002,\n",
       "       -0.24978   ,  0.92136002,  0.034514  ,  0.46744999,  1.10790002,\n",
       "       -0.19358   , -0.074575  ,  0.23353   , -0.052062  , -0.22044   ,\n",
       "        0.057162  , -0.15806   , -0.30798   , -0.41624999,  0.37972   ,\n",
       "        0.15006   , -0.53211999, -0.20550001, -1.25259995,  0.071624  ,\n",
       "        0.70564997,  0.49744001, -0.42063001,  0.26148   , -1.53799999,\n",
       "       -0.30223   , -0.073438  , -0.28312001,  0.37103999, -0.25217   ,\n",
       "        0.016215  , -0.017099  , -0.38984001,  0.87423998, -0.72569001,\n",
       "       -0.51058   , -0.52028   , -0.1459    ,  0.82779998,  0.27061999])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_len = input_text_processor.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Input, LSTM\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_words, EMBEDDING_SIZE, weights=[embedding_matrix], input_length=max_input_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_NODES = 256\n",
    "encoder_inputs_placeholder = Input(shape=(input_text_processor.vocabulary_size(),))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "\n",
    "encoder = LSTM(LSTM_NODES, return_state=True)\n",
    "\n",
    "encoder_outputs, h, c = encoder(x)\n",
    "encoder_states = [h, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_words_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16092\\1452612650.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdecoder_inputs_placeholder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_text_processor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdecoder_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_words_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM_NODES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdecoder_inputs_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_inputs_placeholder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_words_output' is not defined"
     ]
    }
   ],
   "source": [
    "decoder_inputs_placeholder = Input(shape=(output_text_processor.vocabulary_size(),))\n",
    "\n",
    "decoder_embedding = Embedding(\n",
    "    , LSTM_NODES)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "decoder_lstm = LSTM(LSTM_NODES, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dense = Dense(num_words_output, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs_placeholder, decoder_inputs_placeholder], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.fit(\n",
    "    dataset,\n",
    "    decoder_targets_one_hot,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 6, 51) (100000, 3, 51) (100000, 3, 51)\n",
      "3125/3125 [==============================] - 79s 22ms/step - loss: 0.6465 - accuracy: 0.7962\n",
      "Accuracy: 99.00%\n",
      "X=[47, 49, 5, 49, 38, 39] y=[5, 49, 47], yhat=[5, 49, 47]\n",
      "X=[5, 20, 23, 21, 11, 39] y=[23, 20, 5], yhat=[23, 20, 5]\n",
      "X=[50, 21, 46, 25, 29, 24] y=[46, 21, 50], yhat=[46, 21, 50]\n",
      "X=[10, 17, 1, 5, 37, 10] y=[1, 17, 10], yhat=[1, 17, 10]\n",
      "X=[31, 35, 34, 33, 12, 43] y=[34, 35, 31], yhat=[34, 35, 31]\n",
      "X=[2, 43, 48, 43, 5, 18] y=[48, 43, 2], yhat=[43, 48, 2]\n",
      "X=[17, 4, 2, 12, 31, 7] y=[2, 4, 17], yhat=[2, 4, 17]\n",
      "X=[30, 40, 23, 36, 6, 14] y=[23, 40, 30], yhat=[23, 40, 30]\n",
      "X=[34, 1, 23, 16, 18, 7] y=[23, 1, 34], yhat=[23, 1, 34]\n",
      "X=[32, 23, 19, 45, 23, 37] y=[19, 23, 32], yhat=[19, 23, 32]\n"
     ]
    }
   ],
   "source": [
    "# from random import randint\n",
    "# import numpy as np\n",
    "# from numpy import array\n",
    "# from numpy import argmax\n",
    "# from numpy import array_equal\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from keras.models import Model\n",
    "# from keras.layers import Input\n",
    "# from keras.layers import LSTM\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# # generate a sequence of random integers\n",
    "# def generate_sequence(length, n_unique):\n",
    "# \treturn [randint(1, n_unique-1) for _ in range(length)]\n",
    "\n",
    "# # prepare data for the LSTM\n",
    "# def get_dataset(n_in, n_out, cardinality, n_samples):\n",
    "# \tX1, X2, y = list(), list(), list()\n",
    "# \tfor _ in range(n_samples):\n",
    "# \t\tsource = generate_sequence(n_in, cardinality)\n",
    "#         # define padded target sequence\n",
    "# \t\ttarget = source[:n_out]\n",
    "# \t\ttarget.reverse()\n",
    "#         # create padded input target sequence\n",
    "# \t\ttarget_in = [0] + target[:-1]\n",
    "#         # encode\n",
    "# \t\tsrc_encoded = to_categorical([source], num_classes=cardinality)\n",
    "# \t\ttar_encoded = to_categorical([target], num_classes=cardinality)\n",
    "# \t\ttar2_encoded = to_categorical([target_in], num_classes=cardinality)\n",
    "#         # store\n",
    "# \t\tX1.append(src_encoded)\n",
    "# \t\tX2.append(tar2_encoded)\n",
    "# \t\ty.append(tar_encoded)\n",
    "# \tX1 = np.squeeze(array(X1), axis=1)\n",
    "# \tX2 = np.squeeze(array(X2), axis=1)\n",
    "# \ty = np.squeeze(array(y), axis=1)\n",
    "# \treturn array(X1), array(X2), array(y)\n",
    "\n",
    "# # returns train, inference_encoder and inference_decoder models\n",
    "# def define_models(n_input, n_output, n_units):\n",
    "# \t# define training encoder\n",
    "# \tencoder_inputs = Input(shape=(None, n_input))\n",
    "# \tencoder = LSTM(n_units, return_state=True)\n",
    "# \tencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# \tencoder_states = [state_h, state_c]\n",
    "# \t# define training decoder\n",
    "# \tdecoder_inputs = Input(shape=(None, n_output))\n",
    "# \tdecoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "# \tdecoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "# \tdecoder_dense = Dense(n_output, activation='softmax')\n",
    "# \tdecoder_outputs = decoder_dense(decoder_outputs)\n",
    "# \tmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "# \t# define inference encoder\n",
    "# \tencoder_model = Model(encoder_inputs, encoder_states)\n",
    "# \t# define inference decoder\n",
    "# \tdecoder_state_input_h = Input(shape=(n_units,))\n",
    "# \tdecoder_state_input_c = Input(shape=(n_units,))\n",
    "# \tdecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "# \tdecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# \tdecoder_states = [state_h, state_c]\n",
    "# \tdecoder_outputs = decoder_dense(decoder_outputs)\n",
    "# \tdecoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "# \t# return all models\n",
    "# \treturn model, encoder_model, decoder_model\n",
    "\n",
    "# # generate target given source sequence\n",
    "# def predict_sequence(infenc, infdec, source, n_steps, cardinality):\n",
    "# \t# encode\n",
    "# \tstate = infenc.predict(source)\n",
    "# \t# start of sequence input\n",
    "# \ttarget_seq = array([0.0 for _ in range(cardinality)]).reshape(1, 1, cardinality)\n",
    "# \t# collect predictions\n",
    "# \toutput = list()\n",
    "# \tfor t in range(n_steps):\n",
    "# \t\t# predict next char\n",
    "# \t\tyhat, h, c = infdec.predict([target_seq] + state)\n",
    "# \t\t# store prediction\n",
    "# \t\toutput.append(yhat[0,0,:])\n",
    "# \t\t# update state\n",
    "# \t\tstate = [h, c]\n",
    "# \t\t# update target sequence\n",
    "# \t\ttarget_seq = yhat\n",
    "# \treturn array(output)\n",
    "\n",
    "# # decode a one hot encoded string\n",
    "# def one_hot_decode(encoded_seq):\n",
    "# \treturn [argmax(vector) for vector in encoded_seq]\n",
    "\n",
    "# # configure problem\n",
    "# n_features = 50 + 1\n",
    "# n_steps_in = 6\n",
    "# n_steps_out = 3\n",
    "# # define model\n",
    "# train, infenc, infdec = define_models(n_features, n_features, 128)\n",
    "# train.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# # generate training dataset\n",
    "# X1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 100000)\n",
    "# print(X1.shape,X2.shape,y.shape)\n",
    "# # train model\n",
    "# train.fit([X1, X2], y, epochs=1,callbacks=[tensorboard_cb])\n",
    "# # evaluate LSTM\n",
    "# total, correct = 100, 0\n",
    "# for _ in range(total):\n",
    "# \tX1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 1)\n",
    "# \ttarget = predict_sequence(infenc, infdec, X1, n_steps_out, n_features)\n",
    "# \tif array_equal(one_hot_decode(y[0]), one_hot_decode(target)):\n",
    "# \t\tcorrect += 1\n",
    "# print('Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))\n",
    "# # spot check some examples\n",
    "# for _ in range(10):\n",
    "# \tX1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 1)\n",
    "# \ttarget = predict_sequence(infenc, infdec, X1, n_steps_out, n_features)\n",
    "# \tprint('X=%s y=%s, yhat=%s' % (one_hot_decode(X1[0]), one_hot_decode(y[0]), one_hot_decode(target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "24c7168f742075337832b93c1fbfd4355e111387279ce758eae35d9b42cad7f4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
